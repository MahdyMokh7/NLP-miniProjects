{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† 1<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 dir='rtl' style=\"color:rgb(90, 255, 184); font-size: 20px;\">Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±Ù‡Ø§ Ùˆ N-gram</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø´Ù‡Ø±Ø²Ø§Ø¯ Ø¢Ø°Ø±ÛŒ Ø¢Ø²Ø§Ø¯ - ÙØ±Ø´Ø§Ø¯ Ø­Ø³Ø§Ù…ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">shahrzad.azari@ut.ac.ir - farshad.hessami@ut.ac.ir</p>\n",
    "\n",
    "<div dir='rtl' style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p dir='rtl' style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ù…Ù‡Ø¯ÛŒ Ù…Ø®ØªØ§Ø±ÛŒ</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: 810101515</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: 2/8/1404</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir; max-width: 100%;word-wrap: break-word;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "<div style=\"padding-right:40px\">\n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ Ù…ÙØ§Ù‡ÛŒÙ… Tokenization, Regular Expression , N-gram Language Modeling Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯. \n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ø§ Regex</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Auto-Correction Ø¨Ø§ Minimum Edit Distance</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Tokenization</span> (25)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Rule-based Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: BPE Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Wordpiece Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Tokenization Visualization</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø³ÙˆÙ… - <span dir=\"ltr\">N-gram Language Modeling</span> (55)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Data cleaning & Tokenization</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ N-gram</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù…Ø¹ÛŒØ§Ø± Perplexity</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Temperature Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„Ø§ØªØŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÛŒ Regex Ùˆ Ù‡Ù…ÛŒÙ†Ø·ÙˆØ± Minimum Distance Ù…ÙˆØ§Ø¬Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø¯Ø± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„â€ŒÙ‚Ø¨ÙˆÙ„ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¨Ø®Ø´ Ø¯ÙˆÙ… Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Minimum Distance ÛŒÚ© Ø³ÛŒØ³ØªÙ… Auto-Correction Ø³Ø§Ø¯Ù‡ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ø§ Regex</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ÙØ§ÛŒÙ„ emails.txt Ú©Ù‡ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ØŒ Ø´Ø§Ù…Ù„ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø³Ù… Ø¨Ù‡â€ŒÙ‡Ù…Ø±Ø§Ù‡ Ø§ÛŒÙ…ÛŒÙ„ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡â€Œâ€ŒØ´Ø§Ù† Ø¯Ø± ÛŒÚ© Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Ø§Ø² Ø´Ù…Ø§ Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¹ØªØ¨Ø± Ù‡Ø³ØªÙ†Ø¯ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² regex Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§ÛŒÙ…ÛŒÙ„ Ø§Ø² Ø¯Ùˆ Ø¨Ø®Ø´ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ú©Ù‡ Ø¨Ø§ @ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø¨Ø®Ø´ Ø§ÙˆÙ„ (Ù‚Ø¨Ù„ Ø§Ø² @) local-part Ù†Ø§Ù… Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ø®Ø´ Ø¯ÙˆÙ… domain.\n",
    "<br>\n",
    "Ù…Ù†Ø¸ÙˆØ± Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø¹Ø§ÛŒØª Ø´Ø¯Ù‡â€ŒØ¨Ø§Ø´Ù†Ø¯:\n",
    "<br>\n",
    "Û±. Ø¯Ùˆ Ø¨Ø®Ø´ Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ø§ ÛŒÚ© Ùˆ ØªÙ†Ù‡Ø§ ÛŒÚ© @ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ¨Ø§Ø´Ù†Ø¯.\n",
    "<br>\n",
    "Û². Ø¯Ø± local-part Ù‡Ù… Ù†Ø§Ù… Ùˆ Ù‡Ù… Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ Ø´Ø®Øµ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Û³. Ø¯Ø± local-part ØªÙ†Ù‡Ø§ Ø­Ø±ÙˆÙ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ -ØŒ_ Ùˆ . Ù…Ø¬Ø§Ø² Ù‡Ø³ØªÙ†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ùˆ Ù†Ù‚Ø·Ù‡ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù¾Ø´Øª Ù‡Ù… Ø¨ÛŒØ§ÛŒÙ†Ø¯.\n",
    "<br>\n",
    "Û´. Ø¯Ø± Ø¨Ø®Ø´ domain ÛŒÚ© Ù…ÛŒØ²Ø¨Ø§Ù† Ø¯Ø§Ø±ÛŒÙ… Ùˆ ÛŒÚ© Ù¾Ø³ÙˆÙ†Ø¯. Ù…ÛŒØ²Ø¨Ø§Ù† Ùˆ Ù¾Ø³ÙˆÙ†Ø¯ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ø§Ø² ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. (Ù…ÛŒØ²Ø¨Ø§Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø®ÙˆØ¯ Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø§ Ø¯Ùˆ Ù†Ù‚Ø·Ù‡â€ŒÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ø¯Ø± domain Ù…Ø¬Ø§Ø² Ù†ÛŒØ³Øª.)\n",
    "<br>\n",
    "Ûµ. Ù¾Ø³ÙˆÙ†Ø¯ Ø§Ø² Ø­Ø±ÙˆÙ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¯Ø§Ø±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù„ÛŒØ³Øª Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙØ§ÛŒÙ„ emails.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "\n",
    "file_name_emails = \"emails.txt\"\n",
    "file_path_emails = os.path.join(\".\", file_name_emails)\n",
    "out_path_emails = \"emails-valid.txt\"\n",
    "\n",
    "\n",
    "def get_name_email(line):\n",
    "    parts = line.split(sep=',')\n",
    "    name = parts[0]\n",
    "    name = name.split(sep='=')[1] \n",
    "\n",
    "    email = parts[1]\n",
    "    email = email.split(sep='=')[1]\n",
    "\n",
    "    return name.strip(), email.strip()\n",
    "\n",
    "\n",
    "def name_in_text(text: str, first_name: str, last_name: str) -> bool:\n",
    "\n",
    "    first = first_name.strip()\n",
    "    last = last_name.strip()\n",
    "    if not first or not last:\n",
    "        return False\n",
    "\n",
    "    pfirst = re.compile(re.escape(first), re.IGNORECASE)\n",
    "    plast = re.compile(re.escape(last), re.IGNORECASE)\n",
    "\n",
    "    # presence check\n",
    "    if pfirst.search(text) is None or plast.search(text) is None:\n",
    "        return False\n",
    "\n",
    "    # collect spans, then test non-overlap\n",
    "    f_spans = [m.span() for m in pfirst.finditer(text)]\n",
    "    l_spans = [m.span() for m in plast.finditer(text)]\n",
    "\n",
    "    for a1, a2 in f_spans:\n",
    "        for b1, b2 in l_spans:\n",
    "            if a1 >= b2 or b1 >= a2:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def email_correctness_checker(name:str, email:str) -> bool:\n",
    "\n",
    "    if not isinstance(name, str) or not isinstance(email, str):\n",
    "        return False\n",
    "\n",
    "    first_name = name.split()[0]\n",
    "    last_name = ' '.join(name.split()[1:])\n",
    "\n",
    "    # print(f\"firstname: {first_name} lastname: {last_name}\")\n",
    "\n",
    "    \n",
    "    Email_Pattern = re.compile(r\"^[\\w-][\\w.-]*@[\\w.-]+\\.[a-zA-Z]{2,}$\")  # Ø¯Ùˆ Ù†Ù‚Ø·Ù‡ Ù¾Ø´Øª Ù‡Ù… Ù†Ù…ÛŒØªÙˆÙ†Ù†Ø¯ Ø¨ÛŒØ§ÛŒÙ†Ø¯\n",
    "\n",
    "    Multi_Dot_Pattern = re.compile(r\"^.*\\.\\.+.*$\")\n",
    "\n",
    "    if Email_Pattern.match(email) is None or Multi_Dot_Pattern.match(email) is not None:\n",
    "        return False\n",
    "    \n",
    "    domain = email.split('@')[1]\n",
    "    local_part = email.split('@')[0]\n",
    "\n",
    "    if not name_in_text(local_part, first_name, last_name):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "emails_valid = list()\n",
    "try:\n",
    "    with open(file_path_emails) as file:\n",
    "        content_lines = file.readlines()\n",
    "        for line in content_lines:\n",
    "            name, email = get_name_email(line)\n",
    "\n",
    "            # print(f\"-{name}- & -{email}-\")\n",
    "\n",
    "            if email_correctness_checker(name, email):\n",
    "                emails_valid.append(email)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    print(\"File not found.\")\n",
    "\n",
    "\n",
    "with open(out_path_emails, 'w', encoding=\"utf-8\") as file:\n",
    "    file.write('\\n'.join(emails_valid))\n",
    "\n",
    "print(emails_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Regex ÛŒÚ© ØªØ§Ø¨Ø¹ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø³Ù… ÛŒÚ© Ø´Ø®Øµ Ø±Ø§ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯ Ùˆ Ø¯Ø±ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø³Ù… Ø¯Ø± Ø¨ÛŒÙ† Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ØŒ Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ†â€ŒØµÙˆØ±ØªØŒ ÛŒÚ© Ù¾ÛŒØºØ§Ù… Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ú†Ø§Ù¾ Ú©Ù†Ø¯.\n",
    "<br>\n",
    "ØªÙˆØ¬Ù‡: Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§ÛŒÙ…ÛŒÙ„ Ø§ÛŒÙ† Ø§Ø´Ø®Ø§ØµØŒ ØªØ­Øª Ù†Ø§Ù… Ø´Ø®Øµ Ø¯ÛŒÚ¯Ø±ÛŒ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ú©Ø§Ø± Ø´Ù…Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø§ÛŒÙ† Ø§ÙØ±Ø§Ø¯ Ø±Ø§ Ø§Ø² Ø¨ÛŒÙ† ØªÙ…Ø§Ù… Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªØ§Ø¨Ø¹ Ø±Ø§ Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name1 = Behnam Khatibi\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name2 = Mehrdad Ebrahimi\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÙØ±Ø§Ø¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def get_email_from_name(name: str) -> str:  # returns the email\n",
    "#     first_name, last_name = name.split()\n",
    "\n",
    "#     for email in emails_valid:\n",
    "#         if first_name.lower() in email.lower() and last_name.lower() in email.lower():\n",
    "#             return email\n",
    "#     print(\"Email doesn't exist\")\n",
    "#     return \"\"\n",
    "        \n",
    "def get_email_from_name(name: str) -> str:\n",
    "\n",
    "    first_name, last_name = name.split()\n",
    "\n",
    "    first_pat = re.compile(re.escape(first_name), re.IGNORECASE)\n",
    "    last_pat  = re.compile(re.escape(last_name),  re.IGNORECASE)\n",
    "\n",
    "    for email in emails_valid:\n",
    "        if first_pat.search(email) and last_pat.search(email):\n",
    "            return email\n",
    "        \n",
    "    print(\"Email doesn't exist\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "name1 = \"Behnam Khatibi\"\n",
    "name2 = \"Mehrdad Ebrahimi\"\n",
    "\n",
    "print(get_email_from_name(name1))\n",
    "print(get_email_from_name(name2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Auto-Correction Ø¨Ø§ Minimum Edit Distance</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Û±. Ø§Ø¨ØªØ¯Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… levenshtein_distance Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† minimum_distance Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡â€ŒØ¯Ø³Øª Ø¢ÙˆØ±ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair1 = \"Athletic\", \"Atlantic\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair2 = \"London\", \"Boston\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"Action\", \"Compact\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"\", \"Sting\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ù‚Ø¯Ø§Ø± minimum distance Ø¨ÛŒÙ† Ø¬ÙØª Ú©Ù„Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(a: str, b: str) -> int:\n",
    "    m, n = len(a), len(b)\n",
    "\n",
    "    # create DP table ( if we solve it recursevily it will cost us time and memory )\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # base cases - was discussed in the class\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # fill table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if a[i - 1] == b[j - 1] else 1  # we put 1 but the cost for replacing accodring to the class and the Jurafsky book was 2 (not levenshtein)\n",
    "\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,      # deletion\n",
    "                dp[i][j - 1] + 1,      # insertion\n",
    "                dp[i - 1][j - 1] + cost  # substitution\n",
    "            )\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "# We know the max value that the min edit distance could be is max(len(a), len(b)) in Levenshtein\n",
    "examples = [(\"Athletic\", \"Atlantic\"), (\"London\", \"Boston\"), (\"Action\", \"Compact\") ,(\"\", \"Sting\")]\n",
    "for inp in examples:\n",
    "    MinDistance = levenshtein_distance(*inp)\n",
    "    print(f\"input is: {inp}   |   Min Edit Distance is(Levenshtein algo): {MinDistance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Û². Ø¨Ø¹Ø¯ Ø§Ø² Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ minimum distance Ø­Ø§Ù„ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù†ØŒ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ø§Ù…Ù„Ø§ÛŒÛŒ Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ú©Ù„Ù…Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ø§Ù…Ù„Ø§ÛŒØ´Ø§Ù† Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª. ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø§Ù…Ù„Ø§ÛŒ ØµØ­ÛŒØ­ Ú©Ù„Ù…Ø§Øª Ú©Ù‡ Ú©Ù„Ù…Ø§Øª Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ù†ÛŒØ² Ø´Ø§Ù…Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„ vocab.txt Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "sentence_to_be_corrected = \n",
    "\"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡â€ŒÛŒ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§ Ú©Ù…Ú© ÙØ§ÛŒÙ„ vocab.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(path=\"vocab.txt\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        words_vocab = [word.strip() for word in content.split(',')]\n",
    "        return words_vocab\n",
    "\n",
    "def autocorrect(word: str, vocab: list, max_dist: int = 2) -> str:\n",
    "\n",
    "    # Trying to hanlde lower case in the word or vocab.txt correctly (still not sure)\n",
    "    word_lower = word.lower()\n",
    "    best_word = word\n",
    "    best_dist = float(\"inf\")\n",
    "\n",
    "    for v in vocab:\n",
    "        d = levenshtein_distance(word_lower, v.lower())\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_word = v\n",
    "            if d == 0:  # perfect match\n",
    "                return v\n",
    "\n",
    "    # preventing correction which should not be corrected (too much editing)\n",
    "    return best_word if best_dist <= max_dist else word\n",
    "\n",
    "\n",
    "vocab = load_vocab() \n",
    "print(vocab)\n",
    "\n",
    "sentence_to_be_corrected = \"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "    \n",
    "def auto_correct_sentence(sent):\n",
    "    corrected_sentence = []\n",
    "    for word in sentence_to_be_corrected.split():\n",
    "        # print(word)\n",
    "        corrected_sentence.append(autocorrect(word, vocab, max_dist=3))\n",
    "\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "print(auto_correct_sentence(sentence_to_be_corrected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_with_heuristics(word: str, vocab: list, max_dist: int = 2) -> str:\n",
    "    word_lower = word.lower()\n",
    "    lw = len(word)\n",
    "    best_word = word\n",
    "    best_dist = float(\"inf\")\n",
    "    best_length_diff = float(\"inf\")\n",
    "\n",
    "    for original, lower in vocab:\n",
    "\n",
    "        # if the length differs a lot dont calc the Min Edit Distance\n",
    "        if abs(len(lower) - lw) > max_dist:\n",
    "            continue\n",
    "\n",
    "        d = levenshtein_distance(word_lower, lower, max_dist)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_word = original\n",
    "            best_length_diff = abs(len(lower) - lw)\n",
    "            if d == 0:\n",
    "                return original\n",
    "\n",
    "        elif d == best_dist:  # for better equil dist decision\n",
    "            length_diff = abs(len(lower) - lw)\n",
    "            if length_diff < best_length_diff:\n",
    "                best_word = original\n",
    "                best_length_diff = length_diff\n",
    "\n",
    "    return best_word if best_dist <= max_dist else word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Tokenization</span> (25)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø§ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ø±ÙˆØ´  Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Rule-based Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¨Ø§ Ú©Ù…Ú© Ø¯Ø³ØªÙˆØ±Ø§Øª regex ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ù…ØªÙ† Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†Ø¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ØµÙˆØ±Øª Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "# from hazm import normalize  \n",
    "\n",
    "\n",
    "# Converts fancy characters into standard ones\n",
    "# Merges lookalikes that serve the same role in text\n",
    "def normalize(text: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "I did my rule based tokenizer with considering the following on the non common words:\n",
    "\n",
    "- Keep contractions\n",
    "- Keep hyphens inside words\n",
    "- Keep abbreviations\n",
    "- Separate punctuation\n",
    "- Split currency symbol from number\n",
    "- Keep numbers whole\n",
    "- Ellipsis as one token\n",
    "- Case preserved\n",
    "- English-only assumptions\n",
    "- Works great for n-gram language models\n",
    "\n",
    "We are using N-gram method in the LM after this.\n",
    "\n",
    "This is for the Rule-Based Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Order matters: longest and most specific first\n",
    "TOKEN_PATTERN = re.compile(\n",
    "    r\"\"\"\n",
    "    (?P<URL>\\bhttps?://[^\\s<>\"']+|\\bwww\\.[^\\s<>\"']+)              # URLs\n",
    "    | (?P<EMAIL>[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})   # Emails\n",
    "    \n",
    "    | (?P<ELLIPSIS>\\.\\.\\.)                                       # Ellipsis â€¦\n",
    "    \n",
    "    | (?P<CURRENCY>[$])                                          # Currency symbol as separate token\n",
    "    | (?P<NUMBER>\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%?)                  # Numbers (12,000.50 and 12.40 and 50%)\n",
    "    \n",
    "    | (?P<ABBR>(?:[A-Za-z]\\.){2,})                               # Abbreviations like U.S.A., e.g.\n",
    "    \n",
    "    | (?P<CONTRACTION>[A-Za-z]+\\'[A-Za-z]+)                      # Contractions (don't, I'm)\n",
    "    \n",
    "    | (?P<HWORD>[A-Za-z]+(?:-[A-Za-z]+)+)                        # Hyphenated compounds\n",
    "    \n",
    "    | (?P<WORD>[A-Za-z]+)                                        # Normal words\n",
    "    \n",
    "    | (?P<PUNCT>[.,!?;:()\\\"'])                                   # Punctuation split\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_rule_based(text: str):\n",
    "    text = normalize(text)\n",
    "    tokens = []\n",
    "    for m in TOKEN_PATTERN.finditer(text):\n",
    "        tokens.append(m.group())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(tokenize_rule_based(\"That U.S.A. poster-print costs $12.40...\"))\n",
    "print(tokenize_rule_based(\"Hello, world! NLP is fun.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§ÛŒØ±Ø§Ø¯Ø§Øª Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ú†ÛŒØ³ØªØŸ\n",
    "<br>\n",
    "Ú†Ù†Ø¯ Ø±Ø§Ù‡â€ŒØ­Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "ÛŒÚ©ÛŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù‡Ø¨ÙˆØ¯ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# stronger normalizationÙˆ keeps your NFKC\n",
    "_trans_table = {\n",
    "    ord(\"â€™\"): \"'\",\n",
    "    ord(\"â€˜\"): \"'\",\n",
    "    ord(\"â€œ\"): '\"',\n",
    "    ord(\"â€\"): '\"',\n",
    "    ord(\"â€”\"): \"-\",   # em dash\n",
    "    ord(\"â€“\"): \"-\",   # en dash\n",
    "    ord(\"â€¦\"): \"...\", # unicode ellipsis -> three dots (so your ELLIPSIS rule hits)\n",
    "    0x00A0: 0x0020,  # NBSP -> space\n",
    "    0x200B: None,    # zero-width space -> remove\n",
    "}\n",
    "\n",
    "def normalize_strong(text: str) -> str:\n",
    "    t = unicodedata.normalize(\"NFKC\", text)\n",
    "    return t.translate(_trans_table)\n",
    "\n",
    "\n",
    "# do some post processing\n",
    "_URL_RE    = re.compile(r\"^(?:https?://[^\\s<>'\\\"]+|www\\.[^\\s<>'\\\"]+)$\", re.I)\n",
    "_EMAIL_RE  = re.compile(r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\")\n",
    "_NUMBER_RE = re.compile(r\"^\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%?$\")\n",
    "\n",
    "# trailing punctuation we want to peel off if it sticks to URL/EMAIL/NUMBER\n",
    "_TRAIL = set(\".,!?:;)\")\n",
    "# and quotes that often trail as well\n",
    "_TRAIL_QUOTES = set(\"\\\"'\")\n",
    "\n",
    "# common single-segment abbreviations that should keep the trailing dot\n",
    "_TITLE_ABBRS = {\"Mr\", \"Ms\", \"Mrs\", \"Dr\", \"Prof\", \"Sr\", \"Jr\", \"St\", \"Mt\", \"Ave\", \"Rd\", \"Blvd\"}\n",
    "\n",
    "# general multi-segment abbreviation pattern: Ph.D., U.S.A., e.g., i.e., U.S., etc.\n",
    "_ABBR_SEG_RE = re.compile(r\"^(?:[A-Za-z]{1,4}\\.){2,}$\")\n",
    "\n",
    "\n",
    "def _strip_trailing_punct(token: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    If token looks like URL/EMAIL/NUMBER but has glued trailing punctuation,\n",
    "    peel punctuation into separate tokens, preserving order.\n",
    "    \"\"\"\n",
    "    base = token\n",
    "    if not (_URL_RE.match(base) or _EMAIL_RE.match(base) or _NUMBER_RE.match(base)):\n",
    "        # maybe itâ€™s base + trailing punct; try stripping\n",
    "        s = base\n",
    "        trail = []\n",
    "        while s and (s[-1] in _TRAIL or (trail and s[-1] in _TRAIL_QUOTES)):\n",
    "            trail.append(s[-1])\n",
    "            s = s[:-1]\n",
    "        if s and (_URL_RE.match(s) or _EMAIL_RE.match(s) or _NUMBER_RE.match(s)):\n",
    "            return [s] + trail  # split off the glued punctuation\n",
    "    return [token]\n",
    "\n",
    "\n",
    "def _merge_abbreviations(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Merge sequences like [\"U\", \".\", \"S\", \".\"] -> [\"U.S.\"]\n",
    "    Merge [\"Ph\", \".\", \"D\", \".\"] -> [\"Ph.D.\"]\n",
    "    Keep known single-word titles with dot: [\"Mr\", \".\"] -> [\"Mr.\"].\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    i = 0\n",
    "    n = len(tokens)\n",
    "    while i < n:\n",
    "        # Title + dot\n",
    "        if i + 1 < n and tokens[i] in _TITLE_ABBRS and tokens[i+1] == \".\":\n",
    "            out.append(tokens[i] + \".\")\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        # Multi-segment abbreviation: series of (letters, \".\") repeating\n",
    "        j = i\n",
    "        parts = []\n",
    "        while j + 1 < n and re.fullmatch(r\"[A-Za-z]{1,4}\", tokens[j]) and tokens[j+1] == \".\":\n",
    "            parts.append(tokens[j] + \".\")\n",
    "            j += 2\n",
    "        if len(parts) >= 2:\n",
    "            merged = \"\".join(parts)\n",
    "            out.append(merged)\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        out.append(tokens[i])\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def tokenize_en_robust(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Wrapper around your tokenizer:\n",
    "      - stronger normalization\n",
    "      - merge abbreviations (Mr., U.S., Ph.D., e.g., i.e., U.S.A.)\n",
    "      - deglue trailing punctuation from URL/EMAIL/NUMBER\n",
    "    \"\"\"\n",
    "    # 1) normalize harder\n",
    "    text = normalize_strong(text)\n",
    "\n",
    "    # 2) your base tokenizer (assumes tokenize() is defined with TOKEN_PATTERN)\n",
    "    base_tokens = tokenize_rule_based(text)\n",
    "\n",
    "    # 3) fix glued punctuation on URL/EMAIL/NUMBER\n",
    "    tokens = []\n",
    "    for tok in base_tokens:\n",
    "        tokens.extend(_strip_trailing_punct(tok))\n",
    "\n",
    "    # 4) merge abbreviations/titles\n",
    "    tokens = _merge_abbreviations(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(tokenize_en_robust(\"That U.S.A. poster-print costs $12.40...\"))\n",
    "print(tokenize_en_robust(\"Hello, world! NLP is fun.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…Ù† Ú©Ù‡ Ù†ÙˆØ´ØªÙ‡ Ø¨ÙˆØ¯Ù… Ø§Ú©Ø«Ø± Ø­Ø§Ù„Ø§Øª Ø±Ø§ Ø¯Ø± Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù¾ÙˆØ´Ø´ Ù…ÛŒØ¯Ø§Ø¯ Ùˆ Ø¨Ø±Ø®ÛŒ ØªØµÙ…ÛŒÙ… Ù‡Ø§ÛŒÛŒ Ø¨ÙˆØ¯Ø¯ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ØªÙˆÚ©Ù† Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒÙ… Ùˆ ÛŒØ§ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒÙ… Ø¢Ù† Ø±Ø§ Ú©Ù‡ Ø·Ø¨Ù‚ Ú©Ø§Ø±Ø¨Ø±Ø¯  Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ù†ÛŒØ§Ø² Ú©Ù‡ Ø¯Ø± Ø®ÙˆØ¯ Ú©Ø¯ Ù‡Ù… Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª Ù…ÙˆØ±Ø¯ Ù…Ù‡Ù… ØªØ± Ø±Ø§ Ù„Ø­Ø§Ø¸ Ú©Ø±Ø¯Ù‡ Ùˆ ØªØµÙ…ÛŒÙ… Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù† Ú¯Ø±ÙØªÛŒÙ….\n",
    "Ø§Ù…Ø§ Ø§ÛŒØ±Ø§Ø¯Ø§Øª ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…ÛŒØªÙˆØ§Ù† Ø¨Ù‡ Ø¨Ø±Ø®ÛŒ Ø­Ø§Ù„Ø§Øª Ù„Ø¨Ù‡ Ùˆ Ø®Ø§Øµ ØªØ± Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ù…Ø§Ù†Ù†Ø¯\n",
    "<ul dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙ Ù‡Ø§ÛŒ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ø§ÛŒ ÛŒØ§ Ø­Ø§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù‡Ø± Ø­Ø±Ù Ø¨Ø¹Ø¯ Ø¢Ù† Ù†Ù‚Ø·Ù‡ Ù†ÛŒØ§Ù…Ø¯Ù‡ Ùˆ Ø¨Ù„Ú©Ù‡ Ú†Ù†Ø¯ ØªØ§ Ø­Ø±Ù Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù† Ù†Ù‚Ø·Ù‡ Ø§Ù…Ø¯Ù‡ Ø§Ù†Ø¯.. Ú©Ù‡ Ù…Ø«Ø§Ù„ Ù‡Ø§ÛŒ Ø¢Ù† Mr. DR. Ùˆ ØºÛŒØ±Ù‡ Ù‡Ø³ØªÙ†Ø¯ .</li>\n",
    "  <li>Ø¹Ù„Ø§Ù…ØªÛŒ Ø§Ú¯Ø± Ù¾Ø³ Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ùˆ ÛŒØ§ Ù„ÛŒÙ†Ú© Ø¨ÛŒØ§ÛŒØ¯ Ù…Ø§ Ø¬Ø²Ùˆ ØªÙˆÚ©Ù† Ù„ÛŒÙ†Ú© Ùˆ Ø§ÛŒÙ…ÛŒÙ„ Ø¯Ø±Ù†Ø¸Ø± Ù…ÛŒÚ¯ÛŒØ±ÛŒÙ… Ú©Ù‡ Ø²ÛŒØ§Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ù†Ù…ÛŒØ¨Ø§Ø´Ø¯.</li>\n",
    "  <li>Ø­Ø§Ù„Ø§ØªÛŒ Ú©Ù‡ Ø¹Ù„Ø§Ù…Øª Ø¯Ø±ØµØ¯ Ø¯Ø± Ù¾Ø´Øª Ø¹Ø¯Ø¯ Ø¨ÛŒØ§ÛŒØ¯ Ø±Ø§ Ø¯Ø±Ø³Øª ØªÙ‚Ø³ÛŒÙ… Ù†Ù…ÛŒÚ©Ù†ÛŒÙ….</li>\n",
    "    <li>Ø¨Ø±Ø®ÛŒ Ø§ÙˆÙ‚Ø§Øª Ù…Ø§Ù†Ù†Ø¯ Ø§Ø³Ù…Ø§Ø±Øª Ú©ÙˆØªÛŒØ´Ù† Ù‡Ø§ Ùˆ Ú†ÛŒØ²Ù‡Ø§ÛŒ Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø¯Ø± ØªØ§Ø¨Ø¹ Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ² Ù‡Ù…Ù‡ Ø§Ù†Ù‡Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÛŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø®ÛŒ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ù…Ù¾ Ø¨Ú©Ù†ÛŒÙ….</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">BPE Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª TinyStories-Farsi Ø±Ø§ Ø§Ø² HuggingFace Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    (Ù„ÛŒÙ†Ú© Ø¯ÛŒØªØ§Ø³Øª)\n",
    "</a>\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ (train) Ø¢Ù† Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ú©Ù†ÙˆÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± BPE Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø§Ù†Ø¹ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset_path = \"taesiri/TinyStories-Farsi\"\n",
    "\n",
    "# ds = load_dataset(dataset_path)\n",
    "\n",
    "dataset_path_val = os.path.join(\".\", \"validation.parquet\")\n",
    "\n",
    "df_val = pd.read_parquet(dataset_path_val)\n",
    "\n",
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path_train = os.path.join(\".\", \"train.parquet\")\n",
    "\n",
    "df_train = pd.read_parquet(dataset_path_train)\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self, num_merges=2000):\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = []\n",
    "        self.vocab = {}\n",
    "\n",
    "    def normalize_farsi(self, text: str) -> str:  # I could have used hazm instead\n",
    "        text = text.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n",
    "        text = text.replace(\"\\u200d\", \"\\u200c\")  # deZWNJ\n",
    "        return text\n",
    "\n",
    "    # Generate vocabulary from corpus (character-level tokenization)\n",
    "    def get_vocab(self, corpus):\n",
    "        vocab = Counter()\n",
    "        for word in corpus:\n",
    "            word = self.normalize_farsi(word.strip())\n",
    "            if not word:\n",
    "                continue\n",
    "            vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "        return vocab\n",
    "\n",
    "    # Compute pair frequencies\n",
    "    def get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    # Merge the most frequent pair into a new subword\n",
    "    def merge_vocab(self, pair, v_in):\n",
    "        v_out = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replace = ''.join(pair)\n",
    "        for word, freq in v_in.items():\n",
    "            new_word = word.replace(bigram, replace)\n",
    "            v_out[new_word] = freq\n",
    "        return v_out\n",
    "\n",
    "    # BPE learning (frequency based)\n",
    "    def train(self, corpus):\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.merges = []\n",
    "\n",
    "        for _ in tqdm(range(self.num_merges), desc=\"Training BPE merges\"):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        return self.merges\n",
    "\n",
    "    # Tokenizer (apply learned merges)\n",
    "    def encode_word(self, word):\n",
    "        word = self.normalize_farsi(word)\n",
    "        chars = list(word) + [\"</w>\"]\n",
    "        i = 0\n",
    "        while i < len(chars) - 1:\n",
    "            pair = (chars[i], chars[i + 1])\n",
    "            if pair in self.merges:\n",
    "                chars[i:i+2] = [''.join(pair)]\n",
    "                i = max(i - 1, 0)\n",
    "            else:\n",
    "                i += 1\n",
    "        # NOT drop </w>\n",
    "        return chars\n",
    "\n",
    "    # Tokenize entire text\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for w in text.split():\n",
    "            tokens.extend(self.encode_word(w))\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had used 7500 -> 49 tokens \n",
    "# Now i increased it to 10000\n",
    "tokenizer_bpe = BPETokenizer(num_merges=10000)\n",
    "\n",
    "train_words = []\n",
    "for text in df_train[\"Persian\"]:\n",
    "    train_words.extend(text.split())\n",
    "\n",
    "tokenizer_bpe.train(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø²ÛŒØ¨Ø§Ø³Øª\"\n",
    "tokens = tokenizer_bpe.tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "tokens = tokenizer_bpe.tokenize(sentence)\n",
    "\n",
    "print(*tokens, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Wordpiece Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø±Ø¨Ø§Ø±Ù‡ Wordpiece Tokenizer ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ø¯Ù‚ÛŒÙ‚ Ø´Ø±Ø­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø³Ù¾Ø³ Ø¢Ù† Ø±Ø§ Ø¨Ø§ BPE Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ú©Ù†ÙˆÙ† ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Wordpiece Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø§Ù†Ø¹ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrapped this WordPieceTokenizer into a class so it will be better structured than the function based on like the BPE\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "\n",
    "    def __init__(self, num_merges=2000, vocab_size=30000):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = []\n",
    "        self.vocab = {}\n",
    "        self.rank = {}\n",
    "\n",
    "    def normalize_farsi(self, text: str) -> str:\n",
    "        text = text.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n",
    "        text = text.replace(\"Ù‡\", \"Ù‡\")\n",
    "        text = text.replace(\"\\u200d\", \" \")\n",
    "        text = text.translate(str.maketrans('0123456789', 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹'))\n",
    "        text = text.replace(\"ØŒ\", \",\")  \n",
    "        text = text.replace(\"Ø›\", \";\") \n",
    "        text = text.replace(\"ØŸ\", \"?\")\n",
    "        text = text.replace(\"â€˜\", \"â€™\")\n",
    "        text = text.replace(\"â€œ\", \"â€\")\n",
    "        text = text.replace(\"â€¦\", \"...\")\n",
    "        text = text.replace(\"Ù„Ø§\", \"Ù„Ø§\")\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    # Generate vocabulary from corpus (character-level tokenization)\n",
    "    def get_vocab(self, corpus):\n",
    "        vocab = defaultdict(int)\n",
    "        for line in corpus:\n",
    "            line = self.normalize_farsi(line.strip())\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split into words within each line (so merges can cross word boundaries)\n",
    "            for word in line.split():\n",
    "                vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "        return vocab\n",
    "\n",
    "    # Compute the frequency of pairs (pairwise count of adjacent symbols)\n",
    "    def get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def get_unigram_counts(self, vocab):\n",
    "        unigrams = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            for tok in word.split():\n",
    "                unigrams[tok] += freq\n",
    "        return unigrams\n",
    "\n",
    "    # # Compute the likelihood score of merging two symbols (pair)\n",
    "    # def compute_score(self, pair, vocab):\n",
    "    #     # Counting the occurrences of the pair (Î±, Î²) and individual frequencies of Î± and Î²\n",
    "    #     bigram = ' '.join(pair)\n",
    "    #     count_pair = sum(freq for word, freq in vocab.items() if bigram in word)\n",
    "    #     count_alpha = sum(freq for word, freq in vocab.items() if pair[0] in word)\n",
    "    #     count_beta = sum(freq for word, freq in vocab.items() if pair[1] in word)\n",
    "    #     if count_alpha * count_beta == 0:  # aviding devision by zero\n",
    "    #         return 0\n",
    "    #     score = count_pair / (count_alpha * count_beta)\n",
    "    #     return score\n",
    "\n",
    "    # Merge the most likely pair into a new subword\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        bigram = ' '.join(pair)\n",
    "        replace = ''.join(pair)\n",
    "        v_out = {}\n",
    "        pattern = re.compile(r'(?<!\\S)' + re.escape(bigram) + r'(?!\\S)')\n",
    "        for word, freq in vocab.items():\n",
    "            # merge only full token pairs, not substrings\n",
    "            new_word = pattern.sub(replace, word)\n",
    "            v_out[new_word] = freq\n",
    "        return v_out\n",
    "\n",
    "    # WordPiece Learner (Merge by maximizing likelihood using the score formula)\n",
    "    def train(self, corpus):\n",
    "        # Normalize and ensure corpus is a list of full sentences\n",
    "        corpus = [self.normalize_farsi(text) for text in corpus]\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.merges = []\n",
    "\n",
    "        for _ in tqdm(range(self.num_merges), desc=\"Training WordPiece merges\"):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            unigrams = self.get_unigram_counts(self.vocab)\n",
    "\n",
    "            def score(p):\n",
    "                c_ab = pairs[p]\n",
    "                c_a = unigrams[p[0]]\n",
    "                c_b = unigrams[p[1]]\n",
    "                return 0 if c_a == 0 or c_b == 0 else c_ab / (c_a * c_b)\n",
    "\n",
    "            best_pair = max(pairs, key=score)\n",
    "            self.vocab = self.merge_vocab(best_pair, self.vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "        self.rank = {p: i for i, p in enumerate(self.merges)}\n",
    "        return self.merges\n",
    "\n",
    "    # Tokenizer (apply learned merges)\n",
    "    def encode_word(self, word):\n",
    "        word = self.normalize_farsi(word)\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "\n",
    "        while True:\n",
    "            best = None\n",
    "            best_rank = None\n",
    "            for i in range(len(symbols) - 1):\n",
    "                p = (symbols[i], symbols[i + 1])\n",
    "                r = self.rank.get(p)\n",
    "                if r is not None and (best_rank is None or r < best_rank):\n",
    "                    best = p\n",
    "                    best_rank = r\n",
    "            if best is None:\n",
    "                break\n",
    "\n",
    "            merged = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == best:\n",
    "                    merged.append(''.join(best))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    merged.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = merged\n",
    "\n",
    "        # drop </w> marker if still present\n",
    "        if symbols and symbols[-1] == \"</w>\":\n",
    "            symbols.pop()\n",
    "        return symbols\n",
    "\n",
    "    # Handle [UNK] token AS WELL\n",
    "    def tokenize(self, text):\n",
    "        text = self.normalize_farsi(text)\n",
    "        tokens = []\n",
    "        for w in text.split():\n",
    "            word_tokens = self.encode_word(w)\n",
    "            if len(word_tokens) > self.vocab_size:\n",
    "                tokens.append(\"[UNK]\")\n",
    "            else:\n",
    "                tokens.extend(word_tokens)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I changed the num_merges=8000 to 10000 so it will have bigger voac therefore on avg. bigger tokens and les number of toknes per sentence\n",
    "tokenizer_wordpiece = WordPieceTokenizer(num_merges=10000)\n",
    "\n",
    "train_words = []\n",
    "for text in df_train[\"Persian\"]:\n",
    "    train_words.extend(text.split())\n",
    "\n",
    "# training\n",
    "tokenizer_wordpiece.train(train_words)\n",
    "\n",
    "\n",
    "example1 = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø²ÛŒØ¨Ø§Ø³Øª\"\n",
    "print(tokenizer_wordpiece.tokenize(example1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_given = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "print(tokenizer_wordpiece.tokenize(example_given))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ§ Ø­Ø¯ÙˆØ¯ÛŒ Ø´Ø¨ÛŒÙ‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BPE Ø§Ø³Øª Ø§Ù„Ø¨ØªÙ‡ Ø¨Ø§ ÛŒÚ©Ø³Ø±ÛŒ ØªØºÛŒÛŒØ±Ø§Øª Ù…Ù‡Ù…ÛŒ. Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ú¯ÙˆÛŒÙ… Ú©Ù‡ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªÙˆØ³Ø· google Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯Ù‡ Ùˆ Ø¯Ø± bert Ù‡Ù… Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù…ØªÙ† Ø¨Ø§Ø² Ù†ÛŒØ³Øª Ø¨Ù‡ Ù‡Ù…ÛŒÙ† ØµÙˆØ±Øª Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø§Ù„Ø¨ØªÙ‡ ØªÙØ³ÛŒØ± Ø¢Ù† Ùˆ Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ø¯ Ø¢Ù† Ù…Ø´Ø®Øµ Ø§Ø³Øª ØªØ§ Ø­Ø¯ÛŒ. <br>\n",
    "Ø§Ù„Ú©ÙˆØ±ÛŒØªÙ… WordPiece Tokenizer Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯. Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ù…ÛŒ Ø¢ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ corpusÛŒÛŒ  Ú©Ù‡ Ø¯Ø§Ø±ÛŒÙ… vocab Ø®ÙˆØ¯ Ø±Ø§ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒØ¯Ù‡Ø¯ Ùˆ Ø¯Ø± Ø¢Ù† ØªÙˆÚ©Ù† Ú©Ù„Ù…Ù‡ Ú¯Ù…Ù†Ø§Ù… UNKN Ø±Ø§ Ù†ÛŒØ² Ù…ÛŒÚ¯Ø°Ø§Ø±Ø¯. Ù‡Ø¯Ù Ù†Ù‡Ø§ÛŒÛŒ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø¯Ø³Øª Ø§ÙˆØ±Ø¯Ù† vocab Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ Ø·ÙˆÙ„ Ø¢Ù† Ø«Ø§Ø¨Øª Ùˆ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø­Ø§Ù„ Ù¾Ø³ ØªØ¹ÛŒÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ù†Ø®Ø³Øª Ù…Ø§ ÙˆÙ‚Øª Ø¢Ù† Ø§Ø³Øª Ù…Ø§Ù†Ù†Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BPE Ù…Ø§ merge Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ù…Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ú©Ø§Ù†Ø³ Ùˆ ØªØ¹Ø¯Ø¯ Ùˆ ØªÚ©Ø±Ø§Ø± Ø¯Ùˆ Ø­Ø±Ù Ú©Ù†Ø§Ø± Ù‡Ù… Ø§Ù…Ø¯Ù† Ù…Ø·Ù„Ù‚ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…ÛŒØ´ÙˆÙ†Ø¯. Ø¨Ù„Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ù† Ø¯ÙˆØªØ§ÛŒÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± Ù‡Ù… Ø¨Ø§ Ù‡Ù… Ø§Ø¯ØºØ§Ù… Ù…ÛŒØ´ÙˆÙ†Ø¯ Ú©Ù‡ likelihood Ø§Ù† Ú©Ù‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ train Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ø±Ø§ Ù…ÛŒØ®ÙˆØ§Ù‡ÛŒÙ… maximize Ø¨Ú©Ù†ÛŒÙ….<br>\n",
    "Ø±Ø§Ø¨Ø·Ù‡ Ø§ÛŒÙ† Ù‡Ù… Ø¨Ù‡ ØµÙˆØ±Øª ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø§Ø± Ù‡Ù… Ø¢Ù…Ø¯Ù† Ø§Ù† Ø¯Ùˆ Ø­Ø±Ù ÙˆÙˆÚ©Ø¨ Ø¨Ø§Ù‡Ù… Ø¯Ø± Ù…ØªÙ† Ø¯Ø§Ø¯Ù‡ Ø§Ù…ÙˆØ²Ø´ÛŒ ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø¶Ø±Ø¨ ØªØ¹Ø¯Ø§Ø¯ Ø¢Ù…Ø¯Ù† Ù‡Ø±Ú©Ø¯Ø§Ù….<br>\n",
    "Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø´Ú©Ù„ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ø±Ø§ Ú¯Ø³ØªØ±Ø´ Ù…ÛŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø¯Ø± Ø¨ÛŒØ§ÛŒØ¯ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ù…Ø§ Ùˆ ØªØ±ØªÛŒØ¨ merge Ú©Ø±Ø¯Ù† Ù‡Ø§.\n",
    "<br>\n",
    "<br>\n",
    "Ø§Ú©Ø«Ø± Ø´Ø¨Ø§Ù‡Øª Ù‡Ø§ Ùˆ ØªÙØ§ÙˆØª Ù‡Ø§ÛŒ wordpiece tokenizer Ø±Ø§ Ø¨Ø§ BPE tokenizer Ø¨ÛŒØ§Ù† Ú©Ø±Ø¯Ù…. Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¯ÛŒÚ¯Ø± Ù‡Ù… Ù…Ø«Ø§Ù„ Ø¨Ø²Ù†Ù… Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ú©Ù„ ÙÙ„Ø³ÙÙ‡ BPE Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø§Ú©Ø³ÛŒÙ…Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø¬ÙØª Ú©Ø§Ø±Ø§Ú©ØªØ± Ù‡Ø§ÛŒ Ú©Ù†Ø§Ø± Ù‡Ù… Ø§Ø³Øª Ù…Ø§ wordpiece Ø¯Ø±ÙˆØ§Ù‚Ø¹ Ø¨Ù‡ Ù‡Ø¯Ù Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø¯Ù† maximum lielihood Ø¯ÛŒØ¯Ù† Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ Ø§Ù…ÙˆØ²Ø´ÛŒ Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯ ØªØ§ subowrd Ù‡Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙˆÚ©Ù† Ø¨ØªÙˆØ§Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ù‡Ø±Ø¯Ùˆ Ù‡Ù… greedy Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒÚ©Ù†Ù†Ø¯ Ø§Ù…Ø§ Ø¨Ø§ ÙÙ„Ø³ÙÙ‡ Ù‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØªÛŒ.<br>\n",
    "ÛŒÚ© ØªÙØ§ÙˆØª Ø¯ÛŒÚ¯Ø± Ø¢Ù† Ù‡Ø§ Ø¨Ø­Ø« Ø³Ø±Ø¹Øª Ùˆ Ø³Ø§Ø¯Ú¯ÛŒ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ Ø§ÛŒÙ† BPE tokenizer Ø§Ø³Øª Ø¯Ø±Ø­Ø§Ù„ÛŒÚ©Ù‡ wordpiece Ú©Ù†Ø¯ØªØ± Ùˆ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª.<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¢Ù† Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ù†Ø¯ÛŒØ¯Ù†Ø¯ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯(OOV, Out-of-Vocabulary)\n",
    "Ú©Ù‡ Ø¯Ø±Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BPE Ù…Ø§ Ø³Ø¹ÛŒ Ø¨Ø± Ø´Ú©Ø§Ù†Ø¯Ù† Ø¢Ù† Ú©Ù„Ù…Ù‡ Ø¯Ø§Ø±ÛŒÙ… Ø§Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ wordpeice Ù…Ø§ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ú†Ø§Ø±Ù‡ Ø§Ø² ØªÙˆÚ©Ù† Ø§Ø² Ù¾ÛŒØ´ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ UNKN Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ….<br>\n",
    "BPE Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø³Ø±Ø¹Øª Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ù…Ø§Ù†Ù†Ø¯ neural machine translation (NMT) <br>\n",
    "Ùˆ wordpiece tokenizer Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù‚Øª Ø­Ø§Ø¹Ø¸ Ø§Ù‡Ù…ÛŒØª Ø§Ø³Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ù…Ø§Ù†Ù†Ø¯ large-scale pre-trained models (BERT, ALBERT, RoBERTa).\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Tokenization Visualization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± \n",
    "<a href=\"https://tiktokenizer.vercel.app/\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    tiktokenizer\n",
    "</a>\n",
    "ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Ù‡Ø± ÛŒÚ© Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ gpt2 Ùˆ gpt4 Ùˆ Meta-Llama-3-8B Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ùˆ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ Ø±Ø§ Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± ÛŒÚ© ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†ÛŒØ¯. Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¹Ù„Øª ØªÙØ§ÙˆØª Ù†ØªÛŒØ¬Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ú†ÛŒØ³ØªØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "gpt4.o results: 42 tokens - Ø¨Ø§ÛŒØ¯ Ø¯Ù‚Øª Ú©Ø±Ø¯ Ú†ÙˆÙ† Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‚ÙˆÛŒ ØªØ± Ù¾ÛŒÚ†ÛŒØ¯Ù‡ ØªØ± Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯ÛŒØªØ§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø±Ø§ Ø¯ÛŒØ¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª subword Ù…ÛŒØªÙˆØ§Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†Ù‡<br>\n",
    "gpt2 results: 139 tokens - ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø­Ø±Ù Ø¨Ù‡ Ø­Ø±Ù ÛŒØ§ Ù‡Ù…Ø§Ù† character Ø§Ù…Ø¯Ù‡ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ ÙˆØ§Ú˜Ú©.<br>\n",
    "Meta-Llama-3-8B results: 40 tokens - ØªÙ‚Ø±ÛŒØ¨Ø§ Ù…Ø§Ù†Ù†Ø¯ gpt4.0 ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø¨Ù‡ ÙˆØ§Ø²Ú¯ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†Ø¯ Ø¬Ù…Ù„Ù‡ Ø±Ø§.<br>\n",
    "<br>\n",
    "Ø¨Ù†Ø¯Ù‡ Ù¾Ø³ Ø§Ø² ØªØ­Ù‚ÛŒÙ‚ Ø±Ø§Ø¬Ø¨ 3 Ù…Ø¯Ù„ Ù†Ø§Ù… Ø¨Ø±Ø¯Ù‡ Ø´Ø¯Ù‡ Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯Ù… Ú©Ù‡ gpt2 Ú©Ù‡ ØªÙ‚Ø±ÛŒØ¨Ø§ Ø§ØµÙ„Ø§ Ø¨Ø±ÙˆÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø®Ø§Ø·Ø± Ú†ÙˆÙ† Ø²Ø¨Ø§Ù† Ø±Ø§ Ù†Ù…ÛŒØ´Ù†Ø§Ø³Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±Ù Ø¨Ù‡ Ø­Ø±Ù ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø´ÛŒÙ† Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ø¯.<br>\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ gpt4 Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ø¯Ù„ lamma3 Ø¨Ø§ 8B Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø§ÛŒÙ† Ø¯Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª multilingual train Ø´Ø¯Ù‡ Ø§Ù†Ø¯. Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ú†ÙˆÙ† Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ø§Ù…ÙˆØ²Ø´ Ø®ÙˆØ¯ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ù‡Ù… Ø¯ÛŒØ¯Ù‡ Ø§Ù†Ø¯ Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ subword Ú©Ù‡ Ø¯Ø± vocab Ø®ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ø¨Ø´Ú©Ø§Ù†Ù†Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ùˆ Ø¨Ù‡ØªØ± Ø§Ø² Ù…Ø¯Ù„ gpt2 Ø¹Ù…Ù„ Ø¨Ú©Ù†Ù†Ø¯.<br>\n",
    "Ø¯Ø± Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ø¨Ù‡ØªØ± Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†ÛŒØ² ØªÙØ§ÙˆØª Ù‡Ø§ÛŒÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ù‡ Ù…Ø¯Ù„ gpt4 Ø¨Ù‡ ØµÙˆØ±Øª Ø±ÙˆØ´ BPE Tokenizer Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯.<br>\n",
    "Ø§Ù…Ø§ Ù…Ø¯Ù„ Meta-Llama-3-8B Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Wordpiece Tokenizer Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†Ø¯.<br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø³ÙˆÙ… - <span dir=\"ltr\">N-gram Language Modeling</span> (55)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø§ N-gram Language Modeling Ùˆ Ø¢Ù† Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²ÛŒØ¯. Ø³Ù¾Ø³ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± perplexity Ùˆ Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ smoothing Ùˆ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Data cleaning & Tokenization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "- Ú©Ù‡ Ø¯Ø± Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ù†ÛŒØ² Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ - Ø±Ø§ Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ (train) Ø¢Ù† Ø±Ø§ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¢Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. (Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¨Ù‡ Ú†Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†Ø¯.)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² BPE Tokenizer Ú©Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯Ø§Ø¯Ù‡â€ŒØ§ÛŒØ¯ØŒ Ø¯Ø§Ø¯Ú¯Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØŒ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b>\n",
    "<br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙ…ÛŒØ²Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "<br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "<br>\n",
    "- ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "# import hazm   we could have used hazm as well\n",
    "\n",
    "# dataset_path_in_huggingface = \"taesiri/TinyStories-Farsi\"\n",
    "# ds = load_dataset(dataset_path_in_huggingface)\n",
    "# df_train = dataset[\"train\"].to_pandas()\n",
    "# df_val = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "file_path_train = os.path.join(\".\", \"train.parquet\")\n",
    "file_path_val = os.path.join(\".\", \"validation.parquet\")\n",
    "\n",
    "df_train = pd.read_parquet(file_path_train)\n",
    "df_val = pd.read_parquet(file_path_val)\n",
    "\n",
    "df_train = df_train[\"Persian\"]\n",
    "df_val = df_val[\"Persian\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([df_train[i] for i in range(20)]))  # to see some data in persian so i could better decide the preprocessing i have to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "import re\n",
    "\n",
    "# Persian char maps\n",
    "ARABIC_TO_PERSIAN = {\n",
    "    \"ÙŠ\": \"ÛŒ\",\n",
    "    \"Ùƒ\": \"Ú©\",\n",
    "    \"Ø©\": \"Ù‡\",\n",
    "    \"Û€\": \"Ù‡\"\n",
    "}\n",
    "\n",
    "DIACRITICS_PATTERN = re.compile(r\"[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]+\")\n",
    "\n",
    "def replace_chars(text, mapping):\n",
    "    for src, target in mapping.items():\n",
    "        text = text.replace(src, target)\n",
    "    return text\n",
    "\n",
    "def normalize_zwnj(text):\n",
    "    # normalize Arabic ZWJ to Persian ZWNJ\n",
    "    text = text.replace(\"\\u200d\", \"\\u200c\")\n",
    "\n",
    "    # common Persian patterns\n",
    "    text = re.sub(r\"\\bÙ…ÛŒ\\s+\", \"Ù…ÛŒâ€Œ\", text)\n",
    "    text = re.sub(r\"\\bÙ†Ù…ÛŒ\\s+\", \"Ù†Ù…ÛŒâ€Œ\", text)\n",
    "\n",
    "    # Plural Ù‡Ø§\n",
    "    text = re.sub(r\"\\s+Ù‡Ø§\\b\", \"â€ŒÙ‡Ø§\", text)\n",
    "\n",
    "    # Possessives\n",
    "    text = re.sub(r\"\\s+(Ø§Ù…|Ø§Øª|Ø§Ø´|Ø§ÛŒÙ…|Ø§ÛŒØ¯|Ø§Ù†Ø¯)\\b\", r\"â€Œ\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    # unify punctuation to Persian-friendly\n",
    "    text = text.replace(\";\", \"Ø›\")\n",
    "    text = text.replace(\",\", \"ØŒ\") if \"ØŒ\" in text else text.replace(\",\", \"ØŒ\")\n",
    "    text = text.replace(\"?\", \"ØŸ\")\n",
    "\n",
    "    # normalize ellipsis\n",
    "    text = text.replace(\"...\", \"â€¦\")\n",
    "\n",
    "    # normalize quotes\n",
    "    text = text.replace(\"â€œ\", \"Â«\").replace(\"â€\", \"Â»\")\n",
    "    text = text.replace(\"\\\"\", \"Â»\")  # fallback for quotes\n",
    "    text = re.sub(r\"[Â«Â»]+\", lambda m: \"Â«\" if m.group()==m.group()[0] else \"Â»\", text)\n",
    "\n",
    "    # remove spaces before punctuation\n",
    "    text = re.sub(r\"\\s+([ØŒØ›:!ØŸ.])\", r\"\\1\", text)\n",
    "\n",
    "    # enforce one space after punctuation\n",
    "    text = re.sub(r\"([ØŒØ›:!ØŸ.])([^ \\nÂ»])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_digits(text):\n",
    "    # convert English digits 0-9 â†’ Persian digits\n",
    "    return text.translate(str.maketrans(\"0123456789\", \"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹\"))\n",
    "\n",
    "def cleanup_whitespace(text):\n",
    "    # collapse multiple spaces + trim\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    return DIACRITICS_PATTERN.sub(\"\", text)\n",
    "\n",
    "def preprocess_farsi(\n",
    "    text: str,\n",
    "    *,\n",
    "    normalize_chars=True,\n",
    "    normalize_zwnj_flag=True,\n",
    "    normalize_punc=True,\n",
    "    unify_digits=True,\n",
    "    strip_diacritics=True,\n",
    "    clean_space=True,\n",
    "):\n",
    "    if normalize_chars:\n",
    "        text = replace_chars(text, ARABIC_TO_PERSIAN)\n",
    "\n",
    "    if strip_diacritics:\n",
    "        text = remove_diacritics(text)\n",
    "\n",
    "    if normalize_zwnj_flag:\n",
    "        text = normalize_zwnj(text)\n",
    "\n",
    "    if normalize_punc:\n",
    "        text = normalize_punctuation(text)\n",
    "\n",
    "    if unify_digits:\n",
    "        text = normalize_digits(text)\n",
    "\n",
    "    if clean_space:\n",
    "        text = cleanup_whitespace(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = df_train.apply(preprocess_farsi)\n",
    "\n",
    "df_val_preprocessed = df_val.apply(preprocess_farsi)\n",
    "\n",
    "print(\"We did the Preprocessing on both the validation and train set.\")\n",
    "print(df_train_preprocessed.head())\n",
    "print(df_val_preprocessed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(num_merges=10000)\n",
    "\n",
    "train_words = []\n",
    "for text in df_train_preprocessed:\n",
    "    train_words.extend(text.split())\n",
    "\n",
    "tokenizer.train(train_words)\n",
    "print(\"Tokenizer training done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exmaple = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "\n",
    "tokenizer.tokenize(exmaple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now doing the tokenization of the rows in the preprocessed dataframe\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_train_tokenized = df_train_preprocessed.progress_apply(lambda s: tokenizer.tokenize(s))\n",
    "df_val_tokenized = df_val_preprocessed.progress_apply(lambda s: tokenizer.tokenize(s))\n",
    "\n",
    "print(\"Tokenization done!\")\n",
    "\n",
    "print(df_train_tokenized.head(3))\n",
    "print(df_val_tokenized.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "Ú†Ù†Ø¯ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù… Ù…Ø®ØµÙˆØµØ§ Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª Ùˆ Ø¨Ø§ Ø¹Ø±Ø¨ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ±Ú©ÛŒØ¨ Ø´ÙˆØ¯ Ùˆ Ø¨Ù‡ Ø¯Ù„Ø§ÛŒÙ„ Ù†ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ù‡Ø§ Ùˆ Ø®Ø§ØµÛŒØª Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ù†Ø¬ÙˆÙ‡ Ù†Ú¯Ø§Ø±Ø´ Ø¢Ù† Ø®ÛŒÙ„ÛŒ Ù…Ø±Ø­Ù„Ù‡ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù‡Ù…ÛŒØª ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ø¯Ø§Ø±Ø¯<br>\n",
    "Ø¨Ù‡ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ Ø¯Ø± Pre-Process Ø­ØªÙ…Ø§ Ø¨Ø§ÛŒØ¯ Ø±Ø¹Ø§ÛŒØª Ú©Ù†Ù… Ø§Ø´Ø§Ø±Ù‡ Ù…ÛŒÚ©Ù†Ù…:<br>\n",
    "1. Ù†ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ù‡Ø§ Ø¯Ø± ÙØ§Ø±Ø³ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù‡Ù…Ù‡ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ú©Ù†Ù… Ùˆ Ø¬Ø§Ù‡Ø§ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù†ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ø±Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± Ù…Ø®ØµÙˆØµ Ø®ÙˆØ¯ Ù¾Ø± Ú©Ù†Ù….<br>\n",
    "2. Ø¨Ø±Ø®ÛŒ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÛŒ Ù…Ø§Ù†Ù†Ø¯ ÛŒ Ú©  Ùˆ Ú†Ù†Ø¯ Ø­Ø±Ù Ø¯ÛŒÚ¯Ø± Ú©Ù‡ Ø¹Ø±Ø¨ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ø§Ù…Ø§ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ù†Ø¯.<br>\n",
    "3. Ø¨Ø§ÛŒØ¯ Ø­Ø±ÙˆÙ ØµØ¯Ø§Ø¯Ø§Ø± Ø¨Ø§Ù„Ø§ÛŒ Ú©Ù„Ø§Ù…Øª Ø±Ø§ Ø­Ø°Ù Ú©Ù†Ù… Ø²ÛŒØ±Ø§ Ø¨Ø§Ø² ÙØ§Ø±Ø³ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø¹Ø±Ø¨ÛŒ Ù†Ø¨Ø§ÛŒØ¯ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø¢Ù† Ø¨Ø§Ø´Ø¯ Ùˆ Ú†ÙˆÙ† Ø¯Ø± Ø§Ú©Ø«Ø± Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ Ù…Ø§ Ø­Ø±ÙˆÙ ØµØ¯Ø§Ø¯Ø§Ø± Ø±Ø§ Ù†Ø¯Ø§Ø±ÛŒÙ….<br>\n",
    "4. Ø´Ù…Ø§Ø±Ù‡ Ù‡Ø§ÛŒ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ø±Ù…Ø§Ù„ Ú©Ù†Ù…. Ø§Ú¯Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ùˆ ÛŒØ§ Ø¹Ø±Ø¨ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯ Ø­Ø°Ù Ú©Ù†Ù….<br>\n",
    "5. Ø¨Ø±Ø®ÛŒ Ø¹Ù„Ø§Ø¹Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ù†Ø³ÛŒØ³ØªÙ†Øª Ú©Ù†Ù… Ùˆ Ø¹Ù…Ù„Ø§ Ú†Ù†Ø¯ ØªØ§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù†ÙˆØ¹ ÙˆØ§Ø­Ø¯ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ù…<br>\n",
    "6. Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù‡Ø§ Ø±Ø§ Ù‡Ù†Ø¯Ù„ Ú©Ù†ÛŒÙ….<br>\n",
    "7. Ø¨Ø§ÛŒØ¯ <<>> Ùˆ \"\" Ø±Ø§ ÛŒÚ©ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒÙ… <br>\n",
    "8. Ø§ÛŒØ§ Ú©Ù„Ù…Ø§Øª ØºÛŒØ± Ø±Ø³Ù…ÛŒ Ø±Ø§ Ú©Ù†Ø§Ø± Ú©Ù„Ù…Ø§Øª Ø±Ø³Ù…ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒÙ… ÛŒØ§ Ø®ÛŒØ± Ú©Ù‡ Ø¨Ù†Ø¸Ø± Ø·Ø¨Ù‚ Ø§ÛŒÙ†Ú©Ù‡ Ø¯ÛŒØªØ§ Ø¯Ø§Ø³ØªØ§Ù† Ø§Ø³Øª Ø¨Ù‡ØªØ± Ø§Ø³Øª Ù†Ú¯Ù‡ Ø¯Ø§Ø±Ù….<br>\n",
    "9. Ù…ÙˆØ±Ø¯ Ø¯ÛŒÚ¯Ø± Ù‡Ù… Ø±Ø§Ø¬Ø¨ Ú†Ù†Ø¯ ØªØ§ Ø¹Ù„Ø§Ù…Øª ØªØ¹Ø¬Ø¨ ÛŒØ§ Ø³ÙˆØ§Ù„ Ù¾Ø´Øª Ù‡Ù… Ø¨ÛŒØ§ÛŒÙ†Ø¯. Ù…Ø¹Ù…ÙˆÙ„Ø§ Ù…Ø§ Ø¨Ù‡ØªØ± Ø§Ø³Øª ÛŒÚ©ÛŒ Ø¢Ù† Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒÙ….\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ N-gram</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ N-gram Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ù‚Ø¨Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ \n",
    "<span dir=\"ltr\"> 2-gram, 4-gram, 8-gram</span>\n",
    "Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ 100 ØªÙˆÚ©Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ùˆ Ú©ÛŒÙÛŒØª Ù…ØªÙˆÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ Ù‡Ù… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªÙØ§ÙˆØª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² Ù†Ø¸Ø± Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ Ùˆ Ø±ÙˆØ§Ù†ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ N-gram Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§ÙØªÙ‡\n",
    "<br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ 100 ØªÙˆÚ©Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "# The smoothing is only implemented for the N=4 \n",
    "\n",
    "class NGramLM:\n",
    "\n",
    "    LAMBDA_BACKOFF = 0.4 \n",
    "    INTERPOLATION_WEIGHTS = (0.4, 0.3, 0.2, 0.1) \n",
    "    \n",
    "    NONE_SMOOTHING = 'none'\n",
    "    LAPLACE = 'laplace'\n",
    "    BACKOFF = \"backoff\"\n",
    "    INTERPOLATION = \"interpolation\"\n",
    "    \n",
    "    def __init__(self, n: int = 4, smoothing: str = \"none\"):\n",
    "        \"\"\"Initialize N-gram model with order n.\"\"\"\n",
    "        self.n = n  # the order of the LM (bigram, trigram, ...)\n",
    "        self.smoothing = smoothing\n",
    "        self.vocab = set()\n",
    "        \n",
    "        # Count dictionaries\n",
    "        self.ngram_counts = {}\n",
    "        self.context_counts = {}\n",
    "\n",
    "        # Probability dictionary\n",
    "        self.probabilities = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.bos_token = \"<bos>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "\n",
    "    def build_vocab(self, tokenized_texts):\n",
    "        \"\"\"Populate vocabulary set from tokenized train data.\"\"\"\n",
    "        for sentence in tokenized_texts:\n",
    "            for token in sentence:\n",
    "                self.vocab.add(token)\n",
    "        # ensure special tokens are present\n",
    "        self.vocab.update([self.bos_token, self.eos_token, self.unk_token])\n",
    "        print(f\"Vocabulary size: {len(self.vocab)} tokens\")\n",
    "        return self.vocab\n",
    "        # Add (nâˆ’1) BOS tokens at the start and one EOS at the end\n",
    "\n",
    "    def count_ngrams(self, tokenized_texts):\n",
    "        \"\"\"Count all N-grams and (Nâˆ’1)-gram contexts for any n.\"\"\"\n",
    "        for sentence in tokenized_texts:\n",
    "            # Add (nâˆ’1) BOS and one EOS token\n",
    "            sentence = [self.bos_token] * (self.n - 1) + sentence + [self.eos_token]\n",
    "            \n",
    "            # Slide a window of size n across the sentence\n",
    "            for i in range(len(sentence) - self.n + 1):\n",
    "                ngram = tuple(sentence[i:i + self.n])\n",
    "                context = tuple(sentence[i:i + self.n - 1])\n",
    "\n",
    "                # Update n-gram counts\n",
    "                self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1\n",
    "\n",
    "                # Update context counts\n",
    "                self.context_counts[context] = self.context_counts.get(context, 0) + 1\n",
    "\n",
    "    def calculate_probabilities(self):\n",
    "        \"\"\"Convert counts to probabilities.\n",
    "        - If n != 4 and smoothing != 'none', fall back to 'none'.\n",
    "        - For n == 4:\n",
    "            * laplace:    (c4 + 1) / (C3 + |V|)\n",
    "            * interpolation: 0.4*P4 + 0.3*P3 + 0.2*P2 + 0.1*P1\n",
    "            * backoff: P4 else 0.4*P3 else 0.4**2*P2 else 0.4**3*P1\n",
    "        \"\"\"\n",
    "        # reset before recomputing\n",
    "        self.probabilities = {}\n",
    "\n",
    "        use_plain = (self.smoothing == \"none\") or (self.n != 4)\n",
    "        if use_plain:\n",
    "            probs = self.probabilities\n",
    "            ctx = self.context_counts\n",
    "            for ngram, c in self.ngram_counts.items():\n",
    "                C = ctx.get(ngram[:-1], 0)\n",
    "                probs[ngram] = (c / C) if C > 0 else 0.0\n",
    "            return probs\n",
    "\n",
    "        # smoothing != none and n == 4\n",
    "        # 1) aggregate lower orders in one pass\n",
    "        tri_counts, tri_ctx = {}, {}\n",
    "        bi_counts, bi_ctx = {}, {}\n",
    "        uni_counts = {}\n",
    "        total_unigrams = 0\n",
    "\n",
    "        for (h3, h2, h1, w), c4 in self.ngram_counts.items():\n",
    "            k3 = (h2, h1, w); tri_counts[k3] = tri_counts.get(k3, 0) + c4\n",
    "            c3k = (h2, h1);   tri_ctx[c3k]     = tri_ctx.get(c3k, 0) + c4\n",
    "\n",
    "            k2 = (h1, w);     bi_counts[k2]    = bi_counts.get(k2, 0) + c4\n",
    "            c2k = (h1,);      bi_ctx[c2k]      = bi_ctx.get(c2k, 0) + c4\n",
    "\n",
    "            uni_counts[w]     = uni_counts.get(w, 0) + c4\n",
    "            total_unigrams   += c4\n",
    "\n",
    "        # 2) precompute MLE tables P4,P3,P2,P1 (dicts)\n",
    "        P4 = {}\n",
    "        P3 = {}\n",
    "        P2 = {}\n",
    "        P1 = {}\n",
    "\n",
    "        ctx4 = self.context_counts\n",
    "        for (h3, h2, h1, w), c4 in self.ngram_counts.items():\n",
    "            C3 = ctx4.get((h3, h2, h1), 0)\n",
    "            P4[(h3, h2, h1, w)] = (c4 / C3) if C3 > 0 else 0.0\n",
    "\n",
    "        for (h2, h1, w), c3 in tri_counts.items():\n",
    "            C2 = tri_ctx.get((h2, h1), 0)\n",
    "            P3[(h2, h1, w)] = (c3 / C2) if C2 > 0 else 0.0\n",
    "\n",
    "        for (h1, w), c2 in bi_counts.items():\n",
    "            C1 = bi_ctx.get((h1,), 0)\n",
    "            P2[(h1, w)] = (c2 / C1) if C1 > 0 else 0.0\n",
    "\n",
    "        inv_total = (1.0 / total_unigrams) if total_unigrams > 0 else 0.0\n",
    "        if inv_total:\n",
    "            for w, c1 in uni_counts.items():\n",
    "                P1[w] = c1 * inv_total\n",
    "        else:\n",
    "            # keep empty -> zeros\n",
    "            pass\n",
    "\n",
    "        probs = self.probabilities\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        if self.smoothing == NGramLM.LAPLACE:\n",
    "            # only 4-gram add-1 as requested\n",
    "            for (h3, h2, h1, w), c4 in self.ngram_counts.items():\n",
    "                C3 = ctx4.get((h3, h2, h1), 0)\n",
    "                probs[(h3, h2, h1, w)] = ((c4 + 1) / (C3 + vocab_size)) if C3 > 0 else (1 / max(vocab_size, 1))\n",
    "            return probs\n",
    "\n",
    "        if self.smoothing == NGramLM.INTERPOLATION:\n",
    "            # weights fixed: 0.4, 0.3, 0.2, 0.1\n",
    "            for (h3, h2, h1, w) in self.ngram_counts.keys():\n",
    "                p = (self.INTERPOLATION_WEIGHTS[0] * P4.get((h3, h2, h1, w), 0.0) +\n",
    "                    self.INTERPOLATION_WEIGHTS[1] * P3.get((h2, h1, w), 0.0) +\n",
    "                    self.INTERPOLATION_WEIGHTS[2] * P2.get((h1, w), 0.0) +\n",
    "                    self.INTERPOLATION_WEIGHTS[3] * P1.get(w, 0.0))\n",
    "                probs[(h3, h2, h1, w)] = p\n",
    "            return probs\n",
    "\n",
    "        if self.smoothing == NGramLM.BACKOFF:\n",
    "            # geometric factor 0.4 per backoff step\n",
    "            for (h3, h2, h1, w) in self.ngram_counts.keys():\n",
    "                p4 = P4.get((h3, h2, h1, w), 0.0)\n",
    "                if p4 > 0.0:\n",
    "                    probs[(h3, h2, h1, w)] = p4\n",
    "                    continue\n",
    "                p3 = P3.get((h2, h1, w), 0.0)\n",
    "                if p3 > 0.0:\n",
    "                    probs[(h3, h2, h1, w)] = NGramLM.LAMBDA_BACKOFF * p3\n",
    "                    continue\n",
    "                p2 = P2.get((h1, w), 0.0)\n",
    "                if p2 > 0.0:\n",
    "                    probs[(h3, h2, h1, w)] = (NGramLM.LAMBDA_BACKOFF ** 2) * p2\n",
    "                    continue\n",
    "                p1 = P1.get(w, 0.0)\n",
    "                probs[(h3, h2, h1, w)] = (NGramLM.LAMBDA_BACKOFF ** 3) * p1\n",
    "            return probs\n",
    "\n",
    "        raise ValueError(f\"Smoothing method '{self.smoothing}' is not recognized.\")\n",
    "\n",
    "\n",
    "    def add_special_tokens(self, tokens):\n",
    "        \"\"\"Insert (nâˆ’1) BOS tokens and one EOS token around a tokenized sentence.\"\"\"\n",
    "        self.vocab.update([self.bos_token, self.eos_token])\n",
    "        \n",
    "        return [self.bos_token] * (self.n - 1) + tokens + [self.eos_token]\n",
    "\n",
    "    def predict_next(self, context, use_sampling_weighted=False, temperature: float = 1.0):\n",
    "        \"\"\"Predict the next token given the last N-1 tokens.\"\"\"\n",
    "\n",
    "        if not isinstance(context, tuple):\n",
    "            context = tuple(context)\n",
    "        context = context[-(self.n - 1):]\n",
    "\n",
    "        # gather candidates that match this exact context at order n\n",
    "        candidates = {\n",
    "            ngram[-1]: prob\n",
    "            for ngram, prob in self.probabilities.items()\n",
    "            if ngram[:-1] == context\n",
    "        }\n",
    "\n",
    "        if not candidates:\n",
    "            return self.unk_token\n",
    "\n",
    "        # deterministic path (argmax)    \n",
    "        if not use_sampling_weighted:\n",
    "            return max(candidates, key=candidates.get)\n",
    "\n",
    "        # sampling path with temperature\n",
    "        tokens, probs = zip(*candidates.items())\n",
    "        probs = [p if p > 0.0 else 1e-9 for p in probs]\n",
    "\n",
    "        if temperature is None or temperature <= 0:\n",
    "            # fall back to greedy if bad temperature\n",
    "            return max(candidates, key=candidates.get)\n",
    "\n",
    "        scaled = [p ** (1.0 / temperature) for p in probs]\n",
    "        total = sum(scaled)\n",
    "        scaled = [p / total for p in scaled]\n",
    "\n",
    "        return random.choices(tokens, scaled)[0]\n",
    "    \n",
    "    def detokenize_bpe(self, tokens):\n",
    "        words = []\n",
    "        current = []\n",
    "        for t in tokens:\n",
    "            if t == \"</w>\":\n",
    "                if current:\n",
    "                    words.append(\"\".join(current))\n",
    "                    current = []\n",
    "            elif t in (self.bos_token, self.eos_token):\n",
    "                continue\n",
    "            else:\n",
    "                current.append(t)\n",
    "        if current:\n",
    "            words.append(\"\".join(current))\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "    def generate(self, length=30, max_in_len=True, use_sampling_weighted=False, temperature: float = 1.0):\n",
    "        context = [self.bos_token] * (self.n - 1)\n",
    "        generated = []\n",
    "\n",
    "        for _ in range(length):\n",
    "            next_token = self.predict_next(\n",
    "                context,\n",
    "                use_sampling_weighted=use_sampling_weighted,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "\n",
    "            if next_token == self.eos_token and max_in_len:\n",
    "                break\n",
    "\n",
    "            generated.append(next_token)\n",
    "            context = (context + [next_token])[-(self.n - 1):]\n",
    "\n",
    "        # if output looks like BPE â†’ detokenize, else just join\n",
    "        if any(tok == \"</w>\" for tok in generated):\n",
    "            return self.detokenize_bpe(generated)\n",
    "        return \" \".join(tok for tok in generated if tok not in (self.bos_token, self.eos_token))\n",
    "\n",
    "\n",
    "\n",
    "    def get_ngram_probability(self, ngram: tuple):\n",
    "        \"\"\"Return the probability of an N-gram.\n",
    "\n",
    "        Prefers precomputed probabilities (from calculate_probabilities).\n",
    "        Falls back to MLE from counts if available. Otherwise returns 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # Coerce to tuple and validate length\n",
    "        if not isinstance(ngram, tuple):\n",
    "            ngram = tuple(ngram)\n",
    "        if len(ngram) != self.n:\n",
    "            # For now we only support exact order-n queries\n",
    "            return 0.0\n",
    "\n",
    "        # 1) If precomputed, return it\n",
    "        if ngram in self.probabilities:\n",
    "            return self.probabilities[ngram]\n",
    "\n",
    "        # 2) Fallback: compute MLE from counts if we can\n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        if count == 0:\n",
    "            return 0.0\n",
    "\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return count / context_count\n",
    "\n",
    "    def perplexity(self, tokenized_texts):\n",
    "        \"\"\"Compute perplexity on tokenized sentences (order-n model).\"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_predicted = 0\n",
    "\n",
    "        for sentence in tokenized_texts:\n",
    "            # pad with (n-1) BOS and one EOS\n",
    "            sent = [self.bos_token] * (self.n - 1) + sentence + [self.eos_token]\n",
    "\n",
    "            # slide an n-gram window over the sentence\n",
    "            for i in range(self.n - 1, len(sent)):\n",
    "                ngram = tuple(sent[i - self.n + 1 : i + 1])\n",
    "\n",
    "                p = self.get_ngram_probability(ngram)\n",
    "                if p <= 0.0:\n",
    "                    return float(\"inf\")  # no smoothing â†’ zero prob â†’ infinite perplexity\n",
    "\n",
    "                total_log_prob += math.log(p)\n",
    "                total_predicted += 1\n",
    "\n",
    "        if total_predicted == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        # cross-entropy (base e), then perplexity\n",
    "        avg_neg_log = - total_log_prob / total_predicted\n",
    "        return math.exp(avg_neg_log)\n",
    "    \n",
    "    def _iter_tokenized(self, tokenized_texts, col: str | None = None):\n",
    "        \"\"\"Internal: yield token lists from iterable/Series/DataFrame.\"\"\"\n",
    "        # pandas Series of token lists\n",
    "        try:\n",
    "            import pandas as pd  # local import to avoid hard dependency at import time\n",
    "            is_series = isinstance(tokenized_texts, pd.Series)\n",
    "            is_df = isinstance(tokenized_texts, pd.DataFrame)\n",
    "        except Exception:\n",
    "            is_series = False\n",
    "            is_df = False\n",
    "\n",
    "        if is_series:\n",
    "            for row in tokenized_texts:\n",
    "                yield row\n",
    "            return\n",
    "\n",
    "        if is_df:\n",
    "            if not col:\n",
    "                raise ValueError(\"When passing a DataFrame, you must provide the 'col' name containing token lists.\")\n",
    "            for row in tokenized_texts[col]:\n",
    "                yield row\n",
    "            return\n",
    "\n",
    "        # Generic iterable of token lists\n",
    "        for row in tokenized_texts:\n",
    "            yield row\n",
    "    \n",
    "    def train(self, tokenized_texts, *, col: str | None = None):\n",
    "        \"\"\"One-shot training pipeline.\"\"\"\n",
    "        iterator = list(self._iter_tokenized(tokenized_texts, col=col))\n",
    "\n",
    "        self.build_vocab(iterator)\n",
    "        self.count_ngrams(iterator)\n",
    "        self.calculate_probabilities()\n",
    "        return self\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model parameters for later use.\"\"\"\n",
    "        dirpath = os.path.dirname(path)\n",
    "        if dirpath:\n",
    "            os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "        model_data = {\n",
    "            \"n\": self.n,\n",
    "            \"vocab\": list(self.vocab),\n",
    "            \"ngram_counts\": self.ngram_counts,\n",
    "            \"context_counts\": self.context_counts,\n",
    "            \"probabilities\": self.probabilities,\n",
    "        }\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved successfully to {path}\")\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load saved model parameters.\"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.n = model_data[\"n\"]\n",
    "        self.vocab = set(model_data[\"vocab\"])\n",
    "        self.ngram_counts = model_data[\"ngram_counts\"]\n",
    "        self.context_counts = model_data[\"context_counts\"]\n",
    "        self.probabilities = model_data[\"probabilities\"]\n",
    "\n",
    "        print(f\"Model loaded successfully from {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Gram\n",
    "lm_2gram = NGramLM(n=2)\n",
    "\n",
    "lm_2gram.train(df_train_tokenized)\n",
    "\n",
    "save_2gram_path = os.path.join(\".\", \"saved_models\", \"lm_2gram_model.pkl\")\n",
    "lm_2gram.save(save_2gram_path)\n",
    "generated_text = lm_2gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\nGenerated text (2-gram model):\\n\")\n",
    "print(textwrap.fill(clean, width=80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Gram\n",
    "lm_4gram = NGramLM(n=4)\n",
    "\n",
    "lm_4gram.train(df_train_tokenized)\n",
    "\n",
    "save_4gram_path = os.path.join(\".\", \"saved_models\", \"lm_4gram_model.pkl\")\n",
    "lm_4gram.save(save_4gram_path)                                           \n",
    "generated_text = lm_4gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\nGenerated text (4-gram model):\\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8Gram\n",
    "lm_8gram = NGramLM(n=8)\n",
    "\n",
    "lm_8gram.train(df_train_tokenized)\n",
    "\n",
    "save_8gram_path = os.path.join(\".\", \"saved_models\", \"lm_8gram_model.pkl\")\n",
    "lm_8gram.save(save_8gram_path)\n",
    "generated_text = lm_8gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\nGenerated text (8-gram model):\\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "Ø¯Ø± 2gram Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¨ÛŒ Ù…Ø§ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ø§ÛŒ Ù‡Ø§ÛŒ Ù¾Ø´Øª Ù‡Ù… Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ ØªØ§ Ø­Ø¯ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø§Ù…Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ù„ÛŒ Ú©Ø§Ù†ØªÚ©Ø³Øª Ø¬Ù…Ù„Ù‡ Ùˆ \n",
    "Ú¯Ø±Ø§Ù…Ø± Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ú©Ù„ÛŒ Ùˆ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª.<br>\n",
    "Ø¯Ø± Ø­Ø§Ù„Øª 8gram Ù‡Ù… Ø¬Ù…Ù„Ù‡ ÛŒØ§ Ø²ÙˆØ¯ ØªÙ…Ø§Ù… Ù…ÛŒØ´ÙˆØ¯ ÛŒØ§ Ø¨Ø¹Ù„Øª Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù† ØªÙ…Ø§Ù…ÛŒ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† generate Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯. Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ù‡Ù… Ø¬Ù…Ù„Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø§Ø®Ù„ Ù…ØªÙ† Ø§Ù…ÙˆØ²Ø´ÛŒ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒÚ©Ù†Ø¯.<br>\n",
    "Ø§Ù…Ø§ Ø¨Ù‡ Ù†Ø³Ø¨Øª 4gram Ø¹Ù„Ù…Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ùˆ Ù…ØªÙˆØ§Ø²Ù† ØªØ±ÛŒ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ùˆ Ù†ÙˆØ´ØªÙ† Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø±Ø¯. Ù‡Ù… context Ù…Ù†Ø§Ø³Ø¨ ØªØ±ÛŒ Ø¯ÛŒØ¯Ù‡ Ø§Ø³Øª Ùˆ Ù‡Ù…Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ ØµÙØ± Ú©Ù…ØªØ± Ø¨Ø±Ø®ÙˆØ±Ø¯ Ù…ÛŒÚ©Ù†Ø¯.<br> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù…Ø¹ÛŒØ§Ø± Perplexity</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "Ø¯Ø§Ø¯Ú¯Ø§Ù† validation ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² N-gram Ù‡Ø§ÛŒ Ø®ÙˆØ¯ØŒ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¨Ú¯ÙˆÛŒÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ validation\n",
    "<br>\n",
    "- Ù†ØªÛŒØ¬Ù‡ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø¯Ø± Ù‡Ø± N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have previously held out the validation data and i have preprocessed and tokenized it and its in the df_val_tokenized\n",
    "\n",
    "# row by row\n",
    "perplexities_2gram = df_val_tokenized.apply(lambda x: lm_2gram.perplexity([x]))  \n",
    "average_perplexity_2gram = perplexities_2gram.mean()\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_2gram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities_4gram = df_val_tokenized.apply(lambda x: lm_4gram.perplexity([x])) \n",
    "average_perplexity_4gram = perplexities_4gram.mean()\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_4gram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities_8gram = df_val_tokenized.apply(lambda x: lm_8gram.perplexity([x]))  \n",
    "average_perplexity_8gram = perplexities_8gram.mean()\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_8gram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ú†Ù‡Ø§Ø± Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ù†ÛŒØ¯ Ú©Ø¯Ø§Ù… Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ù…Ø­ØªÙ…Ù„â€ŒØªØ±Ù†Ø¯ Ú©Ù‡ ØªÙˆØ³Ø· ÛŒÚ© <span dir=\"ltr\">4-gram</span> Ú©Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡â€ŒØ§Ø³ØªØŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø§ÙˆÙ„ = Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¯Ø± Ù…Ø§Ø³Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ Ø¬Ø²Ø± Ùˆ Ù…Ø¯ Ø¢Ø¨ Ø±Ø§ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ù†Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯ÙˆÙ… = Ø¬ÛŒÙ„ Ùˆ ØªØ§Ù… Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø§Ù…Ø§Ù† Ùˆ Ø¨Ø§Ø¨Ø§ Ø¨Ù‡ Ø³Ø§Ø­Ù„ Ø±ÙØªÙ†Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø³ÙˆÙ… = ØªØ§Ù… Ø¨Ø·Ø±ÛŒâ€Œ Ù†ÙˆØ´Ø§Ø¨Ù‡â€Œ Ø±Ø§ ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¯Ø§Ø®Øª Ùˆ Ø¨Ù‡ Ø³Ù…Øª Ø§Ùˆ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ú†Ù‡Ø§Ø±Ù… = Ø¨Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ù…Ù†Ø²Ù„ Ù†Ù‚Ø§Ø´ÛŒ Ú©Ù†Ø¯ Ùˆ Ø¨Ø§ Ù¾Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø¸Ø±Ù‡ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ø¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¯Ø± Ù…Ø§Ø³Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ Ø¬Ø²Ø± Ùˆ Ù…Ø¯ Ø¢Ø¨ Ø±Ø§ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ù†Ø¯\",\n",
    "    \"Ø¬ÛŒÙ„ Ùˆ ØªØ§Ù… Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø§Ù…Ø§Ù† Ùˆ Ø¨Ø§Ø¨Ø§ Ø¨Ù‡ Ø³Ø§Ø­Ù„ Ø±ÙØªÙ†Ø¯\",\n",
    "    \"ØªØ§Ù… Ø¨Ø·Ø±ÛŒâ€Œ Ù†ÙˆØ´Ø§Ø¨Ù‡â€Œ Ø±Ø§ ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¯Ø§Ø®Øª Ùˆ Ø¨Ù‡ Ø³Ù…Øª Ø§Ùˆ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯\",\n",
    "    \"Ø¨Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ù…Ù†Ø²Ù„ Ù†Ù‚Ø§Ø´ÛŒ Ú©Ù†Ø¯ Ùˆ Ø¨Ø§ Ù¾Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø¸Ø±Ù‡ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ø¯\",\n",
    "]\n",
    "\n",
    "tokenized_sents = [tokenizer.tokenize(s) for s in sentences]\n",
    "\n",
    "perps = []\n",
    "for sent_tokens in tokenized_sents:\n",
    "    ppl = lm_4gram.perplexity([sent_tokens])  # our method expects a list of token-lists\n",
    "    perps.append(ppl)\n",
    "\n",
    "for i, (s, p) in enumerate(zip(sentences, perps), 1):\n",
    "    print(f\"Ø¬Ù…Ù„Ù‡ {i}: perp = {p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "Ø¨Ù‡ Ù„Ø­Ø§Ø¸ ØªØ­Ù„ÛŒÙ„ Ø®Ø§Ù… Ø¨Ø§ÛŒØ¯ Ú¯ÙØª Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ø±Ø§ ÛŒÚ© 4gram ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.<br>\n",
    "Ø¨Ø§ÛŒØ¯ Ú¯ÙØª Ú©Ù‡ Ú©Ù‡ Ø¬Ù…Ù„Ù‡ Ø¯ÙˆÙ… Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ù„Ø­Ø§Ø¸ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù‡Ø± Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ú©Ù†Ø§Ø± Ù‡Ù… Ø¨ÛŒØ´ØªØ± Ø¨Ø§ Ù‡Ù… Ø§Ø±ØªØ¨Ø§Ø· Ø¯Ø§Ø±Ù†Ø¯ ØªØ§ Ø¬Ù…Ù„Ù‡ Ø¨Ø§Ù‡Ù… Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø§Ø² ÛŒÚ© Ngram Ø¨Ø§ n < 4 ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.<br>\n",
    "Ø±Ø§Ø¬Ø¨ Ø¬Ù…Ù„Ù‡ Ø³ÙˆÙ… Ùˆ Ú†Ù‡Ø§Ø±Ù… Ù‡Ù… Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ø±ØªØ¨Ø§Ø· ØªÙ…Ø§Ù… ØªÛŒÚ©Ù‡ Ù‡Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø¨Ø§Ù‡Ù… Ø¨Ù†Ø¸Ø± Ù…ÛŒ Ø¢ÛŒØ¯ ÛŒØ§ Ø§Ø² Ø·Ø±ÛŒÙ‚ ÛŒÚ© Ngram Ø¨Ø§ n Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² 4 ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ ØªÙˆØ³Ø· ÛŒÚ© llm Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¹Ù…Ù„Ø§ Ú©Ù„Ù…Ø§Øª Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø¬Ù…Ù„Ù‡ Ú©Ø§Ù…Ù„Ø§ Ø§Ø² Ú©Ø§Ù†ØªÚ©Ø³Øª Ú©Ù„Ù…Ø§Øª Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ø¬Ù…Ù„Ù‡ Ø®Ø¨Ø± Ø¯Ø§Ø±Ù†Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ (Smoothing) Ú©Ù‡ Ø¯Ø± Ø¯Ø±Ø³ Ø®ÙˆØ§Ù†Ø¯Ù‡â€ŒØ§ÛŒØ¯ (Laplace, Interpolation, Backoff) Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ÛŒÚ© Ù…Ø¯Ù„\n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "(Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ù…Ø¯Ù„ \n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "Ø¢Ù…ÙˆØ²Ø´â€ŒÛŒØ§ÙØªÙ‡ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.)\n",
    "<br>\n",
    "Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Interpolation Ù…Ù‚Ø§Ø¯ÛŒØ± Î» Ø±Ø§ 0.4, 0.3, 0.2, 0.1 Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯. Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª:\n",
    "Pâ€‹(wâˆ£h3â€‹,h2â€‹,h1â€‹)=0.4Pâ€‹(wâˆ£h3â€‹,h2â€‹,h1â€‹)+0.3Pâ€‹(wâˆ£h2â€‹,h1â€‹)+0.2Pâ€‹(wâˆ£h1â€‹)+0.1Pâ€‹(w)\n",
    "<br>\n",
    "Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Backoff Ù…Ù‚Ø¯Ø§Ø± Î» Ø±Ø§ 0.4 Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø± ÛŒÚ© Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ 100 ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªÙØ§ÙˆØª Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Ù†Ø¸Ø± Ø±ÙˆØ§Ù†ÛŒ Ùˆ ØªÙ†ÙˆØ¹ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¯Ø± Ù†Ù‡Ø§ÛŒØªØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± ÛŒÚ© Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ØŒ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø±Ø§ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ validation - Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø³ÙˆÙ… Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ - Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù…Ù‚Ø§Ø¯ÛŒØ± Perplexity Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø¯Ù„ ØºÛŒØ±Ù‡Ù…ÙˆØ§Ø± (Unsmoothed) Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø¨Ú¯ÙˆÛŒÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ø°Ú©Ø± Ø´Ø¯Ù‡\n",
    "<br>\n",
    "- Ù…Ø¹ÛŒØ§Ø± Perplexity Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ø°Ú©Ø± Ø´Ø¯Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have done most of smoothing coding and i have put it into the class of the Ngram that we have, so here we are just using it\n",
    "# The smoothing is only implemented for the N=4\n",
    "\n",
    "# 4Gram with smoothign Laplace(add-one)\n",
    "lm_4gram_laplace = NGramLM(n=4, smoothing=NGramLM.LAPLACE)\n",
    "\n",
    "lm_4gram_laplace.train(df_train_tokenized)\n",
    "\n",
    "generated_text = lm_8gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_4gram_backoff = NGramLM(n=4, smoothing=NGramLM.BACKOFF)\n",
    "\n",
    "lm_4gram_backoff.train(df_train_tokenized)\n",
    "\n",
    "generated_text = lm_4gram_backoff.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_4gram_interpolation = NGramLM(n=4, smoothing=NGramLM.INTERPOLATION)\n",
    "\n",
    "lm_4gram_interpolation.train(df_train_tokenized)\n",
    "\n",
    "generated_text = lm_4gram_interpolation.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_perplexity_per_row(model, df_val_tokenized):\n",
    "    perplexities = []\n",
    "    for sentence in df_val_tokenized:\n",
    "        ppl = model.perplexity([sentence])  # calculate for one sentence\n",
    "        if ppl != float(\"inf\"):\n",
    "            perplexities.append(ppl)\n",
    "    return np.mean(perplexities) if perplexities else float(\"inf\")\n",
    "\n",
    "\n",
    "# Laplace\n",
    "ppl_laplace = average_perplexity_per_row(lm_4gram_laplace, df_val_tokenized)\n",
    "print(f\"Average Perplexity (4-gram Laplace): {ppl_laplace:.2f}\")\n",
    "\n",
    "# Backoff\n",
    "ppl_backoff = average_perplexity_per_row(lm_4gram_backoff, df_val_tokenized)\n",
    "print(f\"Average Perplexity (4-gram Backoff): {ppl_backoff:.2f}\")\n",
    "\n",
    "# Interpolation\n",
    "ppl_interp = average_perplexity_per_row(lm_4gram_interpolation, df_val_tokenized)\n",
    "print(f\"Average Perplexity (4-gram Interpolation): {ppl_interp:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Temperature</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ§Ø«ÛŒØ± Temperature Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ ØªÙˆØ¶ÛŒØ­â€ŒØ¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ù‡ Ù‡Ù†Ú¯Ø§Ù… Sampling Ø§Ø² N-gramØŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù† Temperature ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒØ¯ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ú©Ù…ØªØ±ÛŒÙ† Temperature Ùˆ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† TemperatureØŒ Ø³Ù‡â€ŒØ¨Ø§Ø± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ Û²Û° ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯. (Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø­Ø§Ù„Øª Ø³Ù‡â€ŒØ¨Ø§Ø±) Ùˆ Ø³Ù¾Ø³ ØªØ§Ø«ÛŒØ± Temperature Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Temperature Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ† (Ø¨Ø±Ø§ÛŒ Ù‡Ø±ÛŒÚ© Û³ Ø¹Ø¯Ø¯ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implemented the tempreture in the NgramLM and i have written it in the class\n",
    "\n",
    "lm = NGramLM(n=4, smoothing=NGramLM.INTERPOLATION)\n",
    "lm.train(df_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text1 = lm.generate(30, use_sampling_weighted=True, temperature=0)\n",
    "generated_text2 = lm.generate(30, use_sampling_weighted=True, temperature=0)\n",
    "generated_text3 = lm.generate(30, use_sampling_weighted=True, temperature=0) \n",
    "\n",
    "\n",
    "clean1 = (\n",
    "    generated_text1\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean1 = re.sub(r\"\\s+\", \" \", clean1).strip()\n",
    "\n",
    "clean2 = (\n",
    "    generated_text2\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean2 = re.sub(r\"\\s+\", \" \", clean2).strip()\n",
    "\n",
    "clean3 = (\n",
    "    generated_text3\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean3 = re.sub(r\"\\s+\", \" \", clean3).strip()\n",
    "\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean1, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean2, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean3, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text1 = lm.generate(30, use_sampling_weighted=True, temperature=1)\n",
    "generated_text2 = lm.generate(30, use_sampling_weighted=True, temperature=1)\n",
    "generated_text3 = lm.generate(30, use_sampling_weighted=True, temperature=1) \n",
    "\n",
    "\n",
    "clean1 = (\n",
    "    generated_text1\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean1 = re.sub(r\"\\s+\", \" \", clean1).strip()\n",
    "\n",
    "clean2 = (\n",
    "    generated_text2\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean2 = re.sub(r\"\\s+\", \" \", clean2).strip()\n",
    "\n",
    "clean3 = (\n",
    "    generated_text3\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean3 = re.sub(r\"\\s+\", \" \", clean3).strip()\n",
    "\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean1, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean2, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean3, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text1 = lm.generate(30, use_sampling_weighted=True, temperature=2)\n",
    "generated_text2 = lm.generate(30, use_sampling_weighted=True, temperature=2)\n",
    "generated_text3 = lm.generate(30, use_sampling_weighted=True, temperature=2) \n",
    "\n",
    "\n",
    "clean1 = (\n",
    "    generated_text1\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean1 = re.sub(r\"\\s+\", \" \", clean1).strip()\n",
    "\n",
    "clean2 = (\n",
    "    generated_text2\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean2 = re.sub(r\"\\s+\", \" \", clean2).strip()\n",
    "\n",
    "clean3 = (\n",
    "    generated_text3\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean3 = re.sub(r\"\\s+\", \" \", clean3).strip()\n",
    "\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean1, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean2, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean3, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø´Ø´Ù…:</b><br>\n",
    "{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px; text-align: right;\">\n",
    "    <p style=\"text-align: right;\" dir=\"rtl\"><strong dir=\"rtl\">Ù…Ù‡Ù„Øª ØªØ­ÙˆÛŒÙ„ :</strong> 10 Ø¢Ø¨Ø§Ù†</p>\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
