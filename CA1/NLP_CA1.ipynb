{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† 1<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 dir='rtl' style=\"color:rgb(90, 255, 184); font-size: 20px;\">Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±Ù‡Ø§ Ùˆ N-gram</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø´Ù‡Ø±Ø²Ø§Ø¯ Ø¢Ø°Ø±ÛŒ Ø¢Ø²Ø§Ø¯ - ÙØ±Ø´Ø§Ø¯ Ø­Ø³Ø§Ù…ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">shahrzad.azari@ut.ac.ir - farshad.hessami@ut.ac.ir</p>\n",
    "\n",
    "<div dir='rtl' style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p dir='rtl' style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ù…Ù‡Ø¯ÛŒ Ù…Ø®ØªØ§Ø±ÛŒ</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: 810101515</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: 2/8/1404</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir; max-width: 100%;word-wrap: break-word;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "<div style=\"padding-right:40px\">\n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ Ù…ÙØ§Ù‡ÛŒÙ… Tokenization, Regular Expression , N-gram Language Modeling Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯. \n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ø§ Regex</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Auto-Correction Ø¨Ø§ Minimum Edit Distance</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Tokenization</span> (25)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Rule-based Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: BPE Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Wordpiece Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Tokenization Visualization</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø³ÙˆÙ… - <span dir=\"ltr\">N-gram Language Modeling</span> (55)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Data cleaning & Tokenization</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ N-gram</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù…Ø¹ÛŒØ§Ø± Perplexity</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Temperature Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„Ø§ØªØŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÛŒ Regex Ùˆ Ù‡Ù…ÛŒÙ†Ø·ÙˆØ± Minimum Distance Ù…ÙˆØ§Ø¬Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø¯Ø± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„â€ŒÙ‚Ø¨ÙˆÙ„ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¨Ø®Ø´ Ø¯ÙˆÙ… Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Minimum Distance ÛŒÚ© Ø³ÛŒØ³ØªÙ… Auto-Correction Ø³Ø§Ø¯Ù‡ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ø§ Regex</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ÙØ§ÛŒÙ„ emails.txt Ú©Ù‡ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ØŒ Ø´Ø§Ù…Ù„ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø³Ù… Ø¨Ù‡â€ŒÙ‡Ù…Ø±Ø§Ù‡ Ø§ÛŒÙ…ÛŒÙ„ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡â€Œâ€ŒØ´Ø§Ù† Ø¯Ø± ÛŒÚ© Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Ø§Ø² Ø´Ù…Ø§ Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¹ØªØ¨Ø± Ù‡Ø³ØªÙ†Ø¯ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² regex Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§ÛŒÙ…ÛŒÙ„ Ø§Ø² Ø¯Ùˆ Ø¨Ø®Ø´ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ú©Ù‡ Ø¨Ø§ @ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø¨Ø®Ø´ Ø§ÙˆÙ„ (Ù‚Ø¨Ù„ Ø§Ø² @) local-part Ù†Ø§Ù… Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ø®Ø´ Ø¯ÙˆÙ… domain.\n",
    "<br>\n",
    "Ù…Ù†Ø¸ÙˆØ± Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø¹Ø§ÛŒØª Ø´Ø¯Ù‡â€ŒØ¨Ø§Ø´Ù†Ø¯:\n",
    "<br>\n",
    "Û±. Ø¯Ùˆ Ø¨Ø®Ø´ Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ø§ ÛŒÚ© Ùˆ ØªÙ†Ù‡Ø§ ÛŒÚ© @ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ¨Ø§Ø´Ù†Ø¯.\n",
    "<br>\n",
    "Û². Ø¯Ø± local-part Ù‡Ù… Ù†Ø§Ù… Ùˆ Ù‡Ù… Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ Ø´Ø®Øµ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Û³. Ø¯Ø± local-part ØªÙ†Ù‡Ø§ Ø­Ø±ÙˆÙ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ -ØŒ_ Ùˆ . Ù…Ø¬Ø§Ø² Ù‡Ø³ØªÙ†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ùˆ Ù†Ù‚Ø·Ù‡ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù¾Ø´Øª Ù‡Ù… Ø¨ÛŒØ§ÛŒÙ†Ø¯.\n",
    "<br>\n",
    "Û´. Ø¯Ø± Ø¨Ø®Ø´ domain ÛŒÚ© Ù…ÛŒØ²Ø¨Ø§Ù† Ø¯Ø§Ø±ÛŒÙ… Ùˆ ÛŒÚ© Ù¾Ø³ÙˆÙ†Ø¯. Ù…ÛŒØ²Ø¨Ø§Ù† Ùˆ Ù¾Ø³ÙˆÙ†Ø¯ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ø§Ø² ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. (Ù…ÛŒØ²Ø¨Ø§Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø®ÙˆØ¯ Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø§ Ø¯Ùˆ Ù†Ù‚Ø·Ù‡â€ŒÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ø¯Ø± domain Ù…Ø¬Ø§Ø² Ù†ÛŒØ³Øª.)\n",
    "<br>\n",
    "Ûµ. Ù¾Ø³ÙˆÙ†Ø¯ Ø§Ø² Ø­Ø±ÙˆÙ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¯Ø§Ø±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù„ÛŒØ³Øª Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙØ§ÛŒÙ„ emails.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['milad.fattahi@uni.ir', 'parsa.golkar@host.ir', 'moradi.reza@uni.ir', 'niloufarheidari@lab.ir', 'mahsa_akbari@domain.com', 'ali.rezaei@example.com', 'zarei-farhad@host.ir', 'hamed.jalali@net.com', 'ramin.shafiei@project.net', 'sadeghi.laleh@work.ir', 'nouri.pouya@mail.ir', 'elhamdavari@office.net', 'behnam.khatibi@mail.org', 'sara.soleimani@host.ir', 'shirin_hashemi@domain.com', 'arman.khalili@school.ir', 'rahaesfandiari@company.com', 'kiana.ghasemi@uni.edu', 'navid_khodadadi@research.org', 'mehrdad.ebrahimi@example.org', 'raminshafiei@project.net', 'hamedjalali@domain.org']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import os\n",
    "\n",
    "file_name_emails = \"emails.txt\"\n",
    "file_path_emails = os.path.join(\".\", file_name_emails)\n",
    "out_path_emails = \"emails-valid.txt\"\n",
    "\n",
    "\n",
    "def get_name_email(line):\n",
    "    parts = line.split(sep=',')\n",
    "    name = parts[0]\n",
    "    name = name.split(sep='=')[1] \n",
    "\n",
    "    email = parts[1]\n",
    "    email = email.split(sep='=')[1]\n",
    "\n",
    "    return name.strip(), email.strip()\n",
    "\n",
    "\n",
    "def name_in_text(text: str, first_name: str, last_name: str) -> bool:\n",
    "\n",
    "    first = first_name.strip()\n",
    "    last = last_name.strip()\n",
    "    if not first or not last:\n",
    "        return False\n",
    "\n",
    "    pfirst = re.compile(re.escape(first), re.IGNORECASE)\n",
    "    plast = re.compile(re.escape(last), re.IGNORECASE)\n",
    "\n",
    "    # presence check\n",
    "    if pfirst.search(text) is None or plast.search(text) is None:\n",
    "        return False\n",
    "\n",
    "    # collect spans, then test non-overlap\n",
    "    f_spans = [m.span() for m in pfirst.finditer(text)]\n",
    "    l_spans = [m.span() for m in plast.finditer(text)]\n",
    "\n",
    "    for a1, a2 in f_spans:\n",
    "        for b1, b2 in l_spans:\n",
    "            if a1 >= b2 or b1 >= a2:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def email_correctness_checker(name:str, email:str) -> bool:\n",
    "\n",
    "    if not isinstance(name, str) or not isinstance(email, str):\n",
    "        return False\n",
    "\n",
    "    first_name = name.split()[0]\n",
    "    last_name = ' '.join(name.split()[1:])\n",
    "\n",
    "    # print(f\"firstname: {first_name} lastname: {last_name}\")\n",
    "\n",
    "    \n",
    "    Email_Pattern = re.compile(r\"^[\\w-][\\w.-]*@[\\w.-]+\\.[a-zA-Z]{2,}$\")  # Ø¯Ùˆ Ù†Ù‚Ø·Ù‡ Ù¾Ø´Øª Ù‡Ù… Ù†Ù…ÛŒØªÙˆÙ†Ù†Ø¯ Ø¨ÛŒØ§ÛŒÙ†Ø¯\n",
    "\n",
    "    Multi_Dot_Pattern = re.compile(r\"^.*\\.\\.+.*$\")\n",
    "\n",
    "    if Email_Pattern.match(email) is None or Multi_Dot_Pattern.match(email) is not None:\n",
    "        return False\n",
    "    \n",
    "    domain = email.split('@')[1]\n",
    "    local_part = email.split('@')[0]\n",
    "\n",
    "    if not name_in_text(local_part, first_name, last_name):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "emails_valid = list()\n",
    "try:\n",
    "    with open(file_path_emails) as file:\n",
    "        content_lines = file.readlines()\n",
    "        for line in content_lines:\n",
    "            name, email = get_name_email(line)\n",
    "\n",
    "            # print(f\"-{name}- & -{email}-\")\n",
    "\n",
    "            if email_correctness_checker(name, email):\n",
    "                emails_valid.append(email)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    print(\"File not found.\")\n",
    "\n",
    "\n",
    "with open(out_path_emails, 'w', encoding=\"utf-8\") as file:\n",
    "    file.write('\\n'.join(emails_valid))\n",
    "\n",
    "print(emails_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Regex ÛŒÚ© ØªØ§Ø¨Ø¹ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø³Ù… ÛŒÚ© Ø´Ø®Øµ Ø±Ø§ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯ Ùˆ Ø¯Ø±ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø³Ù… Ø¯Ø± Ø¨ÛŒÙ† Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ØŒ Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ†â€ŒØµÙˆØ±ØªØŒ ÛŒÚ© Ù¾ÛŒØºØ§Ù… Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ú†Ø§Ù¾ Ú©Ù†Ø¯.\n",
    "<br>\n",
    "ØªÙˆØ¬Ù‡: Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§ÛŒÙ…ÛŒÙ„ Ø§ÛŒÙ† Ø§Ø´Ø®Ø§ØµØŒ ØªØ­Øª Ù†Ø§Ù… Ø´Ø®Øµ Ø¯ÛŒÚ¯Ø±ÛŒ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ú©Ø§Ø± Ø´Ù…Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø§ÛŒÙ† Ø§ÙØ±Ø§Ø¯ Ø±Ø§ Ø§Ø² Ø¨ÛŒÙ† ØªÙ…Ø§Ù… Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªØ§Ø¨Ø¹ Ø±Ø§ Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name1 = Behnam Khatibi\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name2 = Mehrdad Ebrahimi\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÙØ±Ø§Ø¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behnam.khatibi@mail.org\n",
      "mehrdad.ebrahimi@example.org\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# def get_email_from_name(name: str) -> str:  # returns the email\n",
    "#     first_name, last_name = name.split()\n",
    "\n",
    "#     for email in emails_valid:\n",
    "#         if first_name.lower() in email.lower() and last_name.lower() in email.lower():\n",
    "#             return email\n",
    "#     print(\"Email doesn't exist\")\n",
    "#     return \"\"\n",
    "        \n",
    "def get_email_from_name(name: str) -> str:\n",
    "\n",
    "    first_name, last_name = name.split()\n",
    "\n",
    "    first_pat = re.compile(re.escape(first_name), re.IGNORECASE)\n",
    "    last_pat  = re.compile(re.escape(last_name),  re.IGNORECASE)\n",
    "\n",
    "    for email in emails_valid:\n",
    "        if first_pat.search(email) and last_pat.search(email):\n",
    "            return email\n",
    "        \n",
    "    print(\"Email doesn't exist\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "name1 = \"Behnam Khatibi\"\n",
    "name2 = \"Mehrdad Ebrahimi\"\n",
    "\n",
    "print(get_email_from_name(name1))\n",
    "print(get_email_from_name(name2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Auto-Correction Ø¨Ø§ Minimum Edit Distance</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Û±. Ø§Ø¨ØªØ¯Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… levenshtein_distance Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† minimum_distance Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡â€ŒØ¯Ø³Øª Ø¢ÙˆØ±ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair1 = \"Athletic\", \"Atlantic\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair2 = \"London\", \"Boston\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"Action\", \"Compact\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"\", \"Sting\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ù‚Ø¯Ø§Ø± minimum distance Ø¨ÛŒÙ† Ø¬ÙØª Ú©Ù„Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is: ('Athletic', 'Atlantic')   |   Min Edit Distance is(Levenshtein algo): 3\n",
      "input is: ('London', 'Boston')   |   Min Edit Distance is(Levenshtein algo): 3\n",
      "input is: ('Action', 'Compact')   |   Min Edit Distance is(Levenshtein algo): 7\n",
      "input is: ('', 'Sting')   |   Min Edit Distance is(Levenshtein algo): 5\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(a: str, b: str) -> int:\n",
    "    m, n = len(a), len(b)\n",
    "\n",
    "    # create DP table ( if we solve it recursevily it will cost us time and memory )\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # base cases - was discussed in the class\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # fill table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if a[i - 1] == b[j - 1] else 1  # we put 1 but the cost for replacing accodring to the class and the Jurafsky book was 2 (not levenshtein)\n",
    "\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,      # deletion\n",
    "                dp[i][j - 1] + 1,      # insertion\n",
    "                dp[i - 1][j - 1] + cost  # substitution\n",
    "            )\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "# We know the max value that the min edit distance could be is max(len(a), len(b)) in Levenshtein\n",
    "examples = [(\"Athletic\", \"Atlantic\"), (\"London\", \"Boston\"), (\"Action\", \"Compact\") ,(\"\", \"Sting\")]\n",
    "for inp in examples:\n",
    "    MinDistance = levenshtein_distance(*inp)\n",
    "    print(f\"input is: {inp}   |   Min Edit Distance is(Levenshtein algo): {MinDistance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Û². Ø¨Ø¹Ø¯ Ø§Ø² Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ minimum distance Ø­Ø§Ù„ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù†ØŒ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ø§Ù…Ù„Ø§ÛŒÛŒ Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ú©Ù„Ù…Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ø§Ù…Ù„Ø§ÛŒØ´Ø§Ù† Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª. ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø§Ù…Ù„Ø§ÛŒ ØµØ­ÛŒØ­ Ú©Ù„Ù…Ø§Øª Ú©Ù‡ Ú©Ù„Ù…Ø§Øª Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ù†ÛŒØ² Ø´Ø§Ù…Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„ vocab.txt Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "sentence_to_be_corrected = \n",
    "\"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡â€ŒÛŒ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§ Ú©Ù…Ú© ÙØ§ÛŒÙ„ vocab.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'first', 'algorithm', 'distance', 'edit', 'code', 'function', 'variable', 'loop', 'condition', 'really', 'string', 'integer', 'list', 'array', 'input', 'output', 'compile', 'execute', 'they', 'debug', 'research', 'project', 'experiment', 'analysis', 'data', 'are', 'result', 'model', 'computation', 'and', 'simulation', 'performance', 'accuracy', 'enjoy', 'precision', 'write', 'read', 'run', 'test', 'learn', 'create', 'university', 'study', 'improve', 'understand', 'compare', 'their', 'evaluate', 'correct', 'it', 'students', 'efficient', 'accurate', 'fast', 'slow', 'easy', 'hard', 'interesting', 'enjoyable', 'practical', 'hello', 'writing', 'at', 'in', 'the']\n",
      "hello students at the university are writing they first edit distance algorithm in Python and they really enjoy it\n"
     ]
    }
   ],
   "source": [
    "def load_vocab(path=\"vocab.txt\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        words_vocab = [word.strip() for word in content.split(',')]\n",
    "        return words_vocab\n",
    "\n",
    "def autocorrect(word: str, vocab: list, max_dist: int = 2) -> str:\n",
    "\n",
    "    # Trying to hanlde lower case in the word or vocab.txt correctly (still not sure)\n",
    "    word_lower = word.lower()\n",
    "    best_word = word\n",
    "    best_dist = float(\"inf\")\n",
    "\n",
    "    for v in vocab:\n",
    "        d = levenshtein_distance(word_lower, v.lower())\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_word = v\n",
    "            if d == 0:  # perfect match\n",
    "                return v\n",
    "\n",
    "    # preventing correction which should not be corrected (too much editing)\n",
    "    return best_word if best_dist <= max_dist else word\n",
    "\n",
    "\n",
    "vocab = load_vocab() \n",
    "print(vocab)\n",
    "\n",
    "sentence_to_be_corrected = \"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "    \n",
    "def auto_correct_sentence(sent):\n",
    "    corrected_sentence = []\n",
    "    for word in sentence_to_be_corrected.split():\n",
    "        # print(word)\n",
    "        corrected_sentence.append(autocorrect(word, vocab, max_dist=3))\n",
    "\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "print(auto_correct_sentence(sentence_to_be_corrected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_with_heuristics(word: str, vocab: list, max_dist: int = 2) -> str:\n",
    "    word_lower = word.lower()\n",
    "    lw = len(word)\n",
    "    best_word = word\n",
    "    best_dist = float(\"inf\")\n",
    "    best_length_diff = float(\"inf\")\n",
    "\n",
    "    for original, lower in vocab:\n",
    "\n",
    "        # if the length differs a lot dont calc the Min Edit Distance\n",
    "        if abs(len(lower) - lw) > max_dist:\n",
    "            continue\n",
    "\n",
    "        d = levenshtein_distance(word_lower, lower, max_dist)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_word = original\n",
    "            best_length_diff = abs(len(lower) - lw)\n",
    "            if d == 0:\n",
    "                return original\n",
    "\n",
    "        elif d == best_dist:  # for better equil dist decision\n",
    "            length_diff = abs(len(lower) - lw)\n",
    "            if length_diff < best_length_diff:\n",
    "                best_word = original\n",
    "                best_length_diff = length_diff\n",
    "\n",
    "    return best_word if best_dist <= max_dist else word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Tokenization</span> (25)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø§ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ø±ÙˆØ´  Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Rule-based Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¨Ø§ Ú©Ù…Ú© Ø¯Ø³ØªÙˆØ±Ø§Øª regex ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ù…ØªÙ† Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†Ø¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ØµÙˆØ±Øª Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'fun', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "# from hazm import normalize  \n",
    "\n",
    "\n",
    "# Converts fancy characters into standard ones\n",
    "# Merges lookalikes that serve the same role in text\n",
    "def normalize(text: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "I did my rule based tokenizer with considering the following on the non common words:\n",
    "\n",
    "- Keep contractions\n",
    "- Keep hyphens inside words\n",
    "- Keep abbreviations\n",
    "- Separate punctuation\n",
    "- Split currency symbol from number\n",
    "- Keep numbers whole\n",
    "- Ellipsis as one token\n",
    "- Case preserved\n",
    "- English-only assumptions\n",
    "- Works great for n-gram language models\n",
    "\n",
    "We are using N-gram method in the LM after this.\n",
    "\n",
    "This is for the Rule-Based Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Order matters: longest and most specific first\n",
    "TOKEN_PATTERN = re.compile(\n",
    "    r\"\"\"\n",
    "    (?P<URL>\\bhttps?://[^\\s<>\"']+|\\bwww\\.[^\\s<>\"']+)              # URLs\n",
    "    | (?P<EMAIL>[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})   # Emails\n",
    "    \n",
    "    | (?P<ELLIPSIS>\\.\\.\\.)                                       # Ellipsis â€¦\n",
    "    \n",
    "    | (?P<CURRENCY>[$])                                          # Currency symbol as separate token\n",
    "    | (?P<NUMBER>\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%?)                  # Numbers (12,000.50 and 12.40 and 50%)\n",
    "    \n",
    "    | (?P<ABBR>(?:[A-Za-z]\\.){2,})                               # Abbreviations like U.S.A., e.g.\n",
    "    \n",
    "    | (?P<CONTRACTION>[A-Za-z]+\\'[A-Za-z]+)                      # Contractions (don't, I'm)\n",
    "    \n",
    "    | (?P<HWORD>[A-Za-z]+(?:-[A-Za-z]+)+)                        # Hyphenated compounds\n",
    "    \n",
    "    | (?P<WORD>[A-Za-z]+)                                        # Normal words\n",
    "    \n",
    "    | (?P<PUNCT>[.,!?;:()\\\"'])                                   # Punctuation split\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_rule_based(text: str):\n",
    "    text = normalize(text)\n",
    "    tokens = []\n",
    "    for m in TOKEN_PATTERN.finditer(text):\n",
    "        tokens.append(m.group())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(tokenize_rule_based(\"That U.S.A. poster-print costs $12.40...\"))\n",
    "print(tokenize_rule_based(\"Hello, world! NLP is fun.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§ÛŒØ±Ø§Ø¯Ø§Øª Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ú†ÛŒØ³ØªØŸ\n",
    "<br>\n",
    "Ú†Ù†Ø¯ Ø±Ø§Ù‡â€ŒØ­Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "ÛŒÚ©ÛŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù‡Ø¨ÙˆØ¯ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'fun', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# stronger normalizationÙˆ keeps your NFKC\n",
    "_trans_table = {\n",
    "    ord(\"â€™\"): \"'\",\n",
    "    ord(\"â€˜\"): \"'\",\n",
    "    ord(\"â€œ\"): '\"',\n",
    "    ord(\"â€\"): '\"',\n",
    "    ord(\"â€”\"): \"-\",   # em dash\n",
    "    ord(\"â€“\"): \"-\",   # en dash\n",
    "    ord(\"â€¦\"): \"...\", # unicode ellipsis -> three dots (so your ELLIPSIS rule hits)\n",
    "    0x00A0: 0x0020,  # NBSP -> space\n",
    "    0x200B: None,    # zero-width space -> remove\n",
    "}\n",
    "\n",
    "def normalize_strong(text: str) -> str:\n",
    "    t = unicodedata.normalize(\"NFKC\", text)\n",
    "    return t.translate(_trans_table)\n",
    "\n",
    "\n",
    "# do some post processing\n",
    "_URL_RE    = re.compile(r\"^(?:https?://[^\\s<>'\\\"]+|www\\.[^\\s<>'\\\"]+)$\", re.I)\n",
    "_EMAIL_RE  = re.compile(r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\")\n",
    "_NUMBER_RE = re.compile(r\"^\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?%?$\")\n",
    "\n",
    "# trailing punctuation we want to peel off if it sticks to URL/EMAIL/NUMBER\n",
    "_TRAIL = set(\".,!?:;)\")\n",
    "# and quotes that often trail as well\n",
    "_TRAIL_QUOTES = set(\"\\\"'\")\n",
    "\n",
    "# common single-segment abbreviations that should keep the trailing dot\n",
    "_TITLE_ABBRS = {\"Mr\", \"Ms\", \"Mrs\", \"Dr\", \"Prof\", \"Sr\", \"Jr\", \"St\", \"Mt\", \"Ave\", \"Rd\", \"Blvd\"}\n",
    "\n",
    "# general multi-segment abbreviation pattern: Ph.D., U.S.A., e.g., i.e., U.S., etc.\n",
    "_ABBR_SEG_RE = re.compile(r\"^(?:[A-Za-z]{1,4}\\.){2,}$\")\n",
    "\n",
    "\n",
    "def _strip_trailing_punct(token: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    If token looks like URL/EMAIL/NUMBER but has glued trailing punctuation,\n",
    "    peel punctuation into separate tokens, preserving order.\n",
    "    \"\"\"\n",
    "    base = token\n",
    "    if not (_URL_RE.match(base) or _EMAIL_RE.match(base) or _NUMBER_RE.match(base)):\n",
    "        # maybe itâ€™s base + trailing punct; try stripping\n",
    "        s = base\n",
    "        trail = []\n",
    "        while s and (s[-1] in _TRAIL or (trail and s[-1] in _TRAIL_QUOTES)):\n",
    "            trail.append(s[-1])\n",
    "            s = s[:-1]\n",
    "        if s and (_URL_RE.match(s) or _EMAIL_RE.match(s) or _NUMBER_RE.match(s)):\n",
    "            return [s] + trail  # split off the glued punctuation\n",
    "    return [token]\n",
    "\n",
    "\n",
    "def _merge_abbreviations(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Merge sequences like [\"U\", \".\", \"S\", \".\"] -> [\"U.S.\"]\n",
    "    Merge [\"Ph\", \".\", \"D\", \".\"] -> [\"Ph.D.\"]\n",
    "    Keep known single-word titles with dot: [\"Mr\", \".\"] -> [\"Mr.\"].\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    i = 0\n",
    "    n = len(tokens)\n",
    "    while i < n:\n",
    "        # Title + dot\n",
    "        if i + 1 < n and tokens[i] in _TITLE_ABBRS and tokens[i+1] == \".\":\n",
    "            out.append(tokens[i] + \".\")\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "        # Multi-segment abbreviation: series of (letters, \".\") repeating\n",
    "        j = i\n",
    "        parts = []\n",
    "        while j + 1 < n and re.fullmatch(r\"[A-Za-z]{1,4}\", tokens[j]) and tokens[j+1] == \".\":\n",
    "            parts.append(tokens[j] + \".\")\n",
    "            j += 2\n",
    "        if len(parts) >= 2:\n",
    "            merged = \"\".join(parts)\n",
    "            out.append(merged)\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        out.append(tokens[i])\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def tokenize_en_robust(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Wrapper around your tokenizer:\n",
    "      - stronger normalization\n",
    "      - merge abbreviations (Mr., U.S., Ph.D., e.g., i.e., U.S.A.)\n",
    "      - deglue trailing punctuation from URL/EMAIL/NUMBER\n",
    "    \"\"\"\n",
    "    # 1) normalize harder\n",
    "    text = normalize_strong(text)\n",
    "\n",
    "    # 2) your base tokenizer (assumes tokenize() is defined with TOKEN_PATTERN)\n",
    "    base_tokens = tokenize_rule_based(text)\n",
    "\n",
    "    # 3) fix glued punctuation on URL/EMAIL/NUMBER\n",
    "    tokens = []\n",
    "    for tok in base_tokens:\n",
    "        tokens.extend(_strip_trailing_punct(tok))\n",
    "\n",
    "    # 4) merge abbreviations/titles\n",
    "    tokens = _merge_abbreviations(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(tokenize_en_robust(\"That U.S.A. poster-print costs $12.40...\"))\n",
    "print(tokenize_en_robust(\"Hello, world! NLP is fun.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…Ù† Ú©Ù‡ Ù†ÙˆØ´ØªÙ‡ Ø¨ÙˆØ¯Ù… Ø§Ú©Ø«Ø± Ø­Ø§Ù„Ø§Øª Ø±Ø§ Ø¯Ø± Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù¾ÙˆØ´Ø´ Ù…ÛŒØ¯Ø§Ø¯ Ùˆ Ø¨Ø±Ø®ÛŒ ØªØµÙ…ÛŒÙ… Ù‡Ø§ÛŒÛŒ Ø¨ÙˆØ¯Ø¯ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ØªÙˆÚ©Ù† Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒÙ… Ùˆ ÛŒØ§ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒÙ… Ø¢Ù† Ø±Ø§ Ú©Ù‡ Ø·Ø¨Ù‚ Ú©Ø§Ø±Ø¨Ø±Ø¯  Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ù†ÛŒØ§Ø² Ú©Ù‡ Ø¯Ø± Ø®ÙˆØ¯ Ú©Ø¯ Ù‡Ù… Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª Ù…ÙˆØ±Ø¯ Ù…Ù‡Ù… ØªØ± Ø±Ø§ Ù„Ø­Ø§Ø¸ Ú©Ø±Ø¯Ù‡ Ùˆ ØªØµÙ…ÛŒÙ… Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù† Ú¯Ø±ÙØªÛŒÙ….\n",
    "Ø§Ù…Ø§ Ø§ÛŒØ±Ø§Ø¯Ø§Øª ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…ÛŒØªÙˆØ§Ù† Ø¨Ù‡ Ø¨Ø±Ø®ÛŒ Ø­Ø§Ù„Ø§Øª Ù„Ø¨Ù‡ Ùˆ Ø®Ø§Øµ ØªØ± Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ù…Ø§Ù†Ù†Ø¯\n",
    "<ul dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙ Ù‡Ø§ÛŒ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ø§ÛŒ ÛŒØ§ Ø­Ø§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù‡Ø± Ø­Ø±Ù Ø¨Ø¹Ø¯ Ø¢Ù† Ù†Ù‚Ø·Ù‡ Ù†ÛŒØ§Ù…Ø¯Ù‡ Ùˆ Ø¨Ù„Ú©Ù‡ Ú†Ù†Ø¯ ØªØ§ Ø­Ø±Ù Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù† Ù†Ù‚Ø·Ù‡ Ø§Ù…Ø¯Ù‡ Ø§Ù†Ø¯.. Ú©Ù‡ Ù…Ø«Ø§Ù„ Ù‡Ø§ÛŒ Ø¢Ù† Mr. DR. Ùˆ ØºÛŒØ±Ù‡ Ù‡Ø³ØªÙ†Ø¯ .</li>\n",
    "  <li>Ø¹Ù„Ø§Ù…ØªÛŒ Ø§Ú¯Ø± Ù¾Ø³ Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ùˆ ÛŒØ§ Ù„ÛŒÙ†Ú© Ø¨ÛŒØ§ÛŒØ¯ Ù…Ø§ Ø¬Ø²Ùˆ ØªÙˆÚ©Ù† Ù„ÛŒÙ†Ú© Ùˆ Ø§ÛŒÙ…ÛŒÙ„ Ø¯Ø±Ù†Ø¸Ø± Ù…ÛŒÚ¯ÛŒØ±ÛŒÙ… Ú©Ù‡ Ø²ÛŒØ§Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ù†Ù…ÛŒØ¨Ø§Ø´Ø¯.</li>\n",
    "  <li>Ø­Ø§Ù„Ø§ØªÛŒ Ú©Ù‡ Ø¹Ù„Ø§Ù…Øª Ø¯Ø±ØµØ¯ Ø¯Ø± Ù¾Ø´Øª Ø¹Ø¯Ø¯ Ø¨ÛŒØ§ÛŒØ¯ Ø±Ø§ Ø¯Ø±Ø³Øª ØªÙ‚Ø³ÛŒÙ… Ù†Ù…ÛŒÚ©Ù†ÛŒÙ….</li>\n",
    "    <li>Ø¨Ø±Ø®ÛŒ Ø§ÙˆÙ‚Ø§Øª Ù…Ø§Ù†Ù†Ø¯ Ø§Ø³Ù…Ø§Ø±Øª Ú©ÙˆØªÛŒØ´Ù† Ù‡Ø§ Ùˆ Ú†ÛŒØ²Ù‡Ø§ÛŒ Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø¯Ø± ØªØ§Ø¨Ø¹ Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ² Ù‡Ù…Ù‡ Ø§Ù†Ù‡Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÛŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø®ÛŒ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ù…Ù¾ Ø¨Ú©Ù†ÛŒÙ….</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">BPE Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª TinyStories-Farsi Ø±Ø§ Ø§Ø² HuggingFace Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    (Ù„ÛŒÙ†Ú© Ø¯ÛŒØªØ§Ø³Øª)\n",
    "</a>\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ (train) Ø¢Ù† Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ú©Ù†ÙˆÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± BPE Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø§Ù†Ø¹ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NoteBook\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27630 entries, 0 to 27629\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   English  27630 non-null  object\n",
      " 1   Persian  27630 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 431.8+ KB\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset_path = \"taesiri/TinyStories-Farsi\"\n",
    "\n",
    "# ds = load_dataset(dataset_path)\n",
    "\n",
    "dataset_path_val = os.path.join(\"data\", \"validation.parquet\")\n",
    "\n",
    "df_val = pd.read_parquet(dataset_path_val)\n",
    "\n",
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Persian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u don't have to be scared of the loud dog, I'l...</td>\n",
       "      <td>ØªØ±Ø¬Ù…Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ:\\n\\nÙ…Ø¬Ø¨ÙˆØ± Ù†ÛŒØ³ØªÛŒ Ø§Ø² Ø³Ú¯ Ø¨Ù„Ù†Ø¯ ØµØ¯Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, in a warm and sunny place, t...</td>\n",
       "      <td>ÛŒÚ© Ø±ÙˆØ² Ø¯Ø± ÛŒÚ© Ø¬Ø§ÛŒ Ú¯Ø±Ù… Ùˆ Ø¢ÙØªØ§Ø¨ÛŒØŒ ÛŒÚ© Ú†Ø§Ù„Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time there was a little girl named...</td>\n",
       "      <td>Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÙˆØ³ÛŒ Ø¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One morning, a cat named Tom woke up. He felt ...</td>\n",
       "      <td>Ø¨Ø§ ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ:\\n\\nÛŒÚ© ØµØ¨Ø­ØŒ Ú¯Ø±Ø¨Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  u don't have to be scared of the loud dog, I'l...   \n",
       "1  Once upon a time, in a warm and sunny place, t...   \n",
       "2  Once upon a time there was a little girl named...   \n",
       "3  One morning, a cat named Tom woke up. He felt ...   \n",
       "\n",
       "                                             Persian  \n",
       "0   ØªØ±Ø¬Ù…Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ:\\n\\nÙ…Ø¬Ø¨ÙˆØ± Ù†ÛŒØ³ØªÛŒ Ø§Ø² Ø³Ú¯ Ø¨Ù„Ù†Ø¯ ØµØ¯Ø§...  \n",
       "1   ÛŒÚ© Ø±ÙˆØ² Ø¯Ø± ÛŒÚ© Ø¬Ø§ÛŒ Ú¯Ø±Ù… Ùˆ Ø¢ÙØªØ§Ø¨ÛŒØŒ ÛŒÚ© Ú†Ø§Ù„Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ...  \n",
       "2   Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÙˆØ³ÛŒ Ø¨...  \n",
       "3   Ø¨Ø§ ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ:\\n\\nÛŒÚ© ØµØ¨Ø­ØŒ Ú¯Ø±Ø¨Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124411 entries, 0 to 124410\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   English  124411 non-null  object\n",
      " 1   Persian  124411 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path_train = os.path.join(\"data\", \"train.parquet\")\n",
    "\n",
    "df_train = pd.read_parquet(dataset_path_train)\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Persian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time there was a little boy named ...</td>\n",
       "      <td>ÛŒÚ©â€ŒØ±ÙˆØ² ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ø§Ø³Ù… Ø¨Ù† Ø¨ÙˆØ¯. Ø¨Ù† Ø¯ÙˆØ³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a reliable otter n...</td>\n",
       "      <td>ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ ÛŒÚ© Ø³Ù…ÙˆØ± Ø¢Ø¨ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little boy named Tim went to the pa...</td>\n",
       "      <td>ÛŒÚ© Ø±ÙˆØ² Ù¾Ø³Ø± Ú©ÙˆÚ†Ú©ÛŒ Ø¨Ù‡ Ø§Ø³Ù… ØªÛŒÙ… Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØª. Ø§Ùˆ ÛŒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time there was a friendly little b...</td>\n",
       "      <td>ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨Ù‡ Ù†Ø§Ù… Ø¨Ø§Ø¨ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€Œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, in a small house, there live...</td>\n",
       "      <td>ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ Ø¯Ø± ÛŒÚ© Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú©ØŒ ÛŒÚ© Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ù†...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  Once upon a time there was a little boy named ...   \n",
       "1  Once upon a time, there was a reliable otter n...   \n",
       "2  One day, a little boy named Tim went to the pa...   \n",
       "3  Once upon a time there was a friendly little b...   \n",
       "4  Once upon a time, in a small house, there live...   \n",
       "\n",
       "                                             Persian  \n",
       "0   ÛŒÚ©â€ŒØ±ÙˆØ² ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ø§Ø³Ù… Ø¨Ù† Ø¨ÙˆØ¯. Ø¨Ù† Ø¯ÙˆØ³...  \n",
       "1   ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ ÛŒÚ© Ø³Ù…ÙˆØ± Ø¢Ø¨ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§...  \n",
       "2   ÛŒÚ© Ø±ÙˆØ² Ù¾Ø³Ø± Ú©ÙˆÚ†Ú©ÛŒ Ø¨Ù‡ Ø§Ø³Ù… ØªÛŒÙ… Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØª. Ø§Ùˆ ÛŒ...  \n",
       "3   ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨Ù‡ Ù†Ø§Ù… Ø¨Ø§Ø¨ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€Œ...  \n",
       "4   ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ Ø¯Ø± ÛŒÚ© Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú©ØŒ ÛŒÚ© Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ù†...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self, num_merges=2000):\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = []\n",
    "        self.vocab = {}\n",
    "\n",
    "    def normalize_farsi(self, text: str) -> str:  # I could have used hazm instead\n",
    "        text = text.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n",
    "        text = text.replace(\"\\u200d\", \"\\u200c\")  # deZWNJ\n",
    "        return text\n",
    "\n",
    "    # Generate vocabulary from corpus (character-level tokenization)\n",
    "    def get_vocab(self, corpus):\n",
    "        vocab = Counter()\n",
    "        for word in corpus:\n",
    "            word = self.normalize_farsi(word.strip())\n",
    "            if not word:\n",
    "                continue\n",
    "            vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "        return vocab\n",
    "\n",
    "    # Compute pair frequencies\n",
    "    def get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    # Merge the most frequent pair into a new subword\n",
    "    def merge_vocab(self, pair, v_in):\n",
    "        v_out = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replace = ''.join(pair)\n",
    "        for word, freq in v_in.items():\n",
    "            new_word = word.replace(bigram, replace)\n",
    "            v_out[new_word] = freq\n",
    "        return v_out\n",
    "\n",
    "    # BPE learning (frequency based)\n",
    "    def train(self, corpus):\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.merges = []\n",
    "\n",
    "        for _ in tqdm(range(self.num_merges), desc=\"Training BPE merges\"):\n",
    "            pairs = self.get_stats(self.vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self.merge_vocab(best, self.vocab)\n",
    "            self.merges.append(best)\n",
    "\n",
    "        return self.merges\n",
    "\n",
    "    # Tokenizer (apply learned merges)\n",
    "    def encode_word(self, word):\n",
    "        word = self.normalize_farsi(word)\n",
    "        chars = list(word) + [\"</w>\"]\n",
    "        i = 0\n",
    "        while i < len(chars) - 1:\n",
    "            pair = (chars[i], chars[i + 1])\n",
    "            if pair in self.merges:\n",
    "                chars[i:i+2] = [''.join(pair)]\n",
    "                i = max(i - 1, 0)\n",
    "            else:\n",
    "                i += 1\n",
    "        # NOT drop </w>\n",
    "        return chars\n",
    "\n",
    "    # Tokenize entire text\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for w in text.split():\n",
    "            tokens.extend(self.encode_word(w))\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE merges: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [35:03<00:00,  4.75it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ÛŒ', '</w>'),\n",
       " ('Ù‡', '</w>'),\n",
       " ('Ø§', '</w>'),\n",
       " ('.', '</w>'),\n",
       " ('Ùˆ', '</w>'),\n",
       " ('Ø¯', '.</w>'),\n",
       " ('Ø¯', '</w>'),\n",
       " ('Ù†', '</w>'),\n",
       " ('Ø±', '</w>'),\n",
       " ('Øª', '</w>'),\n",
       " ('Ù…', '</w>'),\n",
       " ('Ù…', 'ÛŒ'),\n",
       " ('Ø¨', 'Ø§'),\n",
       " ('Ú©', '</w>'),\n",
       " ('Ø¨', 'Ù‡</w>'),\n",
       " ('ØŒ', '</w>'),\n",
       " ('Ú©', 'Ø±'),\n",
       " ('Ù…ÛŒ', '\\u200c'),\n",
       " ('Ø¯', 'Ø§'),\n",
       " ('Ø±', 'Ø§</w>'),\n",
       " ('Ø±', 'Ùˆ'),\n",
       " ('Ù†', 'Ø¯.</w>'),\n",
       " ('Ø®', 'Ùˆ'),\n",
       " ('Ø´', '</w>'),\n",
       " ('Ø²', '</w>'),\n",
       " ('Ø§', 'Ù†'),\n",
       " ('Ø¨', 'Ùˆ'),\n",
       " ('ÛŒ', 'Ú©</w>'),\n",
       " ('Ø§', 'Ùˆ</w>'),\n",
       " ('Ø§', 'Ø³'),\n",
       " ('Ù†', 'Ø¯</w>'),\n",
       " ('Ø¯', 'Ùˆ'),\n",
       " ('Ù‡', 'Ø§</w>'),\n",
       " ('Ù„', 'ÛŒ</w>'),\n",
       " ('Ù', 'Øª'),\n",
       " ('Ø§', 'ÛŒ'),\n",
       " ('Ù„', '</w>'),\n",
       " ('Ø¯', 'ÛŒ'),\n",
       " ('Ù…', 'Ø§'),\n",
       " ('Ø¢', 'Ù†'),\n",
       " ('Ú©', 'Ù‡</w>'),\n",
       " ('Ø²', 'ÛŒ</w>'),\n",
       " ('Ø¯', 'Ø±</w>'),\n",
       " ('Ø¨', 'Ø±'),\n",
       " ('Ø³', 'Øª'),\n",
       " ('Ø§', 'Ø²</w>'),\n",
       " ('ÛŒ', 'Ù„ÛŒ</w>'),\n",
       " ('Ú©', 'Ù†'),\n",
       " ('Ú©Ø±', 'Ø¯'),\n",
       " ('Øª', 'Ùˆ'),\n",
       " ('Ø¨', '</w>'),\n",
       " ('Ø¯', 'Ù‡</w>'),\n",
       " ('Ø§', 'Ù…</w>'),\n",
       " ('Ø¯', 'Ø±'),\n",
       " ('Ø¨Ø§', 'Ø²ÛŒ</w>'),\n",
       " ('ÛŒ', 'Ù…</w>'),\n",
       " ('Ú¯', '</w>'),\n",
       " ('Ú¯', 'Ø±'),\n",
       " ('Ø¯Ø§', 'Ø´'),\n",
       " ('\"', '</w>'),\n",
       " (':', '</w>'),\n",
       " ('Ø§', 'Ø±'),\n",
       " ('Ø¢Ù†', 'Ù‡Ø§</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯.</w>'),\n",
       " ('Ú¯', 'ÙØª'),\n",
       " ('Ø¯', 'ØŒ</w>'),\n",
       " ('Ø¨', 'Ø²'),\n",
       " ('Øª', 'ÛŒÙ…</w>'),\n",
       " ('Ø­', 'Ø§'),\n",
       " ('Ø¨', 'ÛŒ'),\n",
       " ('Ø®', 'ÛŒÙ„ÛŒ</w>'),\n",
       " ('Ù¾', 'Ø±'),\n",
       " ('Ø¨Ø²', 'Ø±'),\n",
       " ('Ø®Ùˆ', 'Ø´'),\n",
       " ('Ù‡', 'Ù…</w>'),\n",
       " ('Ø³', 'ÛŒ'),\n",
       " ('Ù‡', '\\u200c'),\n",
       " ('Ø³', '</w>'),\n",
       " ('Ø²', 'ÛŒ'),\n",
       " ('Ú¯ÙØª', ':</w>'),\n",
       " ('Ø¯', 'Ù†Ø¯.</w>'),\n",
       " ('Ø¯Ùˆ', 'Ø³Øª</w>'),\n",
       " ('Ú©', 'Ùˆ'),\n",
       " ('Øª', 'Ø±'),\n",
       " ('Ø´', 'ÛŒ'),\n",
       " ('\\u200c', 'Ù‡Ø§</w>'),\n",
       " ('Ø¯Ø§Ø´', 'Øª'),\n",
       " ('Ù¾', 'ÛŒ'),\n",
       " ('Ù†', 'Ú¯'),\n",
       " ('Ø­Ø§', 'Ù„</w>'),\n",
       " ('Ø±Ùˆ', 'Ø²</w>'),\n",
       " ('Ú©Ùˆ', 'Ú†'),\n",
       " ('Ø§', 'Ù…Ø§</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯</w>'),\n",
       " ('Ø¨Ø²Ø±', 'Ú¯</w>'),\n",
       " ('Ø³', 'Ø±'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ø®Ùˆ'),\n",
       " ('Ø®ÙˆØ´', 'Ø­Ø§Ù„</w>'),\n",
       " ('Ø¯Ùˆ', 'Ø³Øª'),\n",
       " ('Ø§ÛŒ', 'Ù†</w>'),\n",
       " ('Ø±Ùˆ', 'Ø²'),\n",
       " ('Ù„', 'ÛŒÙ„ÛŒ</w>'),\n",
       " ('Ù†', 'Ø¯'),\n",
       " ('Ù…', 'Ùˆ'),\n",
       " ('Ø±', 'Ø§'),\n",
       " ('Øª', 'ÛŒ</w>'),\n",
       " ('Ù‡', 'Ø§ÛŒ</w>'),\n",
       " ('Øª', 'Ø§</w>'),\n",
       " ('ÛŒ', 'Ù†</w>'),\n",
       " ('!', '</w>'),\n",
       " ('Ù…', 'Ù†</w>'),\n",
       " ('Øª', 'Ø§Ù…</w>'),\n",
       " ('Ùˆ', 'ÛŒ'),\n",
       " ('.', '\"</w>'),\n",
       " ('Ø±ÙˆØ²', 'ØŒ</w>'),\n",
       " ('Ùˆ', 'Ù†</w>'),\n",
       " ('Ù‡', 'Ù…'),\n",
       " ('Ù…ÛŒ\\u200c', 'ØªÙˆ'),\n",
       " ('Ú©', 'Ù…'),\n",
       " ('Ø®', 'Ø§Ù†'),\n",
       " ('Ú¯', 'ÛŒ</w>'),\n",
       " ('Ø¯', 'Ù†Ø¯</w>'),\n",
       " ('Ø´', 'Ø¯.</w>'),\n",
       " ('Ù¾ÛŒ', 'Ø¯Ø§</w>'),\n",
       " ('Ø§', 'Ù‡</w>'),\n",
       " ('ÛŒ', 'Ù‡</w>'),\n",
       " ('Ø¹', '</w>'),\n",
       " ('Ø´', 'Ø§Ù†</w>'),\n",
       " ('Øª', 'Ø§'),\n",
       " ('Ø¢Ù†', '\\u200cÙ‡Ø§</w>'),\n",
       " ('Ú†', 'ÛŒ'),\n",
       " ('Ù‚', '</w>'),\n",
       " ('Ù', '</w>'),\n",
       " ('Ù†', 'ÛŒ'),\n",
       " ('Ù†', 'Ø§Ù…</w>'),\n",
       " ('Ù…Ø§', 'Ø¯Ø±'),\n",
       " ('Ø¯Ø±', 'Ø®'),\n",
       " ('Ø¬', 'Ø§'),\n",
       " ('Ù¾Ø±', 'Ù†Ø¯Ù‡</w>'),\n",
       " ('Ù†', 'Ø§'),\n",
       " ('!', '\"</w>'),\n",
       " ('Ø¯', 'Ù†</w>'),\n",
       " ('Ù„', 'Ø§'),\n",
       " ('Ú†', 'Ù‡</w>'),\n",
       " ('Ø±', 'ÙØª'),\n",
       " ('Ø¨', 'Ø®'),\n",
       " ('Ùˆ', 'Ù‚'),\n",
       " ('Ø§Ø³', 'Ø¨Ø§'),\n",
       " ('Ø¯ÛŒ', 'Ø¯</w>'),\n",
       " ('Ú©Ù…', 'Ú©</w>'),\n",
       " ('Ú¯', 'Ø°'),\n",
       " ('ÛŒ', 'Ø§'),\n",
       " ('Ø³', 'Ø§'),\n",
       " ('Ù…Ø§Ø¯Ø±', 'Ø´</w>'),\n",
       " ('Ø¨', 'Ù„'),\n",
       " ('Ù…ÛŒ\\u200cØ®Ùˆ', 'Ø§Ø³Øª</w>'),\n",
       " ('Ø¨', 'Ø¹'),\n",
       " ('Ù…', 'Ø±'),\n",
       " ('Ø¯Ø§Ø´Øª', '.</w>'),\n",
       " ('Ø´', 'Ø¯</w>'),\n",
       " ('Ù‡', 'Ø±</w>'),\n",
       " ('Ø§Ø³Øª', '.</w>'),\n",
       " ('Ù‡', '.</w>'),\n",
       " ('Ø´', 'Øª'),\n",
       " ('Ù„', 'ÛŒ'),\n",
       " ('Ù‡', 'Ø§ÛŒ'),\n",
       " ('Ø¯', 'Ø®'),\n",
       " ('Ù¾', '</w>'),\n",
       " ('Ù…', '.</w>'),\n",
       " ('Ùˆ', 'Ø±'),\n",
       " ('Ø³', 'Ú¯</w>'),\n",
       " ('Ú¯', 'ÛŒ'),\n",
       " ('Ù„', 'Ùˆ'),\n",
       " ('Ú¯Ø±', 'Ø¨Ù‡</w>'),\n",
       " ('Ú©', 'Ø³</w>'),\n",
       " ('Ø®Ø§Ù†', 'Ù‡</w>'),\n",
       " ('Ø¯ÛŒ', 'Ø¯.</w>'),\n",
       " ('Ø§', 'Ø­'),\n",
       " ('Ù‡', 'ØŒ</w>'),\n",
       " ('Ù†Ú¯', 'Ø§Ù‡</w>'),\n",
       " ('Ú©Ù†', 'Ù†Ø¯.</w>'),\n",
       " ('Ø®', 'Ø±'),\n",
       " ('Ù‡\\u200c', 'Ø§ÛŒ</w>'),\n",
       " ('Ú©ÙˆÚ†', 'Ú©</w>'),\n",
       " ('Ù‡Ù…', 'Ù‡</w>'),\n",
       " ('Ù¾', 'Ø§Ø±'),\n",
       " ('Ø§', 'Ù‡'),\n",
       " ('Ú©', 'ÛŒ</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú©Ø±Ø¯.</w>'),\n",
       " ('ÙˆÙ‚', 'ØªÛŒ</w>'),\n",
       " ('Ø¯Ø±Ø®', 'Øª</w>'),\n",
       " ('Ø¯', 'Ù‡'),\n",
       " ('Ø§', 'Ø´</w>'),\n",
       " ('Ù‡', 'Ø±'),\n",
       " ('ØªÙˆ', 'Ù¾</w>'),\n",
       " ('Ø³', 'Ùˆ</w>'),\n",
       " ('Ùˆ', 'Ø§'),\n",
       " ('ØŸ', '\"</w>'),\n",
       " ('Ø´', 'Ø±Ùˆ'),\n",
       " ('Ø´Ø±Ùˆ', 'Ø¹</w>'),\n",
       " ('Ù„', 'Ø¨Ø®'),\n",
       " ('Ø¯ÛŒ', 'Ú¯Ø±</w>'),\n",
       " ('Ù…', 'Ù†'),\n",
       " ('Ú¯', 'Ùˆ'),\n",
       " ('Ø¨', 'Ù‡'),\n",
       " ('Ù„Ø¨Ø®', 'Ù†Ø¯</w>'),\n",
       " ('Ù…', 'ØŒ</w>'),\n",
       " ('Ù¾', 'Ø§'),\n",
       " ('Ø´', 'Ùˆ'),\n",
       " ('Ù', 'Ø§'),\n",
       " ('Ù‡', 'Ø³Øª'),\n",
       " ('Ù†Ø¯', 'Ú¯ÛŒ</w>'),\n",
       " ('Ø§Ø­', 'Ø³Ø§Ø³</w>'),\n",
       " ('Ø®', 'Øª'),\n",
       " ('\\u200c', 'Ù‡Ø§ÛŒ</w>'),\n",
       " ('Ø¨Ø§ÛŒ', 'Ø¯</w>'),\n",
       " ('Ú©ÙˆÚ†', 'Ùˆ'),\n",
       " ('Ø²', 'Ù†Ø¯Ú¯ÛŒ</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ø¢', 'Ù…'),\n",
       " ('Ø¨', 'Ù†</w>'),\n",
       " ('Ù¾Ø§Ø±', 'Ú©</w>'),\n",
       " ('Ø§', 'ÙˆÙ†</w>'),\n",
       " ('Ù', 'Ú©Ø±</w>'),\n",
       " ('Ù…', '.\"</w>'),\n",
       " ('Ø¨ÛŒ', 'Ø±ÙˆÙ†</w>'),\n",
       " ('Ø´', 'Ù‡</w>'),\n",
       " ('Ø¨', '\\u200c'),\n",
       " ('Ø´', 'Ø¯Ù‡</w>'),\n",
       " ('Ù…', 'Øª'),\n",
       " ('Ú¯ÛŒ', 'Ø±'),\n",
       " ('Ù¾', 'Ø³</w>'),\n",
       " ('ÛŒØ§', 'Ø¯</w>'),\n",
       " ('Ú©', 'Ø§'),\n",
       " ('Ø§Ø³Ø¨Ø§', 'Ø¨</w>'),\n",
       " ('Øª', 'ÛŒ'),\n",
       " ('Ø²', 'Ø¯</w>'),\n",
       " ('Ø¨Ø§Ø±', 'Ù‡</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯ØŒ</w>'),\n",
       " ('Ø¯', ':</w>'),\n",
       " ('Ø²', 'Ù‡</w>'),\n",
       " ('Øª', 'Ù…Ø§Ù…</w>'),\n",
       " ('Ø·', '</w>'),\n",
       " ('Ø¯', 'Ù†'),\n",
       " ('Ø¨Ø§', 'Ø²ÛŒ'),\n",
       " ('Ø¨Ø²Ø±', 'Ú¯ÛŒ</w>'),\n",
       " ('Ø´', 'Ø§'),\n",
       " ('Ø¯Ø§', 'Ø¯</w>'),\n",
       " ('Ù†', 'Ù…ÛŒ\\u200c'),\n",
       " ('Ø¨Ø§', 'Ø´'),\n",
       " ('Ø§ÛŒ', 'Ù†'),\n",
       " ('Ø±', 'ÛŒ'),\n",
       " ('Ø¨', 'Ø³ÛŒØ§Ø±</w>'),\n",
       " ('Øª', 'Øµ'),\n",
       " ('Ú¯Ùˆ', 'Ø´</w>'),\n",
       " ('Ù‡', 'ÛŒ'),\n",
       " ('Ø§', 'Ùˆ'),\n",
       " ('Ø¯', '!</w>'),\n",
       " ('Ø§Ø³Ø¨Ø§', 'Ø¨\\u200c'),\n",
       " ('Ø´', 'Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ø¯Ø§', 'Ø¯.</w>'),\n",
       " ('Øº', 'Ù…'),\n",
       " ('Ù†', 'Ù‡</w>'),\n",
       " ('Ù¾Ø±', 'Ø³ÛŒ'),\n",
       " ('Ø¬', 'Ø¹'),\n",
       " ('Ù…Ø§', 'Ø´ÛŒÙ†</w>'),\n",
       " ('Ø¯ÙˆØ³ØªØ§Ù†', 'Ø´</w>'),\n",
       " ('Ø¯Ùˆ', 'Ø¨Ø§Ø±Ù‡</w>'),\n",
       " ('Ø¯', 'Ù…</w>'),\n",
       " ('Ø¹', 'ÛŒ</w>'),\n",
       " ('Ú©', 'ÛŒ'),\n",
       " ('Ù…', 'Ú©Ø³</w>'),\n",
       " ('Ø§', 'ÙØªØ§'),\n",
       " ('Ø­', 'Ø±'),\n",
       " ('Â»', '</w>'),\n",
       " ('Ù…', 'Ù‡Ø±'),\n",
       " ('Ø§Ø³', 'Ù…</w>'),\n",
       " ('Ù†', 'Ùˆ'),\n",
       " ('Ø±ÙØª', '.</w>'),\n",
       " ('Ø±', 'Ù‡</w>'),\n",
       " ('Ø¨Ø¹', 'Ø¯ØŒ</w>'),\n",
       " ('Ø¨Ø±', 'Ú¯'),\n",
       " ('Ø³', 'Ù…Øª</w>'),\n",
       " ('Ù…', 'Ù‡</w>'),\n",
       " ('Ø§Ø³', 'Ù¾Ø§'),\n",
       " ('Ù¾', 'Ø³Ø±</w>'),\n",
       " ('Øº', 'Ø°'),\n",
       " ('Ø®Ùˆ', 'Ø¨ÛŒ</w>'),\n",
       " ('Ù…', '!\"</w>'),\n",
       " ('Ù…Ø§', 'Ù…Ø§Ù†'),\n",
       " ('Ø±', 'Ø³ÛŒ'),\n",
       " ('Ø¢', 'Ø¨</w>'),\n",
       " ('Ù‚', 'Ø§'),\n",
       " ('Ø¨Ø¹', 'Ø¯</w>'),\n",
       " ('Ø¬Ø¹', 'Ø¨Ù‡</w>'),\n",
       " ('Ø¨', 'Ú†Ù‡</w>'),\n",
       " ('Ú¯Ø°', 'Ø§'),\n",
       " ('Ú¯', 'Ù‡'),\n",
       " ('Ø²', 'Ø¯Ù‡</w>'),\n",
       " ('Ù‡Ù…ÛŒ', 'Ø´Ù‡</w>'),\n",
       " ('Ø²', 'Ø¯.</w>'),\n",
       " ('Ø¯', 'Ø´</w>'),\n",
       " ('Øº', '</w>'),\n",
       " ('Ø³', 'Ø¹ÛŒ</w>'),\n",
       " ('Øª', 'Ø¹'),\n",
       " ('ØºÙ…', 'Ú¯ÛŒÙ†</w>'),\n",
       " ('Ù†', 'Ø²'),\n",
       " ('Ù‚', 'Ø±'),\n",
       " ('Ø¯Ø§', 'Ø®'),\n",
       " ('Ù†Ø§Ø±Ø§Ø­', 'Øª</w>'),\n",
       " ('Ø¨Ø§', 'Ù„Ø§</w>'),\n",
       " ('Ø¯', 'Ø³Øª</w>'),\n",
       " ('Ù„Ùˆ', 'Ø³ÛŒ</w>'),\n",
       " ('Ù¾', 'Ùˆ'),\n",
       " ('Ù†Ø§', 'Ú¯Ù‡'),\n",
       " ('Ú©ÙˆÚ†Ùˆ', 'Ù„Ùˆ</w>'),\n",
       " ('Ø¬', 'Ùˆ'),\n",
       " ('Ø²ÛŒØ§', 'Ø¯ÛŒ</w>'),\n",
       " ('Ø§Ø³Ù¾Ø§', 'Øª</w>'),\n",
       " ('\\u200c', 'Ù‡Ø§ÛŒ'),\n",
       " ('ØªØµ', 'Ù…ÛŒÙ…</w>'),\n",
       " ('Ú†ÛŒ', 'Ø²'),\n",
       " ('Ù…Ø±', 'Ø¯</w>'),\n",
       " ('Ø³Øª', '.</w>'),\n",
       " ('Ø®', 'Ø§'),\n",
       " ('Ø¨', 'Ø¨ÛŒ'),\n",
       " ('Ù', 'Ù‚'),\n",
       " ('Øª', 'Ú©Ø§Ù†</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯Ù†Ø¯</w>'),\n",
       " ('Ù‚Ø±', 'Ù…'),\n",
       " ('Ø§', 'ØªØ§'),\n",
       " ('Ø¯Ø§Ù†', 'Ø³Øª</w>'),\n",
       " ('Ø¸', 'Ø±</w>'),\n",
       " ('Ùˆ', 'Ù„ÛŒ</w>'),\n",
       " ('Ø¯Ø§Ø®', 'Ù„</w>'),\n",
       " ('Ø³Øª', 'Ù‡</w>'),\n",
       " ('Ú©', 'Øª</w>'),\n",
       " ('Ø´', 'Ú©'),\n",
       " ('Ø¨Ù„', 'Ù†Ø¯</w>'),\n",
       " ('ØªØ§', 'Ø¨</w>'),\n",
       " ('Ø¬', 'ÛŒ'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú©Ø±Ø¯</w>'),\n",
       " ('Ø±ÙˆØ²ÛŒ', 'ØŒ</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú©Ø±Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ù…Ù†', 'Øª'),\n",
       " ('Ú†', 'ÙˆÙ†</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú¯ÙˆÛŒ'),\n",
       " ('Ø²', 'Ù…ÛŒÙ†</w>'),\n",
       " ('Ø§', 'Øª'),\n",
       " ('Ú©', 'Ø´ÛŒ'),\n",
       " ('Ø­Ø§', 'Ù„ÛŒ</w>'),\n",
       " ('Ú¯Ø°Ø§', 'Ø´Øª'),\n",
       " ('Ø¨Ù‡', 'ØªØ±ÛŒÙ†</w>'),\n",
       " ('Ø§Øª', 'ÙØ§'),\n",
       " ('Ù„', 'Ù‡</w>'),\n",
       " ('Ù†Ø²', 'Ø¯ÛŒÚ©</w>'),\n",
       " ('Ø¬', '</w>'),\n",
       " ('Ú©Ù†', 'Ù‡.</w>'),\n",
       " ('Ø¯Ø±', 'Ø³Øª</w>'),\n",
       " ('Ø®', '</w>'),\n",
       " ('Ø¨Ø§', 'Ù„</w>'),\n",
       " ('Ø®Ùˆ', 'Ø¨</w>'),\n",
       " ('Ø­', 'ÛŒ'),\n",
       " ('Ø¢Ù…', 'Ø¯.</w>'),\n",
       " ('Ø³', 'Ùˆ'),\n",
       " ('Øµ', 'Ø¯Ø§ÛŒ</w>'),\n",
       " ('Ú¯Ø±', 'Ù…</w>'),\n",
       " ('Ù…Ù‡Ø±', 'Ø¨Ø§Ù†</w>'),\n",
       " ('Ù', 'Ø±'),\n",
       " ('Ø¯Ø§Ø´Øª', 'Ù†Ø¯</w>'),\n",
       " ('Ù‚Ø±Ù…', 'Ø²</w>'),\n",
       " ('Ø¨', 'Ú¯ÛŒØ±'),\n",
       " ('Ù…Ø§', 'Ú©Ø³</w>'),\n",
       " ('Ù†', 'Ø´Ø§Ù†</w>'),\n",
       " ('Ù‚', 'Ø¯Ø±</w>'),\n",
       " ('Ø¬Ùˆ', 'Ø¯</w>'),\n",
       " ('Ø¯Ù‡', 'Ø¯.</w>'),\n",
       " ('Ø¨', 'Ú†'),\n",
       " ('Ø¬', 'Ø¯ÛŒØ¯</w>'),\n",
       " ('Ø³Øª', 'Ù†Ø¯</w>'),\n",
       " ('Ø¨Ø§', 'Ø¨</w>'),\n",
       " ('Ø²ÛŒ', 'Ø¨Ø§</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú©Ù†'),\n",
       " ('Ù…', 'Ø«'),\n",
       " ('Ø®', 'Ù†Ø¯ÛŒ'),\n",
       " ('Ú©ÙˆÚ†', 'Ú©ÛŒ</w>'),\n",
       " ('Ø³', 'Ù‡</w>'),\n",
       " ('Ø´Ùˆ', 'Ø¯.</w>'),\n",
       " ('Ø¨Ø²Ø±', 'Ú¯'),\n",
       " ('Ú¯ÙØª', '.</w>'),\n",
       " ('Ø·', 'ÙˆØ±</w>'),\n",
       " ('Ø§ÛŒÙ†', 'Ú©Ù‡</w>'),\n",
       " ('Ù¾Ø§ÛŒ', 'ÛŒÙ†</w>'),\n",
       " ('Ø´', 'Ø¯ØŒ</w>'),\n",
       " ('Ø¬', 'Ø¯ÛŒ'),\n",
       " ('Ùˆ', 'Ø¬ÙˆØ¯</w>'),\n",
       " ('Ø¢', 'ÙˆØ±'),\n",
       " ('Ø´', 'ØŒ</w>'),\n",
       " ('Ø®Ùˆ', 'Ø¯</w>'),\n",
       " ('Ø²ÛŒ', 'Ø±</w>'),\n",
       " ('Ø³', 'Ù†Ú¯</w>'),\n",
       " ('Ø¨', 'Øº'),\n",
       " ('Ø¨Ø±Ú¯', 'Ø´Øª'),\n",
       " ('Ø´', 'Ù†ÛŒ'),\n",
       " ('Ø§Ù†', 'Ø¬Ø§Ù…</w>'),\n",
       " ('Ø¬', 'Ù†Ú¯'),\n",
       " ('Ù…Ø«', 'Ù„</w>'),\n",
       " ('Ø§ÙØªØ§', 'Ø¯.</w>'),\n",
       " ('Ù…Ø±Ø§', 'Ù‚'),\n",
       " ('Ø¬Ø§', 'Ù„'),\n",
       " ('Ù…Ùˆ', 'Ø´</w>'),\n",
       " ('Ù¾', 'Ø³Ø±'),\n",
       " ('Ø¯ÙˆØ³Øª', 'Ø´</w>'),\n",
       " ('Ø§Ø³Ø¨Ø§Ø¨\\u200c', 'Ø¨Ø§Ø²ÛŒ</w>'),\n",
       " ('Ø®Ø±', 'Ø³</w>'),\n",
       " ('Ù…ÛŒ', 'Ø²</w>'),\n",
       " ('Ø²', 'Ùˆ'),\n",
       " ('Ø¢', 'ÛŒØ§</w>'),\n",
       " ('Ù…', 'ØŸ\"</w>'),\n",
       " ('Ù„', 'Ø°'),\n",
       " ('Ø¯', 'Ø³Øª'),\n",
       " ('Ø¹', 'Ø§'),\n",
       " ('ÛŒ', 'Ú©'),\n",
       " ('Ø´', 'Ø¯Ù†Ø¯</w>'),\n",
       " ('\"', 'Ù…Ù†</w>'),\n",
       " ('Ø®Ø±', 'Ú¯ÙˆØ´</w>'),\n",
       " ('Ø­', 'Ø´'),\n",
       " ('Ø­Ø±', 'Ù</w>'),\n",
       " ('Ø¢', 'Ø³'),\n",
       " ('Ø¯Ù†', 'Ø¨Ø§Ù„</w>'),\n",
       " ('Ø´', 'Ù…Ø§</w>'),\n",
       " ('ÙÙ‚', 'Ø·</w>'),\n",
       " ('Ø¯Ø§Ø±', 'Ø¯.</w>'),\n",
       " ('Ù‚', 'ÛŒ</w>'),\n",
       " ('Ø±ÙØª', 'Ù†Ø¯.</w>'),\n",
       " ('Ø¨Øº', 'Ù„</w>'),\n",
       " ('Ù‚', 'Ù‡</w>'),\n",
       " ('Ú¯', 'Ù„</w>'),\n",
       " ('Ú†', 'Ø±'),\n",
       " ('Ù„Ø°', 'Øª</w>'),\n",
       " ('Ø®Ùˆ', 'Ø¯Ø´</w>'),\n",
       " ('Ø§Ø³Ø¨Ø§Ø¨\\u200c', 'Ø¨Ø§Ø²ÛŒ'),\n",
       " ('Ø³Ø±ÛŒ', 'Ø¹</w>'),\n",
       " ('Ù†', '.</w>'),\n",
       " ('Ù…', 'Ø­'),\n",
       " ('Ø¹', 'Øµ'),\n",
       " ('Ù¾Ø±Ø³ÛŒ', 'Ø¯:</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯!</w>'),\n",
       " ('Ø·', 'Ø±'),\n",
       " ('Ø®ÙˆØ´', 'Ù…'),\n",
       " ('Øº', 'ÛŒ'),\n",
       " ('Ù†', 'Ø¨ÙˆØ¯.</w>'),\n",
       " ('Ø´ÛŒ', 'Ø¯</w>'),\n",
       " ('Ù¾', 'Ø¯Ø±'),\n",
       " ('Ø¨', 'Ø¯</w>'),\n",
       " ('Ù‡\\u200c', 'Ø§Ø´</w>'),\n",
       " ('Ø¨Ø±', 'Ø¯.</w>'),\n",
       " ('Ø¬', 'Ù‡</w>'),\n",
       " ('Øª', 'Ø´Ú©Ø±</w>'),\n",
       " ('Ù†', 'Ù‡'),\n",
       " ('Ø¸', 'Ø±'),\n",
       " ('Ø¨Ø§', 'Øº'),\n",
       " ('Ú¯Ø±ÙØª', 'Ù†Ø¯</w>'),\n",
       " ('Ù‚', 'Ø¯Ù…</w>'),\n",
       " ('Ù†', 'Ø¸Ø±</w>'),\n",
       " ('Ø¢Ø³', 'Ù…Ø§Ù†</w>'),\n",
       " ('Ø¯Ø§Ø´Øª', 'Ù‡</w>'),\n",
       " ('Ù‡Ø³Øª', 'Ù†Ø¯.</w>'),\n",
       " ('Ù†', 'Ø´'),\n",
       " ('Ø§ØªÙØ§', 'Ù‚</w>'),\n",
       " ('ØªØ±Ø³ÛŒ', 'Ø¯Ù‡</w>'),\n",
       " ('Ù†Ø§Ú¯Ù‡', 'Ø§Ù†</w>'),\n",
       " ('Ø¨', 'Øª</w>'),\n",
       " ('Ø®', 'ÛŒ'),\n",
       " ('Ù', 'ÛŒ'),\n",
       " ('Ø³', 'ØŒ</w>'),\n",
       " ('Ø¹', 'Ø±Ùˆ'),\n",
       " ('Ø¨Ù„', 'Ù‡ØŒ</w>'),\n",
       " ('Ù”', '</w>'),\n",
       " ('Ø­ÛŒ', 'ÙˆØ§Ù†Ø§Øª</w>'),\n",
       " ('Ø¢', 'Ø¨ÛŒ</w>'),\n",
       " ('Ø¢', 'Ø´'),\n",
       " ('\\u200cÙ‡Ø§ÛŒ', 'Ø´</w>'),\n",
       " ('Ù„', 'Ø´</w>'),\n",
       " ('Ù‹', '</w>'),\n",
       " ('Ø­Ø§', 'Ù„Ø§</w>'),\n",
       " ('Ø¯Ø§Ø´Øª', 'Ù†Ø¯.</w>'),\n",
       " ('Ø¨Ø±', 'Ø¯Ø§Ø´Øª</w>'),\n",
       " ('Ø³', 'Ù…</w>'),\n",
       " ('Ø²', 'Ø±'),\n",
       " ('Øº', 'Ùˆ'),\n",
       " ('.', 'Â»</w>'),\n",
       " ('Ø§ØªØ§', 'Ù‚</w>'),\n",
       " ('Ù…', 'Ø´</w>'),\n",
       " ('Ù†', 'Ù…ÛŒ</w>'),\n",
       " ('Ù†', 'Ø±'),\n",
       " ('Ù‚', 'Øµ'),\n",
       " ('Ø¹Ø±Ùˆ', 'Ø³'),\n",
       " ('Ø±ÙˆØ²', 'Ú¯'),\n",
       " ('Ø¯ÙˆÛŒ', 'Ø¯</w>'),\n",
       " ('Øº', 'Ø§'),\n",
       " ('\\u200c', 'ØªØ±</w>'),\n",
       " ('Ø­Ø´', 'Ø±Ù‡</w>'),\n",
       " ('Ù…', 'Ø¹'),\n",
       " ('Ø¯ÛŒ', 'Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ú¯Ø°', 'Ø±Ø§Ù†Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ú†ÛŒØ²', 'Ù‡Ø§ÛŒ</w>'),\n",
       " ('Ø±', 'Ù†Ú¯</w>'),\n",
       " ('Øµ', 'Ø¯Ø§</w>'),\n",
       " ('Ù‡', 'Ø§'),\n",
       " ('Ø§', 'Ù…ÛŒ</w>'),\n",
       " ('Ú©', 'Ø´</w>'),\n",
       " ('\"', 'Ø³'),\n",
       " ('Ù…', '!</w>'),\n",
       " ('Ø®ÙˆØ´Ù…', 'Ø²Ù‡</w>'),\n",
       " ('Ù†', 'ØŒ</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ø¯Ø§Ù†Ø³Øª</w>'),\n",
       " ('Øª', 'Ù…Ø§'),\n",
       " ('Ø¬', 'Ø¨</w>'),\n",
       " ('Ù…ÛŒ\\u200cØªÙˆØ§Ù†', 'Ù…</w>'),\n",
       " ('ØŸ', '</w>'),\n",
       " ('Ø¯', '.\"</w>'),\n",
       " ('Ú†', '</w>'),\n",
       " ('Ø¹Øµ', 'Ø¨Ø§Ù†ÛŒ</w>'),\n",
       " ('Ú†', 'Ùˆ'),\n",
       " ('Ø±Ø³ÛŒ', 'Ø¯.</w>'),\n",
       " ('Ù„ÙˆÛŒ', 'ÛŒ</w>'),\n",
       " ('Ù„ÛŒ', 'Ù„ÛŒ'),\n",
       " ('Ø¢Ù…', 'Ø¯</w>'),\n",
       " ('Ø§', 'Ø´ØªØ±Ø§'),\n",
       " ('Ø¹', 'Ù‡</w>'),\n",
       " ('\\u200c', 'Ø²Ø¯Ù‡</w>'),\n",
       " ('Ú©Ù†', 'Ù†Ø¯</w>'),\n",
       " ('Ù…Ø±Ø§Ù‚', 'Ø¨</w>'),\n",
       " ('Ø§Ø³Øª', 'ÙØ§'),\n",
       " ('ØºÛŒ', 'Ø±'),\n",
       " ('Ù¾', 'Ø´Øª</w>'),\n",
       " ('Ø§Ø³ØªÙØ§', 'Ø¯Ù‡</w>'),\n",
       " ('Ø²', 'Ø¯Ù†</w>'),\n",
       " ('Ø³Ø±', 'Ú¯Ø±Ù…</w>'),\n",
       " ('Ù„', 'Ø·'),\n",
       " ('Ø¬Ø§', 'Ø¯ÙˆÛŒ'),\n",
       " ('Øµ', 'ÙˆØ±'),\n",
       " ('Ù†', 'Ù†Ø¯.</w>'),\n",
       " ('Ø¢', 'Ø±'),\n",
       " ('Ù‚', 'ÛŒ'),\n",
       " ('Ø¨Ø§', 'Ù„Ø§ÛŒ</w>'),\n",
       " ('Ù‚', 'ÙˆØ±'),\n",
       " ('Ú©ÙˆÚ†Ùˆ', 'Ù„ÙˆÛŒÛŒ</w>'),\n",
       " ('ØºØ°', 'Ø§</w>'),\n",
       " ('Ø´Ø§', 'Ø¯</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ø²'),\n",
       " ('Ù¾Ø¯Ø±', 'Ø´</w>'),\n",
       " ('Ù‚ÙˆØ±', 'Ø¨Ø§Øº'),\n",
       " ('Ù†', 'Ù…</w>'),\n",
       " ('\\u200c', 'Ø§Ø´</w>'),\n",
       " ('Ú©', 'ØŒ</w>'),\n",
       " ('\"', 'Ø¨Ù„Ù‡ØŒ</w>'),\n",
       " ('Ø§Ø´ØªØ±Ø§', 'Ú©</w>'),\n",
       " ('Ø±', 'Ù†Ú¯'),\n",
       " ('ØºÛŒØ±', 'Ù…Ù†Øª'),\n",
       " ('ØªØ¹', 'Ø¬Ø¨</w>'),\n",
       " ('ÙˆÛŒ', 'Ú˜'),\n",
       " ('Ú¯', 'Ø§Ù‡</w>'),\n",
       " ('Ù„', 'Ù…</w>'),\n",
       " ('Ø­Ø±', 'Ú©Øª</w>'),\n",
       " ('Øª', 'Ù…ÛŒØ²</w>'),\n",
       " ('Ù‡ÛŒ', 'Ø¬Ø§Ù†</w>'),\n",
       " ('Ù‡Ø±', 'Ú¯'),\n",
       " ('Ú†', 'Ø·ÙˆØ±</w>'),\n",
       " ('Ù‡Ø±Ú¯', 'Ø²</w>'),\n",
       " ('Ø´', 'Ù†</w>'),\n",
       " ('Ú†', 'Ø§'),\n",
       " ('Ù¾Ø³Ø±', 'Ø¨Ú†Ù‡</w>'),\n",
       " ('Ù‡\\u200c', 'ÛŒ</w>'),\n",
       " ('!', 'Â»</w>'),\n",
       " ('Ø¨Ù‡', 'ØªØ±</w>'),\n",
       " ('Ù…ÛŒ', 'Ùˆ'),\n",
       " ('Ø¢', 'ÙØªØ§'),\n",
       " ('Ø­', 'ØªÛŒ</w>'),\n",
       " ('Ø¬Ø§Ø¯ÙˆÛŒ', 'ÛŒ</w>'),\n",
       " ('Ù…Øª', 'Ù†</w>'),\n",
       " ('Ù¾', 'Ø³ØŒ</w>'),\n",
       " ('ØªÙˆØ§Ù†', 'Ø³Øª</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú©Ø±Ø¯Ù†Ø¯</w>'),\n",
       " ('Ù‚Ø§ÛŒ', 'Ù‚</w>'),\n",
       " ('Ø¢Ù†', 'Ø¬Ø§</w>'),\n",
       " ('Ø¯Ø±Ø®', 'Ø´Ø§Ù†</w>'),\n",
       " ('Ù…ÙˆØ±', 'Ø¯</w>'),\n",
       " ('Ø´Ø§ÛŒ', 'Ø¯</w>'),\n",
       " ('Ù¾', 'Ø´'),\n",
       " ('Ù…Ø±', 'Ø¯Ù…</w>'),\n",
       " ('Ø®ÙˆØ´', 'Ø­Ø§Ù„ÛŒ</w>'),\n",
       " ('Ø·', 'Ùˆ'),\n",
       " ('Ù„', 'Ú©Ù‡</w>'),\n",
       " ('Ú¯', 'Ù„'),\n",
       " ('Ø¯Ø§Ø±', 'Ø¯</w>'),\n",
       " ('Ù‚', 'Ø·'),\n",
       " ('Ø±ÙØª', 'Ù†Ø¯</w>'),\n",
       " ('Ø²', 'Ø´</w>'),\n",
       " ('Ú©', 'Ø³ÛŒ</w>'),\n",
       " ('Ù‚ÙˆØ±Ø¨Ø§Øº', 'Ù‡</w>'),\n",
       " ('Ø³', 'Ù†'),\n",
       " ('Ø§ÛŒ', 'Ø¯Ù‡</w>'),\n",
       " ('Ú¯', 'Ø´Øª'),\n",
       " ('Ø­', '</w>'),\n",
       " ('Ø³Ø§', 'Ù„ÛŒ</w>'),\n",
       " ('Ø´ÛŒ', 'Ø±</w>'),\n",
       " ('Ø¬Ø¯ÛŒ', 'Ø¯ÛŒ</w>'),\n",
       " ('Ú©ÛŒ', 'ØªÛŒ</w>'),\n",
       " ('Ø³Ø±', 'Ø´</w>'),\n",
       " ('Ø§ØªØ§', 'Ù‚'),\n",
       " ('Ú†', 'Ø´'),\n",
       " ('Ú©', 'ØªØ§Ø¨</w>'),\n",
       " ('ÙˆÙ‚', 'Øª</w>'),\n",
       " ('\"', 'Ø§ÛŒÙ†</w>'),\n",
       " ('Ù†ÛŒ', 'Ø³Øª.</w>'),\n",
       " ('Ú¯ÙØª', ':'),\n",
       " ('Ø®ÙˆØ±', 'Ø´ÛŒØ¯</w>'),\n",
       " ('Ø¹', 'Øª</w>'),\n",
       " ('Ù¾Ø±ÛŒ', 'Ø¯</w>'),\n",
       " ('Ú¯', 'Ù…</w>'),\n",
       " ('Ø´', 'Ø¬Ø§'),\n",
       " ('Ø§', 'Ú¯Ø±</w>'),\n",
       " ('Ù¾', 'Ø²'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ø¨ÛŒ'),\n",
       " ('Ø³Ù†', 'Ø¬Ø§'),\n",
       " ('Ø¯ÙˆÛŒ', 'Ø¯.</w>'),\n",
       " ('Ø®Ù†Ø¯ÛŒ', 'Ø¯Ù†Ø¯</w>'),\n",
       " ('Ø¬', 'Ù…Ù‡</w>'),\n",
       " ('Øª', 'Ù„Ø§Ø´</w>'),\n",
       " ('Ù…ÛŒ\\u200cÚ¯ÙˆÛŒ', 'Ø¯:</w>'),\n",
       " ('Ù…', 'Ø³Ø§'),\n",
       " ('Ø§', 'Ø·Ø±Ø§'),\n",
       " ('Ø§', 'Ù…ÛŒ'),\n",
       " ('Ø¨Ø§', 'Ø¯</w>'),\n",
       " ('Ø³', 'Ø®Øª</w>'),\n",
       " ('Ø¨', 'Ù†Ø§'),\n",
       " ('Ø¸Ø±', 'Ù‡\\u200cØ§ÛŒ</w>'),\n",
       " ('ÛŒÚ©', 'Ø¯ÛŒÚ¯Ø±</w>'),\n",
       " ('Ù', 'Ù„'),\n",
       " ('\"', 'Ù…'),\n",
       " ('Ú¯Ø°Ø§Ø´Øª', '.</w>'),\n",
       " ('Ø¬', 'Ú©</w>'),\n",
       " ('Ø¨', 'Ø´'),\n",
       " ('Ø­ÛŒØ§', 'Ø·</w>'),\n",
       " ('Ø·Ø±', 'Ù</w>'),\n",
       " ('Øª', 'Ø´</w>'),\n",
       " ('Ù…', 'Ø¯'),\n",
       " ('ÙˆØ§', 'Ù‚'),\n",
       " ('Ø³Ø§', 'Ø®Øª'),\n",
       " ('Ù', 'Ø¸'),\n",
       " ('Ø´', 'Ù‡Ø±</w>'),\n",
       " ('Ù‡Ù…', 'Ú†'),\n",
       " ('Ú©', 'Ù„Ø§Ù‡</w>'),\n",
       " ('Ø¬Ø§Ù„', 'Ø¨</w>'),\n",
       " ('Ø§', 'Ø¯Ø§'),\n",
       " ('Ú©Ù†', 'Ø¬'),\n",
       " ('Ù', 'Ù‡Ù…ÛŒ'),\n",
       " ('Ø¨Ø§', 'Ø¯'),\n",
       " ('Ù…', 'Ø´'),\n",
       " ('Ø®ÙˆØ§Ù‡', 'Ø¯</w>'),\n",
       " ('Ø§Ø¯Ø§', 'Ù…Ù‡</w>'),\n",
       " ('ØªØ±', 'Ø¬Ù…Ù‡</w>'),\n",
       " ('Ø±', 'Ù‚Øµ'),\n",
       " ('Ù…ØªÙˆ', 'Ø¬Ù‡</w>'),\n",
       " ('Ø²', 'Ø§'),\n",
       " ('Ø´', 'Ù†Ø§</w>'),\n",
       " ('Ù‚', 'ÙˆÛŒ</w>'),\n",
       " ('ØºÛŒØ±Ù…Ù†Øª', 'Ø¸Ø±Ù‡\\u200cØ§ÛŒ</w>'),\n",
       " ('Ù‡Ø§ÛŒ', 'Ø´</w>'),\n",
       " ('Ø¹', 'Ù„Ø§'),\n",
       " ('Ø¨Ú¯ÛŒØ±', 'Ø¯.</w>'),\n",
       " ('Ù†ÛŒ', 'Ø³Øª'),\n",
       " ('Ù„', 'Ø¨Ø§Ø³</w>'),\n",
       " ('Ø´', 'Ø¯Ù†</w>'),\n",
       " ('Ø¨ÛŒ', 'Ø´ØªØ±</w>'),\n",
       " ('Ù', 'Ø´</w>'),\n",
       " ('Ø¨Ø±Ùˆ', 'Ø¯.</w>'),\n",
       " ('Ø§', 'Ù„'),\n",
       " ('Ù‚', 'Ø´'),\n",
       " ('Ø¢', 'ÛŒ'),\n",
       " ('Ù…Ø­', 'Ú©Ù…</w>'),\n",
       " ('Ø®Ùˆ', 'Ø¯'),\n",
       " ('Ù…ØªÙˆÙ‚', 'Ù</w>'),\n",
       " ('Ù‡', 'Ù†Ú¯'),\n",
       " ('Ù…Ù†Øª', 'Ø¸Ø±</w>'),\n",
       " ('Ù‡', 'Ù”</w>'),\n",
       " ('Ú†Ùˆ', 'Ø¨</w>'),\n",
       " ('Ø®', 'Ø³ØªÙ‡</w>'),\n",
       " ('Ù…', 'Ø®'),\n",
       " ('Ù…Ø³Ø§', 'Ø¨'),\n",
       " ('Ø¨', 'Ø¯'),\n",
       " ('Ø±ÙˆØ²Ú¯', 'Ø§Ø±ÛŒ'),\n",
       " ('Ù…Ø§', 'Ø´ÛŒ'),\n",
       " ('Ù†Ø±', 'Ù…</w>'),\n",
       " ('Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ', 'ØŒ</w>'),\n",
       " ('Ø¯Ø®ØªØ±', 'Ú©</w>'),\n",
       " ('Ù‚', 'Ù„'),\n",
       " ('Ø§', 'Ù…Øª'),\n",
       " ('Ø¯ÛŒ', 'Ø¯Ù†Ø¯</w>'),\n",
       " ('Ù…Ø§', 'Ø¬'),\n",
       " ('Ø¢Ù†', 'Ù‚Ø¯Ø±</w>'),\n",
       " ('Ù‡', 'Ù†Ùˆ'),\n",
       " ('Ø¯ÛŒ', 'Ú¯Ø±Ø§Ù†</w>'),\n",
       " ('Ø«', '</w>'),\n",
       " ('Ù¾', 'Ù†'),\n",
       " ('Ø¯ÛŒ', 'Ú¯Ø±ÛŒ</w>'),\n",
       " ('Øª', 'Ù¾'),\n",
       " ('Ù‡', 'Ùˆ'),\n",
       " ('Ù„', 'Ø­'),\n",
       " ('ÙˆÛŒÚ˜', 'Ù‡</w>'),\n",
       " ('Ú©Ù†', 'Ù†Ø¯Ù‡</w>'),\n",
       " ('Ø§', 'Ø¬Ø§'),\n",
       " ('Ú©Ù†ÛŒ', 'Ù….\"</w>'),\n",
       " ('Ù‡Ø§ÛŒ', 'ÛŒ</w>'),\n",
       " ('Ø¢Ø´', 'Ù¾Ø²'),\n",
       " ('Ø®Ø§Ù†ÙˆØ§', 'Ø¯Ù‡</w>'),\n",
       " ('Ø±ÛŒ', 'Ø®Øª'),\n",
       " ('Ø¨Ù„', 'Ù†Ø¯ÛŒ</w>'),\n",
       " ('Ø¯Ø§Ù†', 'Ø³ØªÙ†Ø¯</w>'),\n",
       " ('Ù‡Ù†Ùˆ', 'Ø²</w>'),\n",
       " ('Ø®', 'Ù‡</w>'),\n",
       " ('Ø¯', 'Ø¹'),\n",
       " ('Ø³', 'Ø¨Ø²</w>'),\n",
       " ('Ø±ÙØª', 'Ù†</w>'),\n",
       " ('Ú©', 'Ø´'),\n",
       " ('Ø®ÙˆØ±', 'Ø¯Ù†</w>'),\n",
       " ('Ù…', 'Ø³</w>'),\n",
       " ('Ù…', '\\u200c'),\n",
       " ('Ø«', 'ÛŒ'),\n",
       " ('Ø¢Ù…Ø§', 'Ø¯Ù‡</w>'),\n",
       " ('Ø¶', '</w>'),\n",
       " ('Ø°', 'Ø±'),\n",
       " ('Ø®ÙˆØ±', 'Ø¯</w>'),\n",
       " ('Ø´Ø§', 'Ø¯ÛŒ</w>'),\n",
       " ('Ù‚', 'Ùˆ'),\n",
       " ('Ú©Ù†Ø¬', 'Ú©Ø§Ùˆ</w>'),\n",
       " ('\"', 'Ø¨ÛŒØ§</w>'),\n",
       " ('Ø¯ÙˆØ³Øª', 'Ø´ØŒ</w>'),\n",
       " ('Ø¬ÙˆØ§', 'Ø¨</w>'),\n",
       " ('Ø¨', 'Ù¾Ø±'),\n",
       " ('Ø¨ÛŒ', 'Ø´ØªØ±ÛŒ</w>'),\n",
       " ('Ø¯Ø§Ø±', 'Ù†Ø¯</w>'),\n",
       " ('Ø²Ùˆ', 'Ø¯ÛŒ</w>'),\n",
       " ('Ù¾Ø±Ø³ÛŒ', 'Ø¯</w>'),\n",
       " ('Ø³Ù†Ø¬Ø§', 'Ø¨</w>'),\n",
       " ('Ø¨', 'Ø³Ø§'),\n",
       " ('Ø¯ÛŒ', 'Ú¯Ù‡</w>'),\n",
       " ('Ø´', 'Ø¯!</w>'),\n",
       " ('Ø¯Ø§', 'Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ù„Ø·', 'ÙØ§'),\n",
       " ('Ø¨ÛŒ', 'ØŒ</w>'),\n",
       " ('Ø²ÛŒ', 'Ø¨Ø§ÛŒ'),\n",
       " ('ØªÛŒ', 'Ù…ØŒ</w>'),\n",
       " ('Ù†', 'Ø³Øª</w>'),\n",
       " ('Ù…', 'Ø¬'),\n",
       " ('Ù…Ø±', 'Øª'),\n",
       " ('Øµ', 'Ù†Ø¯'),\n",
       " ('Ù…Ø³Ø§Ø¨', 'Ù‚Ù‡</w>'),\n",
       " ('Ø§Ø·Ø±Ø§', 'Ù</w>'),\n",
       " ('Ù…ÛŒ', 'Ø±</w>'),\n",
       " ('Ù†', 'Ù‚Ø§'),\n",
       " ('Ø§', 'Ø¨Ø¯</w>'),\n",
       " ('ÙØ±Ùˆ', 'Ø´'),\n",
       " ('Øº', 'Ø±ÙˆØ±</w>'),\n",
       " ('Ø´Ù†ÛŒ', 'Ø¯.</w>'),\n",
       " ('ØºÙˆ', 'Ù„</w>'),\n",
       " ('Ø­', 'Ùˆ'),\n",
       " ('Ø´Ø¬Ø§', 'Ø¹</w>'),\n",
       " ('Ø§Ùˆ', 'Ù†Ø§</w>'),\n",
       " ('\"', '.</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ú©Ø±Ø¯ØŒ</w>'),\n",
       " ('Ø¨ÛŒ', '\\u200c'),\n",
       " ('Ú©Ø§', 'ØºØ°'),\n",
       " ('Ø§Ù…Øª', 'Ø­Ø§Ù†</w>'),\n",
       " ('Øª', 'Ù†Ù‡Ø§</w>'),\n",
       " ('Ø³Ø±', 'Ø³Ø±Ù‡</w>'),\n",
       " ('Ø¯Ø±', 'Ø¨Ø§Ø±Ù‡</w>'),\n",
       " ('Ø§Ø±', 'Ø¯'),\n",
       " ('Ù…', 'Ù„Ø§'),\n",
       " ('Ø´', 'Ú¯ÙØª'),\n",
       " ('Ø¹', 'Ø¬ÛŒ'),\n",
       " ('Ú©', 'Ø«ÛŒ'),\n",
       " ('Ø³ÛŒ', 'Ø¨</w>'),\n",
       " ('Øµ', 'Ø­'),\n",
       " ('ØªÙ…Ø§', 'Ø´Ø§</w>'),\n",
       " ('\\u200c', '</w>'),\n",
       " ('Ù…', 'Ø²Ø±'),\n",
       " ('Ø¨Ø±', 'Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ø¯ÛŒ', 'Ø¯Ù†</w>'),\n",
       " ('Ø§ÛŒÙ†', 'Ø¬Ø§</w>'),\n",
       " ('Ù¾Ø´', 'Ù…Ø§Ù„Ùˆ</w>'),\n",
       " ('Ù‡Ø§', 'ØŒ</w>'),\n",
       " ('Ø¯Ø±', 'Ø³</w>'),\n",
       " ('Ø¨', 'Ø³Øª'),\n",
       " ('Ø¢Ø´Ù¾Ø²', 'Ø®Ø§Ù†Ù‡</w>'),\n",
       " ('Ø®', 'Ù„Ø§'),\n",
       " ('Ø´Ùˆ', 'Ù†Ø¯.</w>'),\n",
       " ('Ø³', 'Ø¯.</w>'),\n",
       " ('Ù„Ø§', 'Ù…ØŒ</w>'),\n",
       " ('ØªÙ¾', 'Ù‡</w>'),\n",
       " ('\"', 'Ù†Ù‡ØŒ</w>'),\n",
       " ('Ø®ÙˆØ§', 'Ø¨</w>'),\n",
       " ('Ø®Ùˆ', 'Ù†Ù‡</w>'),\n",
       " ('ØŸ', 'Â»</w>'),\n",
       " ('Ú©ÙˆÚ†', 'Ú©ØŒ</w>'),\n",
       " ('Ø§Ø¬Ø§', 'Ø²Ù‡</w>'),\n",
       " ('Ø±Ù‚Øµ', 'ÛŒ'),\n",
       " ('Ø¨', 'ØŒ</w>'),\n",
       " ('Ø¨Ø®ÙˆØ±', 'Ø¯.</w>'),\n",
       " ('Ù…ÙˆØ§', 'ÙÙ‚'),\n",
       " ('Ù…Ø±Øª', 'Ø¨</w>'),\n",
       " ('Ø²', 'Ù†</w>'),\n",
       " ('Ù…ÙˆØ§ÙÙ‚', 'Øª</w>'),\n",
       " ('Ù‡ÛŒ', 'Ø¬Ø§Ù†'),\n",
       " ('Ú©Ø´ÛŒ', 'Ø¯.</w>'),\n",
       " ('Ú¯', 'Ø§Ù‡ÛŒ</w>'),\n",
       " ('Ø¢ÙØªØ§', 'Ø¨ÛŒØŒ</w>'),\n",
       " ('Ù…', 'Ø·'),\n",
       " ('Ø¢Ø±', 'Ø²Ùˆ</w>'),\n",
       " ('Ø²', 'Ù†'),\n",
       " ('ØµÙ†Ø¯', 'Ù„ÛŒ</w>'),\n",
       " ('Ù„', 'Ù…Ø³</w>'),\n",
       " ('Ø¯Ø§', 'Ø¯Ù†Ø¯</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯Ù†</w>'),\n",
       " ('Ø¯Ø§Ø±', 'Ù†Ø¯.</w>'),\n",
       " ('Ú†', 'Ù‚Ø¯Ø±</w>'),\n",
       " ('Ø²ÛŒØ¨Ø§ÛŒ', 'ÛŒ</w>'),\n",
       " ('Øª', 'ØŒ</w>'),\n",
       " ('Ú©', 'Ù„'),\n",
       " ('Ø®Ø§', 'Ø·Ø±</w>'),\n",
       " ('Ø°', 'Ù‡'),\n",
       " ('Ú†', 'Ù†Ø¯</w>'),\n",
       " ('ÙÙ‡Ù…ÛŒ', 'Ø¯</w>'),\n",
       " ('Øµ', 'Ø¯Ø§ÛŒ'),\n",
       " ('Ø¨Ú†', 'Ù‡\\u200cÙ‡Ø§</w>'),\n",
       " ('Ø¬', 'Ù…Ø¹</w>'),\n",
       " ('ØºØ§', 'ÙÙ„'),\n",
       " ('Ø¢', 'ØºÙˆ'),\n",
       " ('Ø®', 'Ø¯Ø§Ø­Ø§'),\n",
       " ('Ú†', 'Ø³'),\n",
       " ('Ø®Ø¯Ø§Ø­Ø§', 'ÙØ¸'),\n",
       " ('Ø²', 'Ø¯Ù†Ø¯</w>'),\n",
       " ('Ø¯Ø§Ø±', 'Ù…</w>'),\n",
       " ('Ø®ÙˆØ´', 'Ø´</w>'),\n",
       " ('Ú©Ø«ÛŒ', 'Ù</w>'),\n",
       " ('Ú©Ù†', 'Ù….\"</w>'),\n",
       " ('Ø¢ÙˆØ±', 'Ø¯.</w>'),\n",
       " ('\"Ø³', 'Ù„Ø§Ù…</w>'),\n",
       " ('Ø¨ÛŒØ§ÛŒ', 'ÛŒ'),\n",
       " ('Ø³', 'Ú¯'),\n",
       " ('Ø¹Ø±ÙˆØ³', 'Ú©</w>'),\n",
       " ('Ù…', '.Â»</w>'),\n",
       " ('Ù¾Ù†', 'Ø¬'),\n",
       " ('Ø§Ù†', 'Øª'),\n",
       " ('Ø¢ØºÙˆ', 'Ø´</w>'),\n",
       " ('Ù†', 'ØªÙˆØ§Ù†Ø³Øª</w>'),\n",
       " ('Ú¯ÙØª', 'Ù†Ø¯:</w>'),\n",
       " ('e', '</w>'),\n",
       " ('Ø´', 'Ø±'),\n",
       " ('Ø¨Ù†Ø§', 'Ø¨Ø±Ø§ÛŒÙ†</w>'),\n",
       " ('Ú©Ù†ÛŒ', 'Ù…!\"</w>'),\n",
       " ('Ø³ÙˆØ±Ø§', 'Ø®</w>'),\n",
       " ('Ú©Ù†ÛŒ', 'Ù….</w>'),\n",
       " ('Ø¢', 'ÙˆØ§Ø²</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¨Ùˆ</w>'),\n",
       " ('Ø¨Ø§', 'Ø¨Ø§</w>'),\n",
       " ('Ù¾Ùˆ', 'Ø´ÛŒ'),\n",
       " ('Ù…Ø§Ø¬', 'Ø±Ø§'),\n",
       " ('Ù…Ø²Ø±', 'Ø¹Ù‡</w>'),\n",
       " ('Ù…', 'Ú©'),\n",
       " ('Ø´', 'Ù‚</w>'),\n",
       " ('\\u200cÙ‡Ø§ÛŒ', 'Ø´Ø§Ù†</w>'),\n",
       " ('Ø¨Ø§Ø´', 'Ù†Ø¯.</w>'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ø¯Ùˆ'),\n",
       " ('Ú†ÛŒØ²', 'Ù‡Ø§</w>'),\n",
       " ('Ù†', 'Ú©Ø±Ø¯.</w>'),\n",
       " ('ØºÙ…', 'Ú¯ÛŒ'),\n",
       " ('ØªÛŒ', 'Ù…ÛŒ</w>'),\n",
       " ('Ú©Ù†', 'Ù….</w>'),\n",
       " ('Ø´ÛŒ', 'Ø±ÛŒ'),\n",
       " ('ØªØ¹', 'Ù…ÛŒØ±</w>'),\n",
       " ('Ø¯Ø§', 'Ø¯Ù†</w>'),\n",
       " ('Ù…ÛŒÙˆ', 'Ù‡</w>'),\n",
       " ('Ù…', 'Ù„</w>'),\n",
       " ('Ø°', 'ÛŒ'),\n",
       " ('ØµØ­', 'Ø¨Øª</w>'),\n",
       " ('Ø¯', '!\"</w>'),\n",
       " ('Ø¨Ø§', 'Ø¹'),\n",
       " ('Ø§ØªÙØ§', 'Ù‚ÛŒ</w>'),\n",
       " ('Ù†Ù‚Ø§', 'Ø´ÛŒ</w>'),\n",
       " ('Ø¨Ø§Ø¹', 'Ø«</w>'),\n",
       " ('Ù‡ÛŒØ¬Ø§Ù†', '\\u200cØ²Ø¯Ù‡</w>'),\n",
       " ('Ø§ÛŒ', 'Ù…ÛŒ</w>'),\n",
       " ('Ø®ÛŒ', 'Ø³</w>'),\n",
       " ('ØªØ§', 'Ù…ØŒ</w>'),\n",
       " ('Ø§ÛŒ', 'Ø¬Ø§'),\n",
       " ('Ø§ÛŒØ¬Ø§', 'Ø¯</w>'),\n",
       " ('Ø°Ù‡', 'Ù†Ø´</w>'),\n",
       " ('Ú©Ù†', 'Ù…ØŸ\"</w>'),\n",
       " ('ÙØ±Ø§', 'Ù…ÙˆØ´</w>'),\n",
       " ('Øª', 'Ø¨Ø¯ÛŒ'),\n",
       " ('Ù¾ÛŒ', 'Ø±</w>'),\n",
       " ('ØªØ¨Ø¯ÛŒ', 'Ù„</w>'),\n",
       " ('Ù†', 'Ø¨ÙˆØ¯ØŒ</w>'),\n",
       " ('Ù…', 'Ù‡Ù…</w>'),\n",
       " ('Ù„ÛŒÙ„ÛŒ', 'ØŒ</w>'),\n",
       " ('Ù‚', 'Ø¨Ù„</w>'),\n",
       " ('Øª', 'Ù‚'),\n",
       " ('Ø²', 'Ø¯Ù†Ø¯.</w>'),\n",
       " ('Ø¨Ø²Ø±Ú¯', 'ØŒ</w>'),\n",
       " ('Ø®ÙˆØ´', 'Ø¨Ø®Øª</w>'),\n",
       " ('Ø¨Ø±', 'Ø¯</w>'),\n",
       " ('Ø®ÙˆØ±', 'Ø¯.</w>'),\n",
       " ('Ø¢', 'Ù‚Ø§ÛŒ</w>'),\n",
       " ('Ù‚', 'Ù'),\n",
       " ('Ø¬Ø¯ÛŒ', 'Ø¯Ø´</w>'),\n",
       " ('Ø®Ø¯Ø§Ø­Ø§ÙØ¸', 'ÛŒ</w>'),\n",
       " ('Ø¨Ø¨ÛŒ', 'Ù†Ø¯</w>'),\n",
       " ('Ø±Ùˆ', 'Ø´Ù†</w>'),\n",
       " ('Ú©', 'ØªØ§'),\n",
       " ('Ù…ÛŒ\\u200cØªÙˆØ§Ù†', 'Ù†Ø¯</w>'),\n",
       " ('Ø¨Ùˆ', 'Ø¯Ù†Ø¯ØŒ</w>'),\n",
       " ('ØªÙ‚', 'Ø³ÛŒÙ…</w>'),\n",
       " ('Ø³Ø±', 'Ú¯Ø±'),\n",
       " ('Ù…Ù„Ø§', 'Ù‚Ø§Øª</w>'),\n",
       " ('Ù„', 'Ù'),\n",
       " ('Ù‡ÛŒ', 'Ú†</w>'),\n",
       " ('Ø¨Ø¨ÛŒ', 'Ù†'),\n",
       " ('Ø§Ø³Øª', '.\"</w>'),\n",
       " ('Ø¯ÙˆØ³Øª', '\\u200cÙ‡Ø§</w>'),\n",
       " ('Ù…Ø§Ø¯Ø±', 'Ø´Ø§Ù†</w>'),\n",
       " ('Ù…Ø´', 'ØºÙˆÙ„</w>'),\n",
       " ('Ø¯Ù‡', 'Ù†Ø¯.</w>'),\n",
       " ('Ø¯Ùˆ', 'Ú†Ø±'),\n",
       " ('Ù…Ø§Ø´ÛŒ', 'Ù†'),\n",
       " ('Ø¨ÛŒØ§ÛŒÛŒ', 'Ø¯</w>'),\n",
       " ('Ø¶', 'ÛŒ</w>'),\n",
       " ('Ù‚', 'Ø¯ÛŒ'),\n",
       " ('Ú©ÛŒ', 'Ø³Ù‡</w>'),\n",
       " ('Ú©', 'Ø¨Ùˆ'),\n",
       " ('Ú¯Ø±', 'Ø³Ù†Ù‡</w>'),\n",
       " ('Ø¯Ø¹', 'ÙˆØ§</w>'),\n",
       " ('Ø§Ø±Ø¯', 'Ú©</w>'),\n",
       " ('Ø¨', 'Øª'),\n",
       " ('Ú¯', 'Ø§Ù†</w>'),\n",
       " ('Ø´Ùˆ', 'Ø¯</w>'),\n",
       " ('Ù…ÛŒ\\u200cÚ¯ÙˆÛŒ', 'Ø¯.</w>'),\n",
       " ('ØªØ±', 'Ø³Ù†Ø§'),\n",
       " ('Ø¯ÛŒ', 'ØŒ</w>'),\n",
       " ('Ù‡Ø§ÛŒ', 'Ø´Ø§Ù†</w>'),\n",
       " ('Ø§', 'ÙØª'),\n",
       " ('\"', 'Ú†Ø±Ø§</w>'),\n",
       " ('Ø²Ø±', 'Ø¯</w>'),\n",
       " ('ÙÛŒ', 'Ø¯</w>'),\n",
       " ('Ø¨', 'Ù†'),\n",
       " ('ÙØ§Ø±Ø³ÛŒ', ':</w>'),\n",
       " ('Ú†ÛŒ', 'Ø³Øª.</w>'),\n",
       " ('Ù¾Ø±ÛŒ', 'Ø¯.</w>'),\n",
       " ('Ø³', 'Ù¾Ø³</w>'),\n",
       " ('Ø¨Ú†', 'Ù‡\\u200cØ§ÛŒ</w>'),\n",
       " ('\"', 'Ù†Ú¯Ø§Ù‡</w>'),\n",
       " ('ØµØ¯Ø§ÛŒ', 'ÛŒ</w>'),\n",
       " ('Ø¯Ø§', 'Ø¯Ù‡</w>'),\n",
       " ('Ø®', 'Ù…</w>'),\n",
       " ('Ø±Ø³ÛŒ', 'Ø¯ØŒ</w>'),\n",
       " ('ÙØ±ÙˆØ´', 'Ú¯Ø§Ù‡</w>'),\n",
       " ('Øª', 'Ù†Ø¯</w>'),\n",
       " ('Ù…ÛŒ', 'Ù…ÛŒ</w>'),\n",
       " ('Ø¬Ø§ÛŒ', 'ÛŒ</w>'),\n",
       " ('Ù†Ù‡Ø§ÛŒ', 'ØªØŒ</w>'),\n",
       " ('Ø¨Ø±Ú¯Ø´Øª', '.</w>'),\n",
       " ('Ù‚Ùˆ', 'Ù„</w>'),\n",
       " ('Ø¬', 'Ø³Øª'),\n",
       " ('Ù…Ø§Ø¬Ø±Ø§', 'Ø¬ÙˆÛŒ'),\n",
       " ('Ù¾Ø±', 'ØªØ§Ø¨</w>'),\n",
       " ('Ø³Ø±', 'Ø¯</w>'),\n",
       " ('Ù¾Ø±', 'Ù†Ø¯'),\n",
       " ('Ø¨Ø±Ø§', 'Ù‚</w>'),\n",
       " ('Ø®Ø±Ø§', 'Ø¨</w>'),\n",
       " ('Ø³Ø±', 'Ø¹Øª</w>'),\n",
       " ('\"Ø³', 'Ù„Ø§Ù…ØŒ</w>'),\n",
       " ('Ø¬', 'Øº'),\n",
       " ('Ú†ÛŒØ²', 'Ù‡Ø§ÛŒÛŒ</w>'),\n",
       " ('Ø¯Ø§Ø±', 'Ù….</w>'),\n",
       " ('Ù‚', 'Ø¯Ø±'),\n",
       " ('Ø¯ÛŒ', 'Ø¯Ù‡</w>'),\n",
       " ('Ú©ÙˆÚ†Ùˆ', 'Ù„Ùˆ'),\n",
       " ('Ø¢ÙˆØ±', 'Ø¯</w>'),\n",
       " ('Ø¦', 'Ù†</w>'),\n",
       " ('Ø¯Ø§Ø´Øª', 'Ù†</w>'),\n",
       " ('ØºØ°', 'Ø§ÛŒ</w>'),\n",
       " ('Ù„ÛŒ', 'Ù„Ø§</w>'),\n",
       " ('Ø§Ø³ØªØ±Ø§Ø­', 'Øª</w>'),\n",
       " ('Ø¨Ø§Ù‡Ùˆ', 'Ø´</w>'),\n",
       " ('Ø§', 'Ø¹'),\n",
       " ('Ø¬', 'Ù†'),\n",
       " ('Øª', 'Ø´'),\n",
       " ('Ù…ÛŒ\\u200c', 'Ø±Ùˆ'),\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I had used 7500 -> 49 tokens \n",
    "# Now i increased it to 10000\n",
    "tokenizer_bpe = BPETokenizer(num_merges=10000)\n",
    "\n",
    "train_words = []\n",
    "for text in df_train[\"Persian\"]:\n",
    "    train_words.extend(text.split())\n",
    "\n",
    "tokenizer_bpe.train(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ú©ØªØ§', 'Ø¨\\u200c', 'Ù‡Ø§', 'ÛŒ', 'Ù…Ø§', 'Ù†</w>', 'Ø²ÛŒØ¨Ø§', 'Ø³Øª', '</w>']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø²ÛŒØ¨Ø§Ø³Øª\"\n",
    "tokens = tokenizer_bpe.tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø±ÙˆØ² ÛŒ</w> ÛŒÚ© </w> Ù…Ø±Ø¯ </w> Ø«Ø±Ùˆ ØªÙ… Ù†Ø¯ ØŒ</w> Ù¾Ø³ Ø±</w> Ø¨Ú† Ù‡</w> Ú©ÙˆÚ†Ú© Ø´</w> Ø±Ø§ </w> Ø¨ Ù€ Ù‡</w> Ø¯Ù‡ </w> Ø¨Ø±Ø¯</w> ØªØ§ </w> Ø¨ Ù€ Ù‡</w> Ø§Ùˆ </w> Ù†Ø´ Ø§Ù† </w> Ø¯Ù‡Ø¯</w> Ù…Ø±Ø¯ Ù…ÛŒ </w> Ú©Ù‡ </w> Ø¯Ø± </w> Ø¢Ù† Ø¬Ø§ </w> Ø²Ù† Ø¯ Ú¯ÛŒ </w> Ù…ÛŒâ€ŒÚ©Ù† Ù†Ø¯ ØŒ</w> Ú† Ù‚Ø¯ Ø±</w> ÙÙ‚ ÛŒØ±</w> Ù‡Ø³Øª Ù†Ø¯ .</w>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "tokens = tokenizer_bpe.tokenize(sentence)\n",
    "\n",
    "print(*tokens, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Wordpiece Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø±Ø¨Ø§Ø±Ù‡ Wordpiece Tokenizer ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ø¯Ù‚ÛŒÙ‚ Ø´Ø±Ø­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø³Ù¾Ø³ Ø¢Ù† Ø±Ø§ Ø¨Ø§ BPE Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ú©Ù†ÙˆÙ† ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Wordpiece Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø§Ù†Ø¹ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I wrapped this WordPieceTokenizer into a class so it will be better structured than the function based on like the BPE\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class WordPieceTokenizer:\n",
    "\n",
    "#     def __init__(self, num_merges=2000, vocab_size=30000):\n",
    "#         self.num_merges = num_merges\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.merges = []\n",
    "#         self.vocab = {}\n",
    "#         self.rank = {}\n",
    "\n",
    "#     def normalize_farsi(self, text: str) -> str:\n",
    "#         text = text.replace(\"ÙŠ\", \"ÛŒ\").replace(\"Ùƒ\", \"Ú©\")\n",
    "#         text = text.replace(\"Ù‡\", \"Ù‡\")\n",
    "#         text = text.replace(\"\\u200d\", \" \")\n",
    "#         text = text.translate(str.maketrans('0123456789', 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹'))\n",
    "#         text = text.replace(\"ØŒ\", \",\")  \n",
    "#         text = text.replace(\"Ø›\", \";\") \n",
    "#         text = text.replace(\"ØŸ\", \"?\")\n",
    "#         text = text.replace(\"â€˜\", \"â€™\")\n",
    "#         text = text.replace(\"â€œ\", \"â€\")\n",
    "#         text = text.replace(\"â€¦\", \"...\")\n",
    "#         text = text.replace(\"Ù„Ø§\", \"Ù„Ø§\")\n",
    "#         text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#         return text\n",
    "\n",
    "#     # Generate vocabulary from corpus (character-level tokenization)\n",
    "#     def get_vocab(self, corpus):\n",
    "#         vocab = defaultdict(int)\n",
    "#         for line in corpus:\n",
    "#             line = self.normalize_farsi(line.strip())\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             # Split into words within each line (so merges can cross word boundaries)\n",
    "#             for word in line.split():\n",
    "#                 vocab[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "#         return vocab\n",
    "\n",
    "#     # Compute the frequency of pairs (pairwise count of adjacent symbols)\n",
    "#     def get_stats(self, vocab):\n",
    "#         pairs = defaultdict(int)\n",
    "#         for word, freq in vocab.items():\n",
    "#             symbols = word.split()\n",
    "#             for i in range(len(symbols) - 1):\n",
    "#                 pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "#         return pairs\n",
    "    \n",
    "#     def get_unigram_counts(self, vocab):\n",
    "#         unigrams = defaultdict(int)\n",
    "#         for word, freq in vocab.items():\n",
    "#             for tok in word.split():\n",
    "#                 unigrams[tok] += freq\n",
    "#         return unigrams\n",
    "\n",
    "#     # # Compute the likelihood score of merging two symbols (pair)\n",
    "#     # def compute_score(self, pair, vocab):\n",
    "#     #     # Counting the occurrences of the pair (Î±, Î²) and individual frequencies of Î± and Î²\n",
    "#     #     bigram = ' '.join(pair)\n",
    "#     #     count_pair = sum(freq for word, freq in vocab.items() if bigram in word)\n",
    "#     #     count_alpha = sum(freq for word, freq in vocab.items() if pair[0] in word)\n",
    "#     #     count_beta = sum(freq for word, freq in vocab.items() if pair[1] in word)\n",
    "#     #     if count_alpha * count_beta == 0:  # aviding devision by zero\n",
    "#     #         return 0\n",
    "#     #     score = count_pair / (count_alpha * count_beta)\n",
    "#     #     return score\n",
    "\n",
    "#     # Merge the most likely pair into a new subword\n",
    "#     def merge_vocab(self, pair, vocab):\n",
    "#         bigram = ' '.join(pair)\n",
    "#         replace = ''.join(pair)\n",
    "#         v_out = {}\n",
    "#         pattern = re.compile(r'(?<!\\S)' + re.escape(bigram) + r'(?!\\S)')\n",
    "#         for word, freq in vocab.items():\n",
    "#             # merge only full token pairs, not substrings\n",
    "#             new_word = pattern.sub(replace, word)\n",
    "#             v_out[new_word] = freq\n",
    "#         return v_out\n",
    "\n",
    "#     # WordPiece Learner (Merge by maximizing likelihood using the score formula)\n",
    "#     def train(self, corpus):\n",
    "#         # Normalize and ensure corpus is a list of full sentences\n",
    "#         corpus = [self.normalize_farsi(text) for text in corpus]\n",
    "#         self.vocab = self.get_vocab(corpus)\n",
    "#         self.merges = []\n",
    "\n",
    "#         for _ in tqdm(range(self.num_merges), desc=\"Training WordPiece merges\"):\n",
    "#             pairs = self.get_stats(self.vocab)\n",
    "#             if not pairs:\n",
    "#                 break\n",
    "#             unigrams = self.get_unigram_counts(self.vocab)\n",
    "\n",
    "#             def score(p):\n",
    "#                 c_ab = pairs[p]\n",
    "#                 c_a = unigrams[p[0]]\n",
    "#                 c_b = unigrams[p[1]]\n",
    "#                 return 0 if c_a == 0 or c_b == 0 else c_ab / (c_a * c_b)\n",
    "\n",
    "#             best_pair = max(pairs, key=score)\n",
    "#             self.vocab = self.merge_vocab(best_pair, self.vocab)\n",
    "#             self.merges.append(best_pair)\n",
    "\n",
    "#         self.rank = {p: i for i, p in enumerate(self.merges)}\n",
    "#         return self.merges\n",
    "\n",
    "#     # Tokenizer (apply learned merges)\n",
    "#     def encode_word(self, word):\n",
    "#         word = self.normalize_farsi(word)\n",
    "#         symbols = list(word) + [\"</w>\"]\n",
    "\n",
    "#         while True:\n",
    "#             best = None\n",
    "#             best_rank = None\n",
    "#             for i in range(len(symbols) - 1):\n",
    "#                 p = (symbols[i], symbols[i + 1])\n",
    "#                 r = self.rank.get(p)\n",
    "#                 if r is not None and (best_rank is None or r < best_rank):\n",
    "#                     best = p\n",
    "#                     best_rank = r\n",
    "#             if best is None:\n",
    "#                 break\n",
    "\n",
    "#             merged = []\n",
    "#             i = 0\n",
    "#             while i < len(symbols):\n",
    "#                 if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == best:\n",
    "#                     merged.append(''.join(best))\n",
    "#                     i += 2\n",
    "#                 else:\n",
    "#                     merged.append(symbols[i])\n",
    "#                     i += 1\n",
    "#             symbols = merged\n",
    "\n",
    "#         # drop </w> marker if still present\n",
    "#         if symbols and symbols[-1] == \"</w>\":\n",
    "#             symbols.pop()\n",
    "#         return symbols\n",
    "\n",
    "#     # Handle [UNK] token AS WELL\n",
    "#     def tokenize(self, text):\n",
    "#         text = self.normalize_farsi(text)\n",
    "#         tokens = []\n",
    "#         for w in text.split():\n",
    "#             word_tokens = self.encode_word(w)\n",
    "#             if len(word_tokens) > self.vocab_size:\n",
    "#                 tokens.append(\"[UNK]\")\n",
    "#             else:\n",
    "#                 tokens.extend(word_tokens)\n",
    "#         return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training WordPiece merges: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [1:19:00<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ú©', 'Øª', 'Ø§', 'Ø¨', '\\u200c', 'Ù‡', 'Ø§', 'ÛŒ', 'Ù…', 'Ø§', 'Ù†', 'Ø²', 'ÛŒ', 'Ø¨', 'Ø§', 'Ø³', 'Øª']\n"
     ]
    }
   ],
   "source": [
    "# # I changed the num_merges=8000 to 10000 so it will have bigger voac therefore on avg. bigger tokens and les number of toknes per sentence\n",
    "# tokenizer_wordpiece = WordPieceTokenizer(num_merges=10000)\n",
    "\n",
    "# train_words = []\n",
    "# for text in df_train[\"Persian\"]:\n",
    "#     train_words.extend(text.split())\n",
    "\n",
    "# # training\n",
    "# tokenizer_wordpiece.train(train_words)\n",
    "\n",
    "\n",
    "# example1 = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø²ÛŒØ¨Ø§Ø³Øª\"\n",
    "# print(tokenizer_wordpiece.tokenize(example1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_given = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "# print(tokenizer_wordpiece.tokenize(example_given))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notebook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.normalizers import NFKC, Replace, Sequence\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "try:\n",
    "    texts = df_train[\"Persian\"].astype(str).tolist()\n",
    "except Exception as e:\n",
    "    texts = df_train.astype(str).to_list()\n",
    "\n",
    "farsi_norm = Sequence([\n",
    "    NFKC(),\n",
    "    Replace(\"ÙŠ\", \"ÛŒ\"),\n",
    "    Replace(\"Ùƒ\", \"Ú©\"),\n",
    "    Replace(\"\\u200d\", \" \"),  #  ZWNG\n",
    "    Replace(\"ØŒ\", \",\"),\n",
    "    Replace(\"Ø›\", \";\"),\n",
    "    Replace(\"ØŸ\", \"?\"),\n",
    "    Replace(\"â€˜\", \"â€™\"),\n",
    "    Replace(\"â€œ\", \"â€\"),\n",
    "    Replace(\"â€¦\", \"...\"),\n",
    "])\n",
    "\n",
    "# build tokenizer\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = farsi_norm\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# trainer\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=8000,  \n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# training\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø±ÙˆØ²ÛŒ', 'ÛŒÚ©', 'Ù…Ø±Ø¯', 'Ø«Ø±ÙˆØªÙ…Ù†Ø¯', ',', 'Ù¾Ø³Ø±', 'Ø¨Ú†Ù‡', 'Ú©ÙˆÚ†Ú©Ø´', 'Ø±Ø§', 'Ø¨', '##Ù€', '##Ù‡', 'Ø¯Ù‡', 'Ø¨Ø±Ø¯', 'ØªØ§', 'Ø¨', '##Ù€', '##Ù‡', 'Ø§Ùˆ', 'Ù†Ø´Ø§Ù†', 'Ø¯Ù‡Ø¯', 'Ù…Ø±Ø¯Ù…ÛŒ', 'Ú©Ù‡', 'Ø¯Ø±', 'Ø¢Ù†Ø¬Ø§', 'Ø²Ù†Ø¯Ú¯ÛŒ', 'Ù…ÛŒ\\u200cÚ©Ù†Ù†Ø¯', ',', 'Ú†Ù‚Ø¯Ø±', 'ÙÙ‚ÛŒØ±', 'Ù‡Ø³ØªÙ†Ø¯', '.']\n"
     ]
    }
   ],
   "source": [
    "example_given = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "encoded = tokenizer.encode(example_given)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªØ§ Ø­Ø¯ÙˆØ¯ÛŒ Ø´Ø¨ÛŒÙ‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BPE Ø§Ø³Øª Ø§Ù„Ø¨ØªÙ‡ Ø¨Ø§ ÛŒÚ©Ø³Ø±ÛŒ ØªØºÛŒÛŒØ±Ø§Øª Ù…Ù‡Ù…ÛŒ. Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ú¯ÙˆÛŒÙ… Ú©Ù‡ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… ØªÙˆØ³Ø· google Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯Ù‡ Ùˆ Ø¯Ø± bert Ù‡Ù… Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù…ØªÙ† Ø¨Ø§Ø² Ù†ÛŒØ³Øª Ø¨Ù‡ Ù‡Ù…ÛŒÙ† ØµÙˆØ±Øª Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø§Ù„Ø¨ØªÙ‡ ØªÙØ³ÛŒØ± Ø¢Ù† Ùˆ Ú†Ú¯ÙˆÙ†Ù‡ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ø¯ Ø¢Ù† Ù…Ø´Ø®Øµ Ø§Ø³Øª ØªØ§ Ø­Ø¯ÛŒ. <br>\n",
    "Ø§Ù„Ú©ÙˆØ±ÛŒØªÙ… WordPiece Tokenizer Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯. Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ù…ÛŒ Ø¢ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ corpusÛŒÛŒ  Ú©Ù‡ Ø¯Ø§Ø±ÛŒÙ… vocab Ø®ÙˆØ¯ Ø±Ø§ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒØ¯Ù‡Ø¯ Ùˆ Ø¯Ø± Ø¢Ù† ØªÙˆÚ©Ù† Ú©Ù„Ù…Ù‡ Ú¯Ù…Ù†Ø§Ù… UNKN Ø±Ø§ Ù†ÛŒØ² Ù…ÛŒÚ¯Ø°Ø§Ø±Ø¯. Ù‡Ø¯Ù Ù†Ù‡Ø§ÛŒÛŒ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø¯Ø³Øª Ø§ÙˆØ±Ø¯Ù† vocab Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ Ø·ÙˆÙ„ Ø¢Ù† Ø«Ø§Ø¨Øª Ùˆ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø­Ø§Ù„ Ù¾Ø³ ØªØ¹ÛŒÛŒÙ† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ù†Ø®Ø³Øª Ù…Ø§ ÙˆÙ‚Øª Ø¢Ù† Ø§Ø³Øª Ù…Ø§Ù†Ù†Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BPE Ù…Ø§ merge Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ù…Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ú©Ø§Ù†Ø³ Ùˆ ØªØ¹Ø¯Ø¯ Ùˆ ØªÚ©Ø±Ø§Ø± Ø¯Ùˆ Ø­Ø±Ù Ú©Ù†Ø§Ø± Ù‡Ù… Ø§Ù…Ø¯Ù† Ù…Ø·Ù„Ù‚ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…ÛŒØ´ÙˆÙ†Ø¯. Ø¨Ù„Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ù† Ø¯ÙˆØªØ§ÛŒÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± Ù‡Ù… Ø¨Ø§ Ù‡Ù… Ø§Ø¯ØºØ§Ù… Ù…ÛŒØ´ÙˆÙ†Ø¯ Ú©Ù‡ likelihood Ø§Ù† Ú©Ù‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ train Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ø±Ø§ Ù…ÛŒØ®ÙˆØ§Ù‡ÛŒÙ… maximize Ø¨Ú©Ù†ÛŒÙ….<br>\n",
    "Ø±Ø§Ø¨Ø·Ù‡ Ø§ÛŒÙ† Ù‡Ù… Ø¨Ù‡ ØµÙˆØ±Øª ØªØ¹Ø¯Ø§Ø¯ Ú©Ù†Ø§Ø± Ù‡Ù… Ø¢Ù…Ø¯Ù† Ø§Ù† Ø¯Ùˆ Ø­Ø±Ù ÙˆÙˆÚ©Ø¨ Ø¨Ø§Ù‡Ù… Ø¯Ø± Ù…ØªÙ† Ø¯Ø§Ø¯Ù‡ Ø§Ù…ÙˆØ²Ø´ÛŒ ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø¶Ø±Ø¨ ØªØ¹Ø¯Ø§Ø¯ Ø¢Ù…Ø¯Ù† Ù‡Ø±Ú©Ø¯Ø§Ù….<br>\n",
    "Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø´Ú©Ù„ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ø±Ø§ Ú¯Ø³ØªØ±Ø´ Ù…ÛŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø¯Ø± Ø¨ÛŒØ§ÛŒØ¯ Ù„ØºØª Ù†Ø§Ù…Ù‡ Ù…Ø§ Ùˆ ØªØ±ØªÛŒØ¨ merge Ú©Ø±Ø¯Ù† Ù‡Ø§.\n",
    "<br>\n",
    "<br>\n",
    "Ø§Ú©Ø«Ø± Ø´Ø¨Ø§Ù‡Øª Ù‡Ø§ Ùˆ ØªÙØ§ÙˆØª Ù‡Ø§ÛŒ wordpiece tokenizer Ø±Ø§ Ø¨Ø§ BPE tokenizer Ø¨ÛŒØ§Ù† Ú©Ø±Ø¯Ù…. Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¯ÛŒÚ¯Ø± Ù‡Ù… Ù…Ø«Ø§Ù„ Ø¨Ø²Ù†Ù… Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ú©Ù„ ÙÙ„Ø³ÙÙ‡ BPE Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø§Ú©Ø³ÛŒÙ…Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø¬ÙØª Ú©Ø§Ø±Ø§Ú©ØªØ± Ù‡Ø§ÛŒ Ú©Ù†Ø§Ø± Ù‡Ù… Ø§Ø³Øª Ù…Ø§ wordpiece Ø¯Ø±ÙˆØ§Ù‚Ø¹ Ø¨Ù‡ Ù‡Ø¯Ù Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø¯Ù† maximum lielihood Ø¯ÛŒØ¯Ù† Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡ Ø§Ù…ÙˆØ²Ø´ÛŒ Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯ ØªØ§ subowrd Ù‡Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙˆÚ©Ù† Ø¨ØªÙˆØ§Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ù‡Ø±Ø¯Ùˆ Ù‡Ù… greedy Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒÚ©Ù†Ù†Ø¯ Ø§Ù…Ø§ Ø¨Ø§ ÙÙ„Ø³ÙÙ‡ Ù‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØªÛŒ.<br>\n",
    "ÛŒÚ© ØªÙØ§ÙˆØª Ø¯ÛŒÚ¯Ø± Ø¢Ù† Ù‡Ø§ Ø¨Ø­Ø« Ø³Ø±Ø¹Øª Ùˆ Ø³Ø§Ø¯Ú¯ÛŒ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ Ø§ÛŒÙ† BPE tokenizer Ø§Ø³Øª Ø¯Ø±Ø­Ø§Ù„ÛŒÚ©Ù‡ wordpiece Ú©Ù†Ø¯ØªØ± Ùˆ Ø¯Ø§Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª.<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¢Ù† Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ù†Ø¯ÛŒØ¯Ù†Ø¯ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯(OOV, Out-of-Vocabulary)\n",
    "Ú©Ù‡ Ø¯Ø±Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BPE Ù…Ø§ Ø³Ø¹ÛŒ Ø¨Ø± Ø´Ú©Ø§Ù†Ø¯Ù† Ø¢Ù† Ú©Ù„Ù…Ù‡ Ø¯Ø§Ø±ÛŒÙ… Ø§Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ wordpeice Ù…Ø§ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ú†Ø§Ø±Ù‡ Ø§Ø² ØªÙˆÚ©Ù† Ø§Ø² Ù¾ÛŒØ´ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ UNKN Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ….<br>\n",
    "BPE Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø³Ø±Ø¹Øª Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ù…Ø§Ù†Ù†Ø¯ neural machine translation (NMT) <br>\n",
    "Ùˆ wordpiece tokenizer Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ù‚Øª Ø­Ø§Ø¹Ø¸ Ø§Ù‡Ù…ÛŒØª Ø§Ø³Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ù…Ø§Ù†Ù†Ø¯ large-scale pre-trained models (BERT, ALBERT, RoBERTa).\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Tokenization Visualization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± \n",
    "<a href=\"https://tiktokenizer.vercel.app/\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    tiktokenizer\n",
    "</a>\n",
    "ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Ù‡Ø± ÛŒÚ© Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ gpt2 Ùˆ gpt4 Ùˆ Meta-Llama-3-8B Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ùˆ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ Ø±Ø§ Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± ÛŒÚ© ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†ÛŒØ¯. Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¹Ù„Øª ØªÙØ§ÙˆØª Ù†ØªÛŒØ¬Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ú†ÛŒØ³ØªØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "gpt4.o results: 42 tokens - Ø¨Ø§ÛŒØ¯ Ø¯Ù‚Øª Ú©Ø±Ø¯ Ú†ÙˆÙ† Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‚ÙˆÛŒ ØªØ± Ù¾ÛŒÚ†ÛŒØ¯Ù‡ ØªØ± Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯ÛŒØªØ§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø±Ø§ Ø¯ÛŒØ¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª subword Ù…ÛŒØªÙˆØ§Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†Ù‡<br>\n",
    "gpt2 results: 139 tokens - ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø­Ø±Ù Ø¨Ù‡ Ø­Ø±Ù ÛŒØ§ Ù‡Ù…Ø§Ù† character Ø§Ù…Ø¯Ù‡ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ ÙˆØ§Ú˜Ú©.<br>\n",
    "Meta-Llama-3-8B results: 40 tokens - ØªÙ‚Ø±ÛŒØ¨Ø§ Ù…Ø§Ù†Ù†Ø¯ gpt4.0 ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø¨Ù‡ ÙˆØ§Ø²Ú¯ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†Ø¯ Ø¬Ù…Ù„Ù‡ Ø±Ø§.<br>\n",
    "<br>\n",
    "Ø¨Ù†Ø¯Ù‡ Ù¾Ø³ Ø§Ø² ØªØ­Ù‚ÛŒÙ‚ Ø±Ø§Ø¬Ø¨ 3 Ù…Ø¯Ù„ Ù†Ø§Ù… Ø¨Ø±Ø¯Ù‡ Ø´Ø¯Ù‡ Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯Ù… Ú©Ù‡ gpt2 Ú©Ù‡ ØªÙ‚Ø±ÛŒØ¨Ø§ Ø§ØµÙ„Ø§ Ø¨Ø±ÙˆÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø®Ø§Ø·Ø± Ú†ÙˆÙ† Ø²Ø¨Ø§Ù† Ø±Ø§ Ù†Ù…ÛŒØ´Ù†Ø§Ø³Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±Ù Ø¨Ù‡ Ø­Ø±Ù ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø´ÛŒÙ† Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ø¯.<br>\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ gpt4 Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ø¯Ù„ lamma3 Ø¨Ø§ 8B Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø§ÛŒÙ† Ø¯Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª multilingual train Ø´Ø¯Ù‡ Ø§Ù†Ø¯. Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ú†ÙˆÙ† Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ø§Ù…ÙˆØ²Ø´ Ø®ÙˆØ¯ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ù‡Ù… Ø¯ÛŒØ¯Ù‡ Ø§Ù†Ø¯ Ù…ÛŒØªÙˆØ§Ù†Ù†Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ subword Ú©Ù‡ Ø¯Ø± vocab Ø®ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ø¨Ø´Ú©Ø§Ù†Ù†Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ùˆ Ø¨Ù‡ØªØ± Ø§Ø² Ù…Ø¯Ù„ gpt2 Ø¹Ù…Ù„ Ø¨Ú©Ù†Ù†Ø¯.<br>\n",
    "Ø¯Ø± Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ø¨Ù‡ØªØ± Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†ÛŒØ² ØªÙØ§ÙˆØª Ù‡Ø§ÛŒÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ù‡ Ù…Ø¯Ù„ gpt4 Ø¨Ù‡ ØµÙˆØ±Øª Ø±ÙˆØ´ BPE Tokenizer Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯.<br>\n",
    "Ø§Ù…Ø§ Ù…Ø¯Ù„ Meta-Llama-3-8B Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Wordpiece Tokenizer Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†Ø¯.<br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø³ÙˆÙ… - <span dir=\"ltr\">N-gram Language Modeling</span> (55)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø§ N-gram Language Modeling Ùˆ Ø¢Ù† Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²ÛŒØ¯. Ø³Ù¾Ø³ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± perplexity Ùˆ Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ smoothing Ùˆ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Data cleaning & Tokenization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "- Ú©Ù‡ Ø¯Ø± Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ù†ÛŒØ² Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ - Ø±Ø§ Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ (train) Ø¢Ù† Ø±Ø§ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¢Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. (Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¨Ù‡ Ú†Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†Ø¯.)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² BPE Tokenizer Ú©Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯Ø§Ø¯Ù‡â€ŒØ§ÛŒØ¯ØŒ Ø¯Ø§Ø¯Ú¯Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØŒ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b>\n",
    "<br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙ…ÛŒØ²Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "<br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "<br>\n",
    "- ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "# import hazm   we could have used hazm as well\n",
    "\n",
    "# dataset_path_in_huggingface = \"taesiri/TinyStories-Farsi\"\n",
    "# ds = load_dataset(dataset_path_in_huggingface)\n",
    "# df_train = dataset[\"train\"].to_pandas()\n",
    "# df_val = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "file_path_train = os.path.join(\"data\", \"train.parquet\")\n",
    "file_path_val = os.path.join(\"data\", \"validation.parquet\")\n",
    "\n",
    "df_train = pd.read_parquet(file_path_train)\n",
    "df_val = pd.read_parquet(file_path_val)\n",
    "\n",
    "df_train = df_train[\"Persian\"]\n",
    "df_val = df_val[\"Persian\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ÛŒÚ©â€ŒØ±ÙˆØ² ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ø§Ø³Ù… Ø¨Ù† Ø¨ÙˆØ¯. Ø¨Ù† Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¯Ù†ÛŒØ§ÛŒ Ø§Ø·Ø±Ø§ÙØ´ Ø±Ø§ Ú©Ø´Ù Ú©Ù†Ø¯. Ø§Ùˆ Ú†ÛŒØ²Ù‡Ø§ÛŒ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø²ÛŒØ§Ø¯ÛŒ Ø¯ÛŒØ¯ØŒ Ù…Ø«Ù„ ÙˆØ§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± ÛŒÚ© Ù…ØºØ§Ø²Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯. ÛŒÚ© Ø±ÙˆØ² Ù‡Ù†Ú¯Ø§Ù…ÛŒâ€ŒÚ©Ù‡ Ø¨Ù† Ø§Ø² Ù…ØºØ§Ø²Ù‡ Ø±Ø¯ Ù…ÛŒâ€ŒØ´Ø¯ ÛŒÚ© ÙˆØ§Ø²Ù‡ Ø¨Ø³ÛŒØ§Ø± ÙˆÛŒÚ˜Ù‡ Ø±Ø§ Ø¯ÛŒØ¯. ÙˆÙ‚ØªÛŒ Ø¨Ù† Ø¢Ù† Ø±Ø§ Ø¯ÛŒØ¯ Ù…Ø¨Ù‡ÙˆØª Ø´Ø¯! \n",
      "Ø§Ùˆ Ú¯ÙØª:Â«ÙˆØ§ÙˆØŒ Ø§ÛŒÙ† ÙˆØ§Ù‚Ø¹Ø§ ÛŒÚ© ÙˆØ§Ø²Ù‡ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø§Ø³Øª! Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¢Ù† Ø±Ø§ Ø¨Ø®Ø±Ù…ØŸÂ»\n",
      "ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯ Ùˆ Ú¯ÙØª: Â«Ø¨Ù„Ù‡ØŒ Ø§Ù„Ø¨ØªÙ‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø¨Ø±ÛŒ Ùˆ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Øª Ù†Ø´Ø§Ù† Ø¯Ù‡ÛŒ Ú©Ù‡ Ú†Ù‚Ø¯Ø± Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø§Ø³Øª!Â»\n",
      "Ù¾Ø³ Ø¨Ù† ÙˆØ§Ø²Ù‡ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ø¯ Ùˆ Ø§Ø² Ø¢Ù† Ø¨Ø³ÛŒØ§Ø± Ù…ÙØªØ®Ø± Ø¨ÙˆØ¯! Ø§Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ø±Ø§ ØµØ¯Ø§ Ú©Ø±Ø¯ Ùˆ ÙˆØ§Ø²Ù‡ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø±Ø§ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯. Ù‡Ù…Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Ø´ ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ ÙˆØ§Ø²Ù‡ Ø²ÛŒØ¨Ø§Ø³Øª Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø¨Ø§ÙˆØ± Ú©Ù†Ù†Ø¯ Ú©Ù‡ Ø¨Ù† Ú†Ù‚Ø¯Ø± Ø®ÙˆØ´â€ŒØ´Ø§Ù†Ø³ Ø§Ø³Øª.\n",
      "Ùˆ Ø§ÛŒÙ†â€ŒÚ¯ÙˆÙ†Ù‡ Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ù† ÛŒÚ© ÙˆØ§Ø²Ù‡ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø±Ø§ Ø¯Ø± Ù…ØºØ§Ø²Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯!  ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ ÛŒÚ© Ø³Ù…ÙˆØ± Ø¢Ø¨ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª. Ø§Ùˆ Ø¯Ø± ÛŒÚ© Ø±ÙˆØ¯Ø®Ø§Ù†Ù‡ Ø¨Ø§ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ø§Ø´ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ù‡Ù…Ù‡ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ Ø´Ù†Ø§ Ú©Ù†Ù†Ø¯.  \n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ù…Ø§Ø¯Ø± Ø§Ù„ÛŒ Ú¯ÙØª: Â«Ø§Ù„ÛŒØŒ Ø²ÙˆØ¯ Ø¨Ø§Ø´ Ùˆ Ø¨Ø±Ø§ÛŒ Ø´Ø§Ù… Ù…Ø§Ù‡ÛŒ Ø¨Ú¯ÛŒØ±!Â» Ø§Ù„ÛŒ Ø³Ø±ÛŒØ¹ Ø´Ù†Ø§ Ú©Ø±Ø¯ ØªØ§ Ù…Ø§Ù‡ÛŒ Ø¨Ú¯ÛŒØ±Ø¯. Ø§Ùˆ Ø¯ÙˆØ³ØªØ´ Ø§Ø±Ø¯Ú© Ø±Ø§ Ø¯ÛŒØ¯. Ø§Ø±Ø¯Ú© Ú¯ÙØª: Â«Ø³Ù„Ø§Ù… Ø§Ù„ÛŒ!Â» Ø§Ù„ÛŒ Ú¯ÙØª: Â«Ø³Ù„Ø§Ù… Ø§Ø±Ø¯Ú©!Â» Ù…Ù† Ø¨Ø§ÛŒØ¯ Ø¹Ø¬Ù„Ù‡ Ú©Ù†Ù… Ùˆ Ø¨Ø±Ø§ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒØ§Ù… Ù…Ø§Ù‡ÛŒ Ø¨Ú¯ÛŒØ±Ù….Â»  \n",
      "Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø§Ù„ÛŒ Ù…Ø´ØºÙˆÙ„ Ú¯Ø±ÙØªÙ† Ù…Ø§Ù‡ÛŒ Ø¨ÙˆØ¯ØŒ ÛŒÚ© Ø³Ù†Ú¯ Ø¨Ø²Ø±Ú¯ Ùˆ Ø¯Ø±Ø®Ø´Ø§Ù† Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯. Ø§Ùˆ ÙÚ©Ø± Ú©Ø±Ø¯: Â«Ø§ÛŒÙ† Ù…Ø§Ù‡ÛŒ Ù†ÛŒØ³ØªØŒ Ø§Ù…Ø§ Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ¨Ø§Ø³Øª!Â» Ø§Ù„ÛŒ Ø³Ù†Ú¯ Ø¯Ø±Ø®Ø´Ø§Ù† Ø±Ø§ Ø¨Ø§ Ø®ÙˆØ¯ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒØ§Ø´ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯. Ù‡Ù…Ù‡ Ø¨Ù‡ Ø³Ù†Ú¯ Ø¯Ø±Ø®Ø´Ø§Ù† Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯Ù†Ø¯. Ø³Ù†Ú¯ Ø¯Ø±Ø®Ø´Ø§Ù† Ù‡Ù…Ù‡ Ø±Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ Ú©Ø±Ø¯ØŒ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø§Ù‡ÛŒ Ø´Ø§Ù… Ø±Ø§ ÙØ±Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ² Ù¾Ø³Ø± Ú©ÙˆÚ†Ú©ÛŒ Ø¨Ù‡ Ø§Ø³Ù… ØªÛŒÙ… Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØª. Ø§Ùˆ ÛŒÚ© Ø¨Ø¨Ø± Ø¨Ø²Ø±Ú¯ Ø¯ÛŒØ¯. Ø¨Ø¨Ø± Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨ÙˆØ¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø­Øª Ù…ÛŒâ€ŒØ´Ø¯ Ø¨Ø§Ù‡Ø§Ø´ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯. ØªÛŒÙ… Ùˆ Ø¨Ø¨Ø± ØªÙ…Ø§Ù… Ø±ÙˆØ² Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯. Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´ Ú¯Ø°Ø±ÙˆÙ†Ø¯Ù†Ø¯.\n",
      "Ø¨Ø¹Ø¯ Ø§ØªÙØ§Ù‚ ØºÛŒØ± Ù…Ù†ØªØ¸Ø±Ù‡â€ŒØ§ÛŒ Ø§ÙØªØ§Ø¯. Ø¨Ø¨Ø± Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ù„Ø±Ø²ÛŒØ¯Ù† Ú©Ø±Ø¯. ØªÛŒÙ… ØªØ±Ø³ÛŒØ¯Ù‡ Ø¨ÙˆØ¯. Ù†Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ø³Øª Ú†Ù‡ Ø§ØªÙØ§Ù‚ÛŒ Ø¯Ø§Ø±Ù‡ Ù…ÛŒÙØªÙ‡. Ø§Ù…Ø§ Ø¨Ø¹Ø¯ Ø¨Ø¨Ø± ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÛŒÚ© Ø³Ú¯ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø´Ø¯. ØªÛŒÙ… Ø®ÛŒÙ„ÛŒ ØªØ¹Ø¬Ø¨ Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯.  \n",
      "ØªÛŒÙ… Ùˆ Ø³Ú¯ Ø§Ù„Ø§Ù† Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯. Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯. Ø³Ú¯ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø±Ø§Ø­Øª Ø¨ÙˆØ¯ Ø¨Ø§Ù‡Ø§Ø´ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯. Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø±ÙˆØ² ØªÛŒÙ… Ø¨Ø§ Ø¯ÙˆØ³Øª Ø¬Ø¯ÛŒØ¯Ø´ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ú¯Ø´Øª.  ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨Ù‡ Ù†Ø§Ù… Ø¨Ø§Ø¨ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. Ø¨Ø§Ø¨ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ú¯Ù„ Ø¨Ú†ÛŒÙ†Ø¯ Ùˆ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù¾Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ú¯Ø±Ø¯Ø¯. ÛŒÚ© Ø±ÙˆØ² ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØª Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ø¨ÛŒØ±ÙˆÙ† Ø¨Ø±ÙˆØ¯ ØªØ§ Ú¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ú†ÛŒÙ†Ø¯.\n",
      "Ø§Ùˆ Ù†Ø§Ú¯Ù‡Ø§Ù† Ú†ÛŒØ² Ø¹Ø¬ÛŒØ¨ÛŒ Ø±Ø§ Ø±ÙˆÛŒ Ø²Ù…ÛŒÙ† Ø¯ÛŒØ¯. ÛŒÚ© Ø§Ù†Ú¯Ø´Øª Ø³Ø¨Ø² Ø¨Ø²Ø±Ú¯! Ø¢Ù†Ù‚Ø¯Ø± Ø¨Ø²Ø±Ú¯ Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ø§Ø¨ ØªØ§ Ø¨Ù‡ Ø­Ø§Ù„ Ú†Ù†ÛŒÙ† Ú†ÛŒØ²ÛŒ Ù†Ø¯ÛŒØ¯Ù‡ Ø¨ÙˆØ¯. Ø¨Ø§Ø¨ Ø¨Ø§ Ú©Ù†Ø¬Ú©Ø§ÙˆÛŒ Ø®Ù… Ø´Ø¯ ØªØ§ Ø¨Ù‡ØªØ± Ø¨Ø¨ÛŒÙ†Ø¯. Ø¨Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ú¯ÙØª: \"Ø¨Ø¨ÛŒÙ†ÛŒØ¯ØŒ Ù…Ù† Ø§ÛŒÙ† Ø§Ù†Ú¯Ø´Øª Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù…! Ø¨Ø§ Ø¢Ù† Ú†Ù‡ Ú©Ø§Ø± Ú©Ù†ÛŒÙ…ØŸ\"\n",
      "Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ø®ÛŒÙ„ÛŒ Ø°ÙˆÙ‚â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù†Ø¯. Ø¨Ù‡ Ø§Ùˆ Ú¯ÙØªÙ†Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ø§ Ø®ÙˆØ¯Ø´ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒØ§Ø´ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯. Ù¾Ø³ Ø¨Ø§Ø¨ Ø¨Ø§ Ø§Ø­ØªÛŒØ§Ø· Ø¢Ù† Ø§Ù†Ú¯Ø´Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø´Øª Ùˆ Ø¨Ø§ Ø®ÙˆØ¯Ø´ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ø¯. ÙˆÙ‚ØªÛŒ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø±Ø³ÛŒØ¯ØŒ Ø¨Ø§Ø¨ Ø¨Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ Ø§Ù†Ú¯Ø´Øª Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒØ§Ø´ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯. Ù¾Ø¯Ø±Ø´ Ø´Ú¯ÙØªâ€ŒØ²Ø¯Ù‡ Ø´Ø¯ Ùˆ Ø¨Ø§Ø¨ Ø±Ø§ Ø¨ØºÙ„ Ú©Ø±Ø¯ ØªØ§ Ù‚Ø¯Ø±Ø¯Ø§Ù†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯.  \n",
      "Ø§Ø² Ø¢Ù† Ø±ÙˆØ² Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø¨Ø§Ø¨ Ù‡Ù…ÛŒØ´Ù‡ Ø¢Ù† Ø§Ù†Ú¯Ø´Øª Ø¨Ø²Ø±Ú¯ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø±Ø§ Ø¨Ø§ Ø®ÙˆØ¯Ø´ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø´Øª ØªØ§ ÛŒØ§Ø¯Ø¢ÙˆØ± Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ú†ÛŒØ²Ù‡Ø§ÛŒ ÙˆÛŒÚ˜Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¯Ø± Ù‡Ø± Ø¬Ø§ÛŒÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯.  ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ Ø¯Ø± ÛŒÚ© Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú©ØŒ ÛŒÚ© Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÙˆØ³ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. Ù„ÙˆØ³ÛŒ Ø±Ù†Ú¯ Ù†Ø§Ø±Ù†Ø¬ÛŒ Ø±Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª. Ø§Ùˆ ÛŒÚ© Ù„Ø¨Ø§Ø³ Ù†Ø§Ø±Ù†Ø¬ÛŒØŒ ØªÙˆÙ¾ Ù†Ø§Ø±Ù†Ø¬ÛŒ Ùˆ Ø­ØªÛŒ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ù†Ø§Ø±Ù†Ø¬ÛŒ Ø¯Ø§Ø´Øª. ÛŒÚ© Ø±ÙˆØ²ØŒ Ù„ÙˆØ³ÛŒ Ø¨Ø§ Ø¯ÙˆØ³Øª Ø¬Ø¯ÛŒØ¯ÛŒ Ø¢Ø´Ù†Ø§ Ø´Ø¯. Ø§ÛŒÙ† Ø¯ÙˆØ³Øª Ù…Ø«Ù„ Ø¯ÙˆØ³ØªØ§Ù† Ø¯ÛŒÚ¯Ø± Ù†Ø¨ÙˆØ¯. Ø§Ùˆ ÛŒÚ© Ø±ÙˆØ­ Ø¨ÙˆØ¯. Ø±ÙˆØ­ Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨ÙˆØ¯ Ùˆ Ø§Ø² Ø¨Ø§Ø²ÛŒ Ø¨Ø§ Ù„ÙˆØ³ÛŒ Ù„Ø°Øª Ù…ÛŒâ€ŒØ¨Ø±Ø¯.  \n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ù„ÙˆØ³ÛŒ Ùˆ Ø±ÙˆØ­ Ø¨Ø§ ØªÙˆÙ¾ Ù†Ø§Ø±Ù†Ø¬ÛŒ Ø§Ø´ Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ Ø³Ø±Ú¯Ø±Ù… Ø¨ÙˆØ¯Ù†Ø¯. Ø¨Ø¹Ø¯ØŒ Ù…Ø§Ø¯Ø± Ù„ÙˆØ³ÛŒ Ø§Ùˆ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø´Ø§Ù… ØµØ¯Ø§ Ø²Ø¯. Ù„ÙˆØ³ÛŒ Ø¨Ù‡ Ø±ÙˆØ­ Ú¯ÙØª: \"Ù…Ù† Ø¨Ø§ÛŒØ¯ Ø§Ù„Ø§Ù† Ø¨Ø±Ù… Ø´Ø§Ù… Ø¨Ø®ÙˆØ±Ù…. Ø¨Ø¹Ø¯Ø§ Ø¨Ø§ Ù…Ù† Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŸ\" Ø±ÙˆØ­ Ø³Ø± ØªÚ©Ø§Ù† Ø¯Ø§Ø¯ Ùˆ Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯.  \n",
      "Ø¯Ø± Ø·ÙˆÙ„ Ø´Ø§Ù…ØŒ Ù„ÙˆØ³ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø§Ø¯Ø±Ø´ Ø§Ø² Ø±ÙˆØ­ Ú¯ÙØª. Ø§Ù…Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ù‡ Ø§Ùˆ Ø§Ø¹ØªÙ‚Ø§Ø¯ Ù†Ø¯Ø§Ø´Øª. Ø§Ùˆ Ú¯ÙØª: \"Ø±ÙˆØ­ ÙˆØ§Ù‚Ø¹ÛŒ Ù†ÛŒØ³ØªØŒ Ù„ÙˆØ³ÛŒ. ØªÙˆ ØªØ®ÛŒÙ„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø±ÛŒ.\" Ù„ÙˆØ³ÛŒ Ù†Ø§Ø±Ø§Ø­Øª Ø´Ø¯ Ú©Ù‡ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ù‡ Ø§Ùˆ Ø§Ø¹ØªÙ‚Ø§Ø¯ Ù†Ø¯Ø§Ø±Ø¯. Ø¨Ø¹Ø¯ Ø§Ø² Ø´Ø§Ù…ØŒ Ø§Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø±Ú¯Ø´Øª ØªØ§ Ø¨Ø§ Ø±ÙˆØ­ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§ ØªÙˆÙ¾ Ù†Ø§Ø±Ù†Ø¬ÛŒ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù†Ø¯. Ù„ÙˆØ³ÛŒ Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø³Øª Ú©Ù‡ Ø­ØªÛŒ Ø§Ú¯Ø± Ø¯ÛŒÚ¯Ø±Ø§Ù† Ø¯ÙˆØ³ØªØ´ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±Ù†Ø¯ØŒ Ø±ÙˆØ­ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ØŒ Ù¾Ø³Ø±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØª ØªØ§ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯. Ø§Ùˆ Ø¯ÙˆØ³ØªØ´ Ø³Ø§Ù… Ø±Ø§ Ø¯ÛŒØ¯ Ú©Ù‡ Ø¨Ø§ Ù…Ø§Ø´ÛŒÙ†â€ŒØ¨Ø§Ø²ÛŒ Ù…Ø´ØºÙˆÙ„ Ø¨Ø§Ø²ÛŒ Ø¨ÙˆØ¯. ØªÛŒÙ… Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ø¨Ù¾ÛŒÙˆÙ†Ø¯Ø¯ Ùˆ Ø¨Ø§ Ø³Ø§Ù… Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ù‡Ø± Ø¯Ùˆ Ø¨Ø§ Ù…Ø§Ø´ÛŒÙ†â€ŒØ¨Ø§Ø²ÛŒ Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ Ùˆ Ø³Ø±ÛŒØ¹ Ù…ÛŒâ€ŒØ±ÙØª. Ø¨Ø§ØªØ±ÛŒ Ø¯Ø±ÙˆÙ† Ù…Ø§Ø´ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´Ø¯ ØªØ§ Ø­Ø±Ú©Øª Ú©Ù†Ø¯. \n",
      "\n",
      "ØªÛŒÙ… Ú¯ÙØª: \"Ø³Ø§Ù…ØŒ Ø¨Ø§ØªØ±ÛŒ Ù…Ø­Ú©Ù… Ø¯Ø± Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Øª. Ù†Ù…ÛŒâ€ŒØ§ÙØªØ¯ Ø¨ÛŒØ±ÙˆÙ†.\" Ø³Ø§Ù… Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§Ø²ÛŒâ€ŒÚ©Ø±Ø¯Ù† Ø±Ø§ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø§Ø´ÛŒÙ† Ø±Ø§ Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ù‡ Ø¯ÙˆØ± Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ØŒ Ù…ÛŒâ€ŒØ®Ù†Ø¯ÛŒØ¯Ù†Ø¯ Ùˆ Ø®ÙˆØ´ Ù…ÛŒâ€ŒÚ¯Ø°Ø±Ø§Ù†Ø¯Ù†Ø¯.\n",
      "\n",
      "Ø§Ù…Ø§ Ù†Ø§Ú¯Ù‡Ø§Ù† Ø§ØªÙØ§Ù‚ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡â€ŒØ§ÛŒ Ø§ÙØªØ§Ø¯. ÛŒÚ© Ø³Ú¯ Ø¨Ø²Ø±Ú¯ Ø¢Ù…Ø¯ Ùˆ Ù…Ø§Ø´ÛŒÙ†â€ŒØ¨Ø§Ø²ÛŒ Ø±Ø§ Ø¯Ø± Ø¯Ù‡Ø§Ù†Ø´ Ú¯Ø±ÙØª! ØªÛŒÙ… Ùˆ Ø³Ø§Ù… ØªØ±Ø³ÛŒØ¯Ù†Ø¯ØŒ Ø§Ù…Ø§ Ø³Ú¯ ÙÙ‚Ø· Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ø§Ùˆ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯. Ù‡Ù…Ù‡ Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ø³Ú¯ Ø¨Ø§ Ù…Ø§Ø´ÛŒÙ† Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨ÙˆØ¯. Ø¯Ø± Ù†Ù‡Ø§ÛŒØªØŒ ØªÛŒÙ…ØŒ Ø³Ø§Ù… Ùˆ Ø³Ú¯ Ø¯ÙˆØ³ØªØ§Ù† Ø®ÙˆØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù… Ø´Ø¯Ù†Ø¯.  Ù…Ø±Ø¯ÛŒ Ø¯Ø±ÛŒØ§Ù†ÙˆØ±Ø¯ Ø¨Ù‡ Ù†Ø§Ù… ØªØ§Ù… Ø¨ÙˆØ¯. Ø§Ùˆ Ú©Ø´ØªÛŒ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø´Øª. ØªØ§Ù… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¯Ø± Ø¯Ø±ÛŒØ§ Ø¨Ø§ Ú©Ø´ØªÛŒ Ø§Ø´ Ø¨Ø±ÙˆØ¯. Ø±ÙˆØ²ÛŒ ØªØ§Ù… Ù…Ø§Ù‡ÛŒ Ú©ÙˆÚ†Ú©ÛŒ Ø¯ÛŒØ¯. Ø¢Ù† Ù…Ø§Ù‡ÛŒ ØºÙ…Ú¯ÛŒÙ† Ø¨ÙˆØ¯. Ú¯Ù… Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø§Ø´ Ø¨Ø±Ú¯Ø±Ø¯Ø¯. \n",
      "\n",
      "ØªØ§Ù… Ú¯ÙØª: Ù…Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù…ØŒ Ù…Ø§Ù‡ÛŒ Ú©ÙˆÚ†ÙˆÙ„Ùˆ. ØªÙˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¯Ø± Ú©Ø´ØªÛŒ Ù…Ù† Ø¬Ø§ Ø´ÙˆÛŒØŒ Ù…Ù† Ù‡Ù… ØªÙˆ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡â€ŒØ§Øª Ù…ÛŒâ€ŒØ±Ø³Ø§Ù†Ù….â€Œ Ø¢Ù† Ù…Ø§Ù‡ÛŒ Ú©ÙˆÚ†Ú© Ø®ÙˆØ´Ø­Ø§Ù„ Ø´Ø¯. Ù¾Ø±ÛŒØ¯ ØªÙˆÛŒ Ú©Ø´ØªÛŒ ØªØ§Ù…. Ø¢Ù†Ù‡Ø§ Ø¨Ø§ Ù‡Ù… Ø¯Ø± Ø¯Ø±ÛŒØ§ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø´Ù†Ø§ Ú©Ø±Ø¯Ù†Ø¯.  \n",
      "\n",
      "Ø¯Ø±ÛŒØ§ Ø¢Ø±Ø§Ù… Ùˆ Ø§Ù…Ù† Ø¨ÙˆØ¯. ØªØ§Ù… Ùˆ Ù…Ø§Ù‡ÛŒ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ø§ Ù‡Ù… Ø­Ø±Ù Ø²Ø¯Ù†Ø¯ Ùˆ Ø®Ù†Ø¯ÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³ØªØ§Ù† Ø®ÙˆØ¨ÛŒ Ø´Ø¯Ù†Ø¯. Ø³Ø±Ø§Ù†Ø¬Ø§Ù…ØŒ Ø®Ø§Ù†Ù‡ Ù…Ø§Ù‡ÛŒ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù†Ø¯. Ù…Ø§Ù‡ÛŒ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ú¯ÙØª: Ù…Ù…Ù†ÙˆÙ†Ù… ØªØ§Ù… Ú©Ù‡ Ú©Ù…Ú©Ù… Ú©Ø±Ø¯ÛŒ. ØªØ§Ù… Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯ Ùˆ Ø®Ø¯Ø§Ø­Ø§ÙØ¸ÛŒ Ú©Ø±Ø¯. Ø§Ùˆ Ø¯ÙˆØ± Ø´Ø¯ Ùˆ Ø±ÙØªØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø³Øª Ø¯ÙˆØ³Øª Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.  ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ø¸Ø±Ù ØµØ§Ù ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª.Ø¢Ù† Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ¨Ø§ Ø¨ÙˆØ¯. Ø§ÛŒÙ† Ø¸Ø±Ù Ø¯Ø± ÛŒÚ© Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ø§ Ø¯Ø®ØªØ±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ùˆ Ù…Ø§Ø¯Ø±Ø´ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. \n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ù„ÛŒÙ„ÛŒ Ùˆ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒ Ø¨Ù‡ Ø¨ÛŒØ±ÙˆÙ† Ø±ÙØªÙ†Ø¯. Ù‚Ø¨Ù„ Ø§Ø² Ø±ÙØªÙ†ØŒ Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª: \"Ù„ÛŒÙ„ÛŒØŒ Ù„Ø·ÙØ§Ù‹ Ø¯Ø± Ø±Ø§ Ø¨Ø¨Ù†Ø¯.\" Ù„ÛŒÙ„ÛŒ Ø¯Ø± Ø±Ø§ Ø¨Ø³Øª Ùˆ Ø¢Ù†Ù‡Ø§ ØªÙ…Ø§Ù… Ø±ÙˆØ² Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯.\n",
      "ÙˆÙ‚ØªÛŒ Ø¨Ø±Ú¯Ø´ØªÙ†Ø¯ØŒ Ø¯ÛŒØ¯Ù†Ø¯ Ø¸Ø±Ù ØµØ§Ù Ø±ÙˆÛŒ Ø²Ù…ÛŒÙ† Ø§ÙØªØ§Ø¯Ù‡ Ùˆ Ø´Ú©Ø³ØªÙ‡! Ù„ÛŒÙ„ÛŒ Ùˆ Ù…Ø§Ø¯Ø±Ø´ Ø®ÛŒÙ„ÛŒ Ù†Ø§Ø±Ø§Ø­Øª Ø´Ø¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ù†ØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø¸Ø±Ù Ø²ÛŒØ¨Ø§ Ø±Ø§ ØªØ¹Ù…ÛŒØ± Ú©Ù†Ù†Ø¯. Ù¾Ø§ÛŒØ§Ù†.  Ø³Ø§Ø±Ø§ Ùˆ Ø¨Ù† Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³ØªÙ†Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø§Ø¯Ø±Ø´Ø§Ù† ÛŒÚ© Ú©Ø§Ø³Ù‡ Ø±Ø§ ØªØ²Ø¦ÛŒÙ† Ú©Ù†Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ ÛŒÚ© Ú©Ø§Ø³Ù‡ Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¯Ø± Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø¹Ø¶ÛŒ Ø±Ù†Ú¯ Ùˆ Ù‚Ù„Ù… Ù…ÙˆÙ‡Ø§ Ø±Ø§. Ø¢Ù†â€ŒÙ‡Ø§ Ú©Ø§Ø³Ù‡ Ùˆ Ø±Ù†Ú¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø´ØªÙ†Ø¯ Ùˆ Ø¨Ù‡ Ø­ÛŒØ§Ø· Ù¾Ø´ØªÛŒ Ø¨Ø±Ø¯Ù†Ø¯ Ùˆ Ø±ÙˆÛŒ ÛŒÚ© Ù…ÛŒØ² Ú¯Ø°Ø§Ø´ØªÙ†Ø¯.\n",
      "\n",
      "Ø³Ø§Ø±Ø§ Ú¯ÙØª: Â«Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø§ÛŒÙ† Ú©Ø§Ø³Ù‡ Ø±Ø§ Ø¨Ø§ Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø²ÛŒØ¨Ø§ Ú©Ù†ÛŒÙ….Â» \n",
      "\n",
      "Ø¨Ù† Ú¯ÙØª: Â«Ø¨Ø§Ø´Ù‡ØŒ Ù…Ù† ÛŒÚ© Ú¯Ù„ Ù…ÛŒâ€ŒÚ©Ø´Ù….Â»\n",
      "\n",
      "Ø¢Ù†â€ŒÙ‡Ø§ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø±Ù†Ú¯ Ø¢Ù…ÛŒØ²ÛŒ Ú©Ø§Ø³Ù‡ Ø¨Ø§ Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø±Ø¯Ù†Ø¯. Ø³Ø§Ø±Ø§ ÛŒÚ© Ù‚Ù„Ø¨ Ù‚Ø±Ù…Ø² Ú©Ø´ÛŒØ¯ Ùˆ Ø¨Ù† ÛŒÚ© Ú¯Ù„ Ø²Ø±Ø¯ Ú©Ø´ÛŒØ¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ø§Ø¯Ù†Ø¯. \n",
      "\n",
      "Ø§Ù…Ø§ Ù†Ø§Ú¯Ù‡Ø§Ù† Ø¨Ø§Ø±Ø§Ù† Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ø¨Ø§Ø±Ø§Ù† Ø®ÛŒØ³ Ùˆ Ø³Ø±Ø¯ Ø¨ÙˆØ¯. Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ø´Ø¯ Ø±Ù†Ú¯â€ŒÙ‡Ø§ Ø¨Ø±ÛŒØ²Ù†Ø¯ Ùˆ Ø¨Ø¯Ø±Ø®Ø´Ù†Ø¯. Ú©Ø§Ø³Ù‡ Ø¨Ù‡ Ù†Ø¸Ø± Ø¨Ù‡Ù… Ø±ÛŒØ®ØªÙ‡ Ùˆ Ø²Ø´Øª Ù…ÛŒâ€ŒØ¢Ù…Ø¯.\n",
      "\n",
      "Ø³Ø§Ø±Ø§ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯: Â«Ø§ÛŒ ÙˆØ§ÛŒØŒ Ø¨Ø§Ø±Ø§Ù† Ú©Ø§Ø³Ù‡ Ù…Ø§ Ø±Ø§ Ø®Ø±Ø§Ø¨ Ú©Ø±Ø¯!Â»\n",
      "\n",
      "Ø¨Ù† Ú¯ÙØª: Â«Ù…Ø§Ù…Ø§Ù† Ø§Ø² Ø¢Ù† Ø®ÙˆØ´Ø´ Ù†Ù…ÛŒâ€ŒØ¢ÛŒØ¯.Â» \n",
      "\n",
      "Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§ Ú©Ø§Ø³Ù‡ Ø¨Ù‡ Ø¯Ø§Ø®Ù„ Ø®Ø§Ù†Ù‡ Ø¯ÙˆÛŒØ¯Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ ØºÙ…Ú¯ÛŒÙ† Ùˆ Ø®ÛŒØ³ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯.\n",
      "\n",
      "Ø¢Ù†â€ŒÙ‡Ø§ Ú©Ø§Ø³Ù‡ Ø±Ø§ Ø¨Ù‡ Ù…Ø§Ø¯Ø±Ø´Ø§Ù† Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ Ú©Ø±Ø¯Ù†Ø¯. \n",
      "\n",
      "Ø§Ù…Ø§ Ù…Ø§Ø¯Ø± Ø¨Ø§ Ù„Ø¨Ø®Ù†Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ØºÙ„ Ú©Ø±Ø¯. Ø§Ùˆ Ú¯ÙØª Ø¯ÙˆØ³ØªØ´Ø§Ù† Ø¯Ø§Ø±Ø¯ Ùˆ Ø§Ø² Ø§ÛŒÙ† Ú©Ø§Ø³Ù‡ Ø®ÙˆØ´Ø´ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª. \n",
      "\n",
      "Ø§Ùˆ Ú¯ÙØª: Â«Ø§ÛŒÙ† ÛŒÚ© Ú©Ø§Ø³Ù‡ Ø²ÛŒØ¨Ø§Ø³Øª. Ø´Ù…Ø§ Ø¢Ù† Ø±Ø§ Ø¨Ø§ Ø¹Ø´Ù‚ Ùˆ Ø®Ù„Ø§Ù‚ÛŒØª Ø¯Ø±Ø³Øª Ú©Ø±Ø¯ÛŒØ¯. Ø¨Ø§Ø±Ø§Ù† Ø¢Ù† Ø±Ø§ ÙˆÛŒÚ˜Ù‡ Ú©Ø±Ø¯. Ø§Ù†Ú¯Ø§Ø± ÛŒÚ© Ú©Ø§Ø³Ù‡ Ø±Ù†Ú¯ÛŒÙ† Ú©Ù…Ø§Ù† Ø§Ø³Øª.Â»\n",
      "\n",
      "Ø³Ø§Ø±Ø§ Ùˆ Ø¨Ù† Ø§Ø­Ø³Ø§Ø³ Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ Ú©Ø±Ø¯Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø§Ø¯Ø±Ø´Ø§Ù† Ø±Ø§ Ø¨ÙˆØ³ÛŒØ¯Ù†Ø¯ Ùˆ Ø§Ø² Ø§Ùˆ ØªØ´Ú©Ø± Ú©Ø±Ø¯Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ†Ø¯ Ú©Ù‡ Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§ØªØŒ Ú†ÛŒØ²Ù‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø®ÙˆØ¨ Ø¨Ø§Ø´Ù†Ø¯ Ø­ØªÛŒ ÙˆÙ‚ØªÛŒ Ø¨Ù‡ Ù†Ø¸Ø± Ø¨Ø¯ Ù…ÛŒâ€ŒØ¢ÛŒÙ†Ø¯.  ÛŒÚ© Ø±ÙˆØ² Ù…Ø§Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ Ø¹Ø±ÙˆØ³Ú©â€ŒÙ‡Ø§ÛŒØ´ Ú¯ÙØª: Ø¹Ø±ÙˆØ³Ú©â€ŒÙ‡Ø§ÛŒ Ø¹Ø²ÛŒØ²Ù…ØŒ Ø§Ù…Ø±ÙˆØ² Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø¨Ø±Ø§ÛŒØªØ§Ù† Ø¯Ø§Ø³ØªØ§Ù†ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ù†Ù…. Ø¹Ø±ÙˆØ³Ú©â€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ Ø°ÙˆÙ‚â€ŒØ²Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ Ùˆ Ø¯ÙˆØ± Ù…Ø§Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ø¬Ù…Ø¹ Ø´Ø¯Ù†Ø¯. \n",
      "\n",
      "Ù…Ø§Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ú¯ÙØª: ÛŒÚ©ÛŒ Ø¨ÙˆØ¯ ÛŒÚ©ÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ø¯Ø± ÛŒÚ© Ø´Ù‡Ø± Ú©ÙˆÚ†Ú©ØŒ Ù¾Ø³Ø±ÛŒ Ù†Ø§Ø´Ù†ÙˆØ§ Ø¨Ù‡ Ø§Ø³Ù… ØªØ§Ù… Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. ØªØ§Ù… Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³Øª ØµØ¯Ø§Ù‡Ø§ Ø±Ø§ Ø¨Ø´Ù†ÙˆØ¯ØŒ Ø§Ù…Ø§ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ø¯Ø± Ø¯ÛŒØ¯Ù† Ùˆ Ù„Ù…Ø³ Ú©Ø±Ø¯Ù† Ú†ÛŒØ²Ù‡Ø§ Ù…Ù‡Ø§Ø±Øª Ø¯Ø§Ø´Øª. Ø§Ùˆ Ø¯ÙˆØ³ØªØ§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø´Øª Ú©Ù‡ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ø§ Ø§Ùˆ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ ØªÚ¯ Ùˆ Ù‚Ø§ÛŒÙ… Ù…ÙˆØ´Ú© Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯. ØªØ§Ù… Ø®ÛŒÙ„ÛŒ Ø³Ø±ÛŒØ¹ Ù…ÛŒâ€ŒØ¯ÙˆÛŒØ¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ø±Ø¯.  \n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ø²Ù†Ú¯ Ø´Ù‡Ø± Ø¯Ø± Ø¸Ù‡Ø± Ø¨Ù‡ ØµØ¯Ø§ Ø¯Ø±Ù…ÛŒâ€ŒØ¢Ù…Ø¯. Ù‡Ù…Ù‡ Ù…Ø´ØªØ§Ù‚ Ø¨ÙˆØ¯Ù†Ø¯ Ú©Ù‡ ØµØ¯Ø§ÛŒ Ø²Ù†Ú¯ Ø±Ø§ Ø¨Ø´Ù†ÙˆÙ†Ø¯ØŒ Ø§Ù…Ø§ ØªØ§Ù… Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø³Øª Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ø´Ù†ÙˆØ¯. Ø§Ùˆ Ø§Ø­Ø³Ø§Ø³ ØºÙ…Ú¯ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯ØŒ Ø§Ù…Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ù†Ù‚Ø´Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø´ØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØªÙ†Ø¯ Ø¨Ù‡ ØªØ§Ù… Ú©Ù…Ú© Ú©Ù†Ù†Ø¯ ØªØ§ Ø§Ùˆ Ù‡Ù… Ù„Ø±Ø²Ø´ Ø²Ù†Ú¯ Ø±Ø§ Ø§Ø­Ø³Ø§Ø³ Ú©Ù†Ø¯. Ø¢Ù†Ù‡Ø§ ØªØ§Ù… Ø±Ø§ Ù†Ø²Ø¯ÛŒÚ© Ø²Ù†Ú¯ Ø¨Ø²Ø±Ú¯ Ø¨Ø±Ø¯Ù†Ø¯ Ùˆ Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ Ø²Ù†Ú¯ Ø¨Ù‡ ØµØ¯Ø§ Ø¯Ø±Ù…ÛŒâ€ŒØ¢ÛŒØ¯ØŒ Ø¢Ù† Ø±Ø§ Ù„Ù…Ø³ Ú©Ù†Ø¯. ÙˆÙ‚ØªÛŒ Ø²Ù†Ú¯ Ø¨Ù‡ ØµØ¯Ø§ Ø¯Ø±Ø¢Ù…Ø¯ØŒ ØªØ§Ù… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³Øª Ù„Ø±Ø²Ø´ Ù‚ÙˆÛŒ Ø¢Ù† Ø±Ø§ Ø§Ø­Ø³Ø§Ø³ Ú©Ù†Ø¯ Ùˆ Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯.  \n",
      "\n",
      "Ù…ÙˆØ±Ø§Ù„ Ø§ÛŒÙ† Ø¯Ø§Ø³ØªØ§Ù† Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø­ØªÛŒ Ø§Ú¯Ø± Ú©Ø³ÛŒ Ù…ØªÙØ§ÙˆØª Ø§Ø³Øª ÛŒØ§ Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ù…Ø«Ù„ Ù†Ø§Ø´Ù†ÙˆØ§ Ø¨ÙˆØ¯Ù† ØªØ§Ù…ØŒ Ø¨Ø§Ø² Ù‡Ù… Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø±Ø§Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨ÛŒØ§Ø¨ÛŒÙ… ØªØ§ Ø¨Ù‡ Ø§Ùˆ Ú©Ù…Ú© Ú©Ù†ÛŒÙ… Ùˆ Ø®ÙˆØ´Ø­Ø§Ù„Ø´ Ú©Ù†ÛŒÙ…. Ù…Ø§ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ù‡Ù…Ù‡ØŒ ØµØ±Ù Ù†Ø¸Ø± Ø§Ø² ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒØ´Ø§Ù†ØŒ Ù…Ù‡Ø±Ø¨Ø§Ù† Ùˆ Ù…Ø±Ø§Ù‚Ø¨ Ø¨Ø§Ø´ÛŒÙ….  ØªØ§Ù… Ùˆ Ù„ÛŒÙ„ÛŒ Ø¯ÙˆÙ‚Ù„ÙˆÙ‡Ø§ÛŒÛŒ Ø¨ÙˆØ¯Ù†Ø¯ Ú©Ù‡ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯. ÛŒÚ© Ø±ÙˆØ² Ø¢Ù†Ù‡Ø§ ÛŒÚ© Ú¯ÛŒØ±Ù‡ Ø¨Ø²Ø±Ú¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø² Ø¨ÙˆØªÙ‡ Ù‡Ø§ÛŒ Ø³Ø¨Ø² Ø¯ÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ù…ÛŒ Ø®ÙˆØ§Ø³ØªÙ†Ø¯ Ø¢Ù† Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†Ù†Ø¯ Ùˆ Ø±Ø§Ù‡ Ø®Ø±ÙˆØ¬ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù†Ø¯. \n",
      "Ø¢Ù†Ù‡Ø§ Ø¯Ø±ÙˆÙ† Ú¯ÛŒØ±Ù‡ Ø¯ÙˆÛŒØ¯Ù†Ø¯ Ùˆ Ù…Ø³ÛŒØ±Ù‡Ø§ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø±Ø¯Ù†Ø¯. Ú¯Ø§Ù‡ÛŒ Ø¨Ù‡ Ú†Ù¾ Ù…ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù†Ø¯ØŒ Ú¯Ø§Ù‡ÛŒ Ø¨Ù‡ Ø±Ø§Ø³Øª. Ø¢Ù†Ù‡Ø§ Ú¯Ù„Ù‡Ø§ Ùˆ Ù¾Ø±Ù†Ø¯Ù‡ Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø¯ÛŒØ¯Ù†Ø¯ØŒ Ø§Ù…Ø§ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ù†Ø¯ÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø§Ø­Ø³Ø§Ø³ Ú¯Ù… Ø´Ø¯Ù† Ùˆ ØªØ±Ø³ÛŒØ¯Ù† Ú©Ø±Ø¯Ù†Ø¯.\n",
      "Ø¢Ù†Ù‡Ø§ ØºØ±Ø´ Ø¨Ù„Ù†Ø¯ÛŒ Ø±Ø§ Ù¾Ø´Øª Ø³Ø±Ø´Ø§Ù† Ø´Ù†ÛŒØ¯Ù†Ø¯. ÛŒÚ© Ø´ÛŒØ± Ø¨Ø²Ø±Ú¯ Ú©Ù‡ Ø¯Ø± Ú¯ÛŒØ±Ù‡ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ Ø¨ÙˆØ¯. Ø§Ùˆ Ú¯Ø±Ø³Ù†Ù‡ Ùˆ Ø¹ØµØ¨Ø§Ù†ÛŒ Ø¨ÙˆØ¯. Ø§Ùˆ Ù…ÛŒ Ø®ÙˆØ§Ø³Øª ØªØ§Ù… Ùˆ Ù„ÛŒÙ„ÛŒ Ø±Ø§ Ø¨Ú¯ÛŒØ±Ø¯ Ùˆ Ø¨Ø®ÙˆØ±Ø¯.  \n",
      "ØªØ§Ù… Ùˆ Ù„ÛŒÙ„ÛŒ Ø§Ø² Ø´ÛŒØ± Ø¯ÙˆØ± Ø´Ø¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ø¬Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÛŒ Ø´Ø¯Ù† Ú¯Ø´ØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ Ø³ÙˆØ±Ø§Ø® Ú©ÙˆÚ†Ú©ÛŒ Ø¯Ø± Ø¨ÙˆØªÙ‡ Ù‡Ø§ Ø¯ÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯Ø§Ø®Ù„ Ø³ÙˆØ±Ø§Ø® Ø®Ø²ÛŒØ¯Ù†Ø¯ Ùˆ Ø§Ù…ÛŒØ¯ÙˆØ§Ø± Ø¨ÙˆØ¯Ù†Ø¯ Ø´ÛŒØ± Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ù†Ø¯.\n",
      "Ø´ÛŒØ± Ù†Ø²Ø¯ÛŒÚ© Ùˆ Ù†Ø²Ø¯ÛŒÚ© ØªØ± Ø´Ø¯. Ø§Ùˆ ØªØ§Ù… Ùˆ Ù„ÛŒÙ„ÛŒ Ø±Ø§ Ø¯Ø± Ø³ÙˆØ±Ø§Ø® Ø¨Ùˆ Ù…ÛŒ Ú©Ø´ÛŒØ¯. Ø§Ùˆ Ú†Ù†Ú¯Ø§Ù„Ø´ Ø±Ø§ Ø¯Ø§Ø®Ù„ Ø³ÙˆØ±Ø§Ø® Ø¨Ø±Ø¯ Ùˆ Ø³Ø¹ÛŒ Ú©Ø±Ø¯ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ú¯ÛŒØ±Ø¯. Ø§Ùˆ Ù¾Ø§ÛŒ Ù„ÛŒÙ„ÛŒ Ø±Ø§ Ø¨Ø§ Ú†Ù†Ú¯Ø§Ù„Ù‡Ø§ÛŒ ØªÛŒØ²Ø´ ÙØ´Ø±Ø¯. Ù„ÛŒÙ„ÛŒ Ø§Ø² Ø¯Ø±Ø¯ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯.  \n",
      "ØªØ§Ù… Ø´Ø¬Ø§Ø¹ Ùˆ Ø¨Ø§Ù‡ÙˆØ´ Ø¨ÙˆØ¯. Ø§Ùˆ ÛŒÚ© Ú†ÙˆØ¨ Ø±ÙˆÛŒ Ø²Ù…ÛŒÙ† Ø¯ÛŒØ¯. Ø¢Ù† Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø´Øª Ùˆ Ú†Ø´Ù… Ø´ÛŒØ± Ø±Ø§ Ø¨Ø§ Ø¢Ù† Ø³ÙˆØ±Ø§Ø® Ú©Ø±Ø¯. Ø´ÛŒØ± Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯ Ùˆ Ø¹Ù‚Ø¨ Ù†Ø´Ø³Øª. Ø§Ùˆ Ù…Ø¬Ø±ÙˆØ­ Ùˆ ØªØ±Ø³ÛŒØ¯Ù‡ Ø¨ÙˆØ¯.\n",
      "ØªØ§Ù… Ùˆ Ù„ÛŒÙ„ÛŒ ÙØ±ØµØª Ø®ÙˆØ¯ Ø±Ø§ Ø¯ÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø§Ø² Ø³ÙˆØ±Ø§Ø® Ùˆ Ø§Ø² Ú¯ÛŒØ±Ù‡ Ø¨ÛŒØ±ÙˆÙ† Ø¯ÙˆÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ù…Ø§Ø¯Ø± Ùˆ Ù¾Ø¯Ø±Ø´Ø§Ù† Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ù…Ù†ØªØ¸Ø±Ø´Ø§Ù† Ø¨ÙˆØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨ØºÙ„ Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒØ´Ø§Ù† ØªØ¹Ø±ÛŒÙ Ú©Ø±Ø¯Ù†Ø¯ Ú†Ù‡ Ø§ØªÙØ§Ù‚ÛŒ Ø§ÙØªØ§Ø¯Ù‡ Ø§Ø³Øª. Ø¢Ù†Ù‡Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ Ùˆ Ø§Ù…Ù† Ø¨ÙˆØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯ÛŒÚ¯Ø± Ù‡Ø±Ú¯Ø² Ø¨Ù‡ Ú¯ÛŒØ±Ù‡ Ø¨Ø±Ù†Ú¯Ø´ØªÙ†Ø¯.  ÛŒÚ© Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ù…ÛŒØ´Ù‡ ÛŒÚ© Ù†ÙØ± Ø¨Ù‡ Ø§Ø³Ù… Ø³Ùˆ Ø¨ÙˆØ¯. Ø§Ùˆ Ø®ÛŒÙ„ÛŒ ÙˆØ²Ù† Ø¯Ø§Ø´Øª Ùˆ Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª Ú©Ù‡ Ø¨Ø§ÛŒØ¯ ØºØ°Ø§ÛŒ Ú©Ù…ØªØ±ÛŒ Ø¨Ø®ÙˆØ±Ø¯ Ùˆ Ø¨ÛŒØ´ØªØ± ÙˆØ±Ø²Ø´ Ú©Ù†Ø¯. Ù¾Ø³ Ø³Ùˆ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØª Ú©Ù‡ Ù‡Ø± Ø±ÙˆØ² Ø¯ÙˆØ± Ù…Ø­Ù„Ù‡ Ù‚Ø¯Ù… Ø¨Ø²Ù†Ø¯.\n",
      "Ø±ÙˆØ² Ø§ÙˆÙ„ Ø³Ùˆ Ø§Ø­Ø³Ø§Ø³ Ú©Ø±Ø¯ Ú©Ù‡ Ø®ÛŒÙ„ÛŒ Ø³Ø®Øª Ø§Ø³Øª. Ø§Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ú†Ù†Ø¯ Ø±ÙˆØ²ØŒ Ø³Ùˆ ÙÙ‡Ù…ÛŒØ¯ Ú©Ù‡ Ø¢Ø³Ø§Ù† Ø§Ø³Øª! Ø§Ùˆ Ù‡Ø± Ø±ÙˆØ² Ø³Ø¨Ú©â€ŒØªØ± Ùˆ Ø³Ø¨Ú©â€ŒØªØ± Ù…ÛŒâ€ŒØ´Ø¯.\n",
      "Ø¨Ù‡ Ø²ÙˆØ¯ÛŒ Ø³Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª ÙˆØ±Ø²Ø´ÛŒ Ø³Ø±Ú¯Ø±Ù…â€ŒÚ©Ù†Ù†Ø¯Ù‡â€ŒØªØ± Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯. Ù…Ø§Ø¯Ø± Ø³Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø±Ø¯ Ú©Ù‡ Ø¨Ø§ Ø¢Ù‡Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡â€ŒØ§Ø´ Ø¯Ø± ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ† Ø¨Ø±Ù‚ØµØ¯. Ø³Ùˆ ÙÚ©Ø± Ú©Ø±Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ø§ÛŒØ¯Ù‡â€ŒÛŒ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª.\n",
      "Ù¾Ø³ Ø³Ùˆ ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ† Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ØŒ Ø¢Ù‡Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡â€ŒØ§Ø´ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯ Ùˆ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø±Ù‚ØµÛŒØ¯Ù† Ú©Ø±Ø¯. Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø³Ø®Øª Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ú†ÙˆÙ† Ø¬Ø§Ù„Ø¨ Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ Ø²ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒØ´ Ø¢Ø³Ø§Ù† Ø´Ø¯. Ùˆ Ø¨Ù‡ Ø²ÙˆØ¯ÛŒØŒ Ø³Ùˆ Ø¯ÛŒÚ¯Ø± Ø§ØµÙ„Ø§ ÙˆØ²Ù†ÛŒ Ù†Ø¯Ø§Ø´Øª!  Ø¨Ø§ ØªØ´ÙƒØ± Ø§Ø² Ø´Ù…Ø§ØŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø´Ø±Ø­ Ø²ÛŒØ± Ø§Ø³Øª:\n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ù¾Ø³Ø± Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¨Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØªÙ†Ø¯. ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø±ÙˆÛŒ ØªØ§Ø¨ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯ Ùˆ Ø§Ø² Ø³Ø±Ø³Ø±Ù‡ Ø¨Ù„ØºØ²Ø¯. Ù…Ø§Ø¯Ø±Ø´ Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø·Ù…Ø¦Ù† Ùˆ Ø§Ùˆ Ø±Ø§ Ø¨Ù‡ Ø¯Ù‚Øª ØªØ­Øª Ù†Ø¸Ø± Ø¯Ø§Ø´Øª. Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù… Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´ Ú¯Ø°Ø±Ø§Ù†Ø¯Ù†Ø¯. \n",
      "Ø¨Ù‡â€ŒØ²ÙˆØ¯ÛŒ ØªÛŒÙ… Ø¨Ù‡ Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª: \"Ù…Ù† Ø¨Ø§ÛŒØ¯ Ø§Ø² ØªÙˆØ§Ù„Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù….\" Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª: \"Ø¨Ø§Ø´Ù‡ØŒ Ø¨ÛŒØ§ Ø¨Ø±ÛŒÙ… ÛŒÚ©ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒÙ….\" Ø¢Ù†â€ŒÙ‡Ø§ ØªÙˆØ§Ù„ØªÛŒ Ù†Ø²Ø¯ÛŒÚ© Ù¾Ø§Ø±Ú© Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù†Ø¯. Ø§Ù…Ø§ ØªÙˆØ§Ù„Øª Ú©Ø«ÛŒÙ Ùˆ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨ÙˆØ¯. Ù…Ø§Ø¯Ø± ØªÛŒÙ… Ø®ÙˆØ´Ø´ Ù†ÛŒØ§Ù…Ø¯ØŒ Ø§Ù…Ø§ ØªÛŒÙ… ÙˆØ§Ù‚Ø¹Ø§ Ø¨Ø§ÛŒØ¯ Ù…ÛŒâ€ŒØ±ÙØª. \n",
      "ØªÛŒÙ… Ø¯Ø§Ø®Ù„ ØªÙˆØ§Ù„Øª Ø±ÙØª Ùˆ Ø³Ø¹ÛŒ Ú©Ø±Ø¯ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯. Ø§Ù…Ø§ Ù†Ø§Ú¯Ù‡Ø§Ù† ØªÙˆØ§Ù„Øª Ø´Ú©Ø³Øª Ùˆ Ø¢Ø¨ Ø¨ÛŒØ±ÙˆÙ† Ø¢Ù…Ø¯. ØªÛŒÙ… Ø®ÛŒÙ„ÛŒ Ø®ÛŒØ³ Ùˆ Ú©Ø«ÛŒÙ Ø´Ø¯.  Ø§Ùˆ Ú¯Ø±ÛŒÙ‡ Ú©Ø±Ø¯ Ùˆ Ú¯ÙØª: \"Ù…Ø§Ù…Ø§Ù†ØŒ Ù…Ù† Ø§ÛŒÙ† ØªÙˆØ§Ù„Øª Ø±Ùˆ Ø¯ÙˆØ³Øª Ù†Ø¯Ø§Ø±Ù…!\" Ù…Ø§Ø¯Ø±Ø´ Ø§Ùˆ Ø±Ø§ Ø¯Ø± Ø¢ØºÙˆØ´ Ú¯Ø±ÙØª Ùˆ Ú¯ÙØª: \"Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ù… ØªÛŒÙ…ØŒ Ø¨Ø§Ø± Ø¯ÛŒÚ¯Ù‡ ÛŒÚ©ÛŒ Ø¨Ù‡ØªØ± Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\" Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø±Ø­Ø§Ù„ÛŒâ€ŒÚ©Ù‡ ØªÛŒÙ… Ù‡Ù†ÙˆØ² ØºÙ…Ú¯ÛŒÙ† Ùˆ Ø®ÛŒØ³ Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ú¯Ø´ØªÙ†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ØŒ Ø¯Ø®ØªØ±Ú© Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÙˆØ³ÛŒ Ø¨Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯Ø±Ø®Øª Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯Ù†Ø¯ Ú©Ù‡ Ù¾Ø± Ø§Ø² Ú¯Ù„ Ø¨ÙˆØ¯. Ú¯Ù„â€ŒÙ‡Ø§ Ø¨ÙˆÛŒÛŒ Ø´ÛŒØ±ÛŒÙ† Ø¯Ø§Ø´ØªÙ†Ø¯. Ù„ÙˆØ³ÛŒ Ø§Ø² Ø¨ÙˆÛŒ Ø¢Ù†Ù‡Ø§ Ø®ÙˆØ´Ø´ Ø¢Ù…Ø¯. Ø§Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ú¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±ÙˆÛŒ Ø¯Ø±Ø®Øª Ø±Ø§ Ø¨Ø´Ù…Ø§Ø±Ø¯. \n",
      "Ù„ÙˆØ³ÛŒ Ù¾Ø±Ø³ÛŒØ¯: \"Ù…Ø§Ù…Ø§Ù†ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ú¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø´Ù…Ø§Ø±Ù…ØŸ\" Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª: \"Ø¨Ù„Ù‡ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ú¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø´Ù…Ø§Ø±ÛŒ. Ù…ÙˆØ§Ø¸Ø¨ Ø¨Ø§Ø´ Ú©Ù‡ Ø¯Ø³Øª Ù†Ø²Ù†ÛŒ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§.\"  \n",
      "Ù„ÙˆØ³ÛŒ Ø´Ù…Ø±Ø¯Ù† Ø±Ø§ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯. \"ÛŒÚ©ØŒ Ø¯ÙˆØŒ Ø³Ù‡\" Ú¯ÙØª. Ø§Ù…Ø§ Ù†Ø§Ú¯Ù‡Ø§Ù† Ú†ÛŒØ² Ø¹Ø¬ÛŒØ¨ÛŒ Ø¯ÛŒØ¯. Ø±ÙˆÛŒ Ú¯Ù„â€ŒÙ‡Ø§ Ù¾ÙˆØ¯Ø±ÛŒ Ø¨ÙˆØ¯. Ø§Ùˆ Ø¢Ù† Ù¾ÙˆØ¯Ø± Ø±Ø§ Ù„Ù…Ø³ Ú©Ø±Ø¯ØŒ Ù†Ø±Ù… Ø¨ÙˆØ¯. Ø¨Ù‡ Ù…Ø§Ø¯Ø±Ø´ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯.  \n",
      "Ù„ÙˆØ³ÛŒ Ù¾Ø±Ø³ÛŒØ¯: \"Ù…Ø§Ù…Ø§Ù†! Ø§ÛŒÙ† Ù¾ÙˆØ¯Ø± Ú†ÛŒØ³ØªØŸ\" Ù…Ø§Ø¯Ø±Ø´ Ø¨Ø§ Ù„Ø¨Ø®Ù†Ø¯ Ú¯ÙØª: \"Ø§ÛŒÙ† Ú¯Ø±Ø¯Ù‡ Ú¯Ù„ Ø§Ø³Øª. Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ú¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\"  \n",
      "Ù„ÙˆØ³ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯ Ú©Ù‡ Ú†ÛŒØ² Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§Ùˆ Ø´Ù…Ø±Ø¯Ù† Ú¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ ØªÙ…Ø§Ù… Ú©Ø±Ø¯ Ùˆ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ú¯Ø´ØªÙ†Ø¯. Ø­Ø§Ù„Ø§ Ù„ÙˆØ³ÛŒ Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø³Øª Ú©Ù‡ Ú¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø·Ø± Ù¾ÙˆØ¯Ø± ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±Ù†Ø¯ ØªØ§ Ø¨Ù‡ Ø±Ø´Ø¯Ø´Ø§Ù† Ú©Ù…Ú© Ú©Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ø´Ú©Ø§Ø±Ú†ÛŒ Ø¨ÙˆØ¯. Ø§Ùˆ Ø¯Ø± ÛŒÚ© Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú© Ù†Ø²Ø¯ÛŒÚ© Ø¬Ù†Ú¯Ù„ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯. Ø§Ùˆ Ø³Ú¯ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ø³Ù¾Ø§Øª Ø¯Ø§Ø´Øª. Ø§Ø³Ù¾Ø§Øª ÛŒÚ© Ø³Ú¯ Ø´Ú©Ø§Ø±ÛŒ ØªÙ†ÙˆÙ…Ù†Ø¯ Ø¨ÙˆØ¯. Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯ÙˆØ³ØªØ§Ù† Ø¨ÙˆØ¯Ù†Ø¯.  \n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ø´Ú©Ø§Ø±Ú†ÛŒ Ùˆ Ø§Ø³Ù¾Ø§Øª Ø¨Ù‡ Ø¯Ø±ÙˆÙ† Ø¬Ù†Ú¯Ù„ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØºØ°Ø§ Ø±ÙØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¯Ø± Ø¯Ø±Ø®Øª Ø¯ÛŒØ¯Ù†Ø¯. Ø´Ú©Ø§Ø±Ú†ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ ØªØ§ Ø´Ù„ÛŒÚ© Ú©Ù†Ø¯. Ø§Ø³Ù¾Ø§Øª Ø¨Ù„Ù†Ø¯ Ø¨Ù„Ù†Ø¯ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯. Ù¾Ø±Ù†Ø¯Ù‡ Ù¾Ø±ÙˆØ§Ø² Ú©Ø±Ø¯ Ùˆ ÙØ±Ø§Ø± Ú©Ø±Ø¯. Ø¢Ù†Ù‡Ø§ Ù†ØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ù¾Ø±Ù†Ø¯Ù‡ Ø±Ø§ Ø¨Ú¯ÛŒØ±Ù†Ø¯ØŒ Ø§Ù…Ø§ ØºÙ…Ú¯ÛŒÙ† Ù†Ø¨ÙˆØ¯Ù†Ø¯.  \n",
      "Ø´Ú©Ø§Ø±Ú†ÛŒ Ùˆ Ø§Ø³Ù¾Ø§Øª Ø¯Ø± Ø¬Ù†Ú¯Ù„ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯ÙˆÛŒØ¯Ù†Ø¯ Ùˆ Ù¾Ø±ÛŒØ¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´ Ú¯Ø°Ø±Ø§Ù†Ø¯Ù†Ø¯. Ø¨Ø¹Ø¯ØŒ Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú©Ø´Ø§Ù† Ø¨Ø±Ú¯Ø´ØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯ Ú©Ù‡ Ø¨Ø§ Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯. Ùˆ Ø¢Ù†Ù‡Ø§ ØªØ§ Ø§Ø¨Ø¯ Ø®ÙˆØ´Ø¨Ø®Øª Ø²Ù†Ø¯Ú¯ÛŒ Ú©Ø±Ø¯Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ØŒ ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ø¬ÙˆØ§Ù† Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª. ØªÛŒÙ… ÛŒÚ© Ù†ÛŒØ²Ù‡ Ø¨Ø§Ø²ÛŒ Ø¯Ø§Ø´Øª. Ø§Ùˆ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ú©Ù‡ ØªÙ…Ø§Ù… Ø±ÙˆØ² Ø¨Ø§ Ù†ÛŒØ²Ù‡ Ø§Ø´ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯. ÛŒÚ© Ø±ÙˆØ² ØªÛŒÙ… Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØª ØªØ§ Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯.\n",
      "Ø¯Ø± Ù¾Ø§Ø±Ú©ØŒ ØªÛŒÙ… ÛŒÚ© Ø¯Ø±Ø®Øª Ø¨Ø²Ø±Ú¯ Ø¯ÛŒØ¯. ØªÛŒÙ… Ø¨Ù‡ Ø¯ÙˆØ³ØªØ´ Ø³Ø§Ù… Ú¯ÙØª Ú©Ù‡ Ø§ÙˆÙ„ Ø¨Ø§Ù„Ø§ÛŒ Ø¯Ø±Ø®Øª Ø¨Ø±ÙˆØ¯. Ø³Ø§Ù… Ú¯ÙØª: \"Ø§ÙˆÙ„ ØªÙˆ Ø¨Ø±Ùˆ Ø¨Ø§Ù„Ø§ÛŒ Ø¯Ø±Ø®Øª!\" Ù¾Ø³ ØªÛŒÙ… Ø¨Ø§ Ù†ÛŒØ²Ù‡ Ø§Ø´ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø¨Ø§Ù„Ø§ Ø±ÙØªÙ† Ø§Ø² Ø¯Ø±Ø®Øª Ú©Ø±Ø¯.  \n",
      "ÙˆÙ‚ØªÛŒ ØªÛŒÙ… Ø¨Ù‡ Ø¨Ø§Ù„Ø§ÛŒ Ø¯Ø±Ø®Øª Ø±Ø³ÛŒØ¯ØŒ ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ø¨Ø²Ø±Ú¯ Ø¯ÛŒØ¯. Ù¾Ø±Ù†Ø¯Ù‡ Ø¨Ù‡ ØªÛŒÙ… Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ú¯ÙØª: \"Ø³Ù„Ø§Ù…! Ù…Ù† ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ø­Ø±Ù Ø²Ù† Ù‡Ø³ØªÙ…!\" ØªÛŒÙ… Ø®ÛŒÙ„ÛŒ ØªØ¹Ø¬Ø¨ Ú©Ø±Ø¯. Ø§Ùˆ Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø³Øª Ù¾Ø±Ù†Ø¯Ú¯Ø§Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø­Ø±Ù Ø¨Ø²Ù†Ù†Ø¯. Ù¾Ø±Ù†Ø¯Ù‡ Ø¯ÙˆØ³Øª Ø¬Ø¯ÛŒØ¯ ØªÛŒÙ… Ø´Ø¯ Ùˆ Ø¢Ù†Ù‡Ø§ ØªÙ…Ø§Ù… Ø±ÙˆØ² Ø±Ø§ Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ØŒ Ø¯Ø®ØªØ±ÛŒ Ø¨Ù‡ Ø§Ø³Ù… Ù„ÛŒÙ„ÛŒ ÛŒÚ© Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ø²ÛŒ Ø´Ú©Ø³ØªÙ‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯. Ø§Ùˆ Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ø¢Ù† Ø±Ø§ ØªØ¹Ù…ÛŒØ± Ú©Ù†Ø¯ Ùˆ Ø²ÛŒØ¨Ø§ÛŒØ´ Ú©Ù†Ø¯. Ù¾Ø³ØŒ Ø±ÙØª Ù¾ÛŒØ´ Ù¾Ø¯Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ú©Ù…Ú©.\n",
      "\"Ø¨Ø§Ø¨Ø§ Ø¬Ø§Ù†ØŒ Ù…ÛŒ ØªÙˆÙ†ÛŒ Ú©Ù…Ú©Ù… Ú©Ù†ÛŒ Ø§ÛŒÙ† Ù…Ø§Ø´ÛŒÙ† Ø±Ùˆ Ø¯Ø±Ø³Øª Ú©Ù†ÛŒÙ… Ùˆ Ø²ÛŒØ¨Ø§Ø´ Ú©Ù†ÛŒÙ…ØŸ\" Ù„ÛŒÙ„ÛŒ Ù¾Ø±Ø³ÛŒØ¯. Ù¾Ø¯Ø±Ø´ Ú¯ÙØª: \"Ø­ØªÙ…Ø§ØŒ Ø¨ÛŒØ§ Ø§Ø² Ø±Ù†Ú¯ Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… ØªØ§ Ø²ÛŒØ¨Ø§ØªØ±Ø´ Ú©Ù†ÛŒÙ…!\" Ù‡Ø± Ø¯Ùˆ Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯ ØªØ§ Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ø²ÛŒ Ø±Ø§ ØªØ¹Ù…ÛŒØ± Ú©Ù†Ù†Ø¯.  \n",
      "Ø¨Ø¹Ø¯ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø§Ø´ÛŒÙ† Ø±Ø§ ØªØ¹Ù…ÛŒØ± Ú©Ø±Ø¯Ù†Ø¯ØŒ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØªÙ†Ø¯ Ø¢Ù† Ø±Ø§ Ø±Ù†Ú¯ Ú©Ù†Ù†Ø¯. Ø¢Ù† Ø±Ø§ Ø¨Ø§ Ø±Ù†Ú¯ Ù‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ø²ÛŒØ§Ø¯ÛŒ Ø±Ù†Ú¯ Ú©Ø±Ø¯Ù†Ø¯. ÙˆÙ‚ØªÛŒ Ú©Ø§Ø±Ø´Ø§Ù† ØªÙ…Ø§Ù… Ø´Ø¯ØŒ Ù…Ø§Ø´ÛŒÙ† Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ¨Ø§ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. ÙˆÙ„ÛŒ Ø¨Ø¹Ø¯ØŒ Ú†ÛŒØ² ØºÛŒØ± Ù…Ù†ØªØ¸Ø±Ù‡ Ø§ÛŒ Ø§ØªÙØ§Ù‚ Ø§ÙØªØ§Ø¯. Ù…Ø§Ø´ÛŒÙ† Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø­Ø±Ú©Øª Ø®ÙˆØ¯ Ø¨Ù‡ Ø®ÙˆØ¯ÛŒ Ú©Ø±Ø¯! Ù„ÛŒÙ„ÛŒ Ùˆ Ù¾Ø¯Ø±Ø´ Ø®ÛŒÙ„ÛŒ ØªØ¹Ø¬Ø¨ Ú©Ø±Ø¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¨Ø§ Ø¢Ù† Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ø²ÛŒ Ø¬Ø§Ø¯ÙˆÛŒÛŒ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ø®ÛŒÙ„ÛŒ Ù„Ø°Øª Ø¨Ø±Ø¯Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ÛŒ Ø¨ÙˆØ¯ØŒ ÛŒÚ© Ø¯Ø®ØªØ± Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… Ú©ÛŒØª Ø¨ÙˆØ¯. Ø§Ùˆ Ø¨Ø§ØºÚ†Ù‡ Ø§ÛŒ Ø²ÛŒØ¨Ø§ Ø¯Ø± Ø­ÛŒØ§Ø· Ù¾Ø´ØªÛŒ Ø®Ø§Ù†Ù‡ Ø§Ø´ Ø¯Ø§Ø´Øª. Ø§Ùˆ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¯Ø± Ø¢Ù† Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯.\n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ø§ØªÙØ§Ù‚ Ø¹Ø¬ÛŒØ¨ÛŒ Ø¯Ø± Ø¨Ø§ØºÚ†Ù‡ Ø§ÙØªØ§Ø¯. ØªÙ…Ø§Ù… Ú¯Ù„ Ù‡Ø§ÛŒ Ø¨Ø§ØºÚ†Ù‡ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ù‚Ù‡ÙˆÙ‡ Ø§ÛŒ Ø´Ø¯Ù† Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ú©ÛŒØª Ø±Ø§ Ø¨Ø³ÛŒØ§Ø± Ù†Ú¯Ø±Ø§Ù† Ú©Ø±Ø¯.\n",
      "Ú©ÛŒØª Ø¨Ù‡ Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª: \"Ù…Ø§Ù…Ø§Ù†ØŒ Ø¨Ø§ØºÚ†Ù‡ Ø§Ù… ÙˆØ­Ø´ØªÙ†Ø§Ú© Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒ Ø±Ø³Ø¯! Ø¨Ø§ÛŒØ¯ Ú†ÛŒÚ©Ø§Ø± Ú©Ù†Ù…ØŸ\"  \n",
      "Ù…Ø§Ø¯Ø±Ø´ Ú¯ÙØª: \"Ù†Ú¯Ø±Ø§Ù† Ù†Ø¨Ø§Ø´ Ø¹Ø²ÛŒØ²Ù…. Ø¨Ø§ Ù‡Ù… Ø­Ù„Ø´ Ù…ÛŒ Ú©Ù†ÛŒÙ….\"\n",
      "Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø¨Ø§ØºÚ†Ù‡ Ø±ÙØªÙ†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø§ØµÙ„Ø§Ø­ Ø¢Ù† ØªÙ„Ø§Ø´ Ú©Ø±Ø¯Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ú¯Ù„ Ù‡Ø§ Ø±Ø§ Ø¢Ø¨ Ø¯Ø§Ø¯Ù†Ø¯ Ùˆ Ø­Ø´Ø±Ø§Øª Ø±Ø§ Ø¯ÙˆØ± Ú©Ø±Ø¯Ù†Ø¯. Ø¨Ù‡ Ø²ÙˆØ¯ÛŒØŒ Ø¨Ø§ØºÚ†Ù‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø²ÛŒØ¨Ø§ Ø´Ø¯.  \n",
      "Ú©ÛŒØª Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯ Ùˆ Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯. Ø§Ùˆ Ø§Ø² Ù…Ø§Ø¯Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ú©Ù…Ú© Ø¨Ù‡ Ø§Ùˆ ØªØ´Ú©Ø± Ú©Ø±Ø¯. Ø§Ùˆ Ù…ÛŒ Ø¯Ø§Ù†Ø³Øª Ù‡Ù…ÛŒØ´Ù‡ Ú©Ø³ÛŒ Ø±Ø§ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø§Ùˆ ØªÚ©ÛŒÙ‡ Ú©Ù†Ø¯ ÙˆÙ‚ØªÛŒ Ù†Ú¯Ø±Ø§Ù† Ø§Ø³Øª.  Ø³Ù… Ùˆ Ø¨Ù† Ø¯ÙˆØ³Øª Ù‡Ø³ØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù†Ø¯ Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯. Ø¢Ù†Ù‡Ø§ ÛŒÚ© Ø³Ú¯ Ø¨Ø²Ø±Ú¯ Ø¨Ø§ ÛŒÚ© Ù…Ø­Ø§ÙØ¸ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù†Ø¯. Ù…Ø­Ø§ÙØ¸ Ú©Ú†Ù„ Ø§Ø³Øª. Ø§Ùˆ Ù…ÙˆÛŒÛŒ Ø¨Ø± Ø³Ø±Ø´ Ù†Ø¯Ø§Ø±Ø¯.\n",
      "\"Ø¨Ù‡ Ø¢Ù† Ø³Ú¯ Ù†Ú¯Ø§Ù‡ Ú©Ù†!\" Ø³Ù… Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯. \"Ø§Ùˆ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ùˆ Ù¾Ø´Ù…Ø§Ù„ÙˆØ³Øª!\"  \n",
      "\"Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ùˆ Ø±Ø§ Ù†ÙˆØ§Ø²Ø´ Ú©Ù†ÛŒÙ…ØŸ\" Ø¨Ù† Ù…ÛŒâ€ŒÙ¾Ø±Ø³Ø¯. \"Ø§Ùˆ Ø¨Ù‡ Ù†Ø¸Ø± Ù…Ù‡Ø±Ø¨Ø§Ù† Ù…ÛŒâ€ŒØ¢ÛŒØ¯.\"  \n",
      "Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø³Ù…Øª Ù…Ø­Ø§ÙØ¸ Ùˆ Ø³Ú¯ Ø±Ø§Ù‡ Ù…ÛŒâ€ŒØ±ÙˆÙ†Ø¯. Ù…Ø­Ø§ÙØ¸ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ù„Ø¨Ø®Ù†Ø¯ Ù…ÛŒâ€ŒØ²Ù†Ø¯.  \n",
      "\"Ø³Ù„Ø§Ù… Ù¾Ø³Ø±Ù‡Ø§ \" Ø§Ùˆ Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯. \"Ø§ÛŒÙ† Ø±Ú©Ø³ Ø§Ø³Øª. Ø§Ùˆ ÛŒÚ© Ø³Ú¯ Ø®ÙˆØ¨ Ø§Ø³Øª. Ø§Ùˆ Ø§Ø² Ø¨Ú†Ù‡ Ù‡Ø§ Ø®ÙˆØ´Ø´ Ù…ÛŒ Ø¢ÛŒØ¯. Ø¢ÛŒØ§ Ù…ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø§Ùˆ Ø±Ø§ Ù†ÙˆØ§Ø²Ø´ Ú©Ù†ÛŒØ¯ØŸ\"\n",
      "\"Ø¨Ù„Ù‡ Ù„Ø·ÙØ§!\" Ø³Ù… Ùˆ Ø¨Ù† Ù…ÛŒâ€ŒÚ¯ÙˆÛŒÙ†Ø¯.  \n",
      "Ø¢Ù†Ù‡Ø§ Ø¯Ø³ØªØ§Ù†Ø´Ø§Ù† Ø±Ø§ Ø¯Ø±Ø§Ø² Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø±Ú©Ø³ Ø¯Ù…Ø´ Ø±Ø§ ØªÚ©Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø§Ùˆ ØµÙˆØ±Øª Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÙ„ÛŒØ³Ø¯. Ø³Ù… Ùˆ Ø¨Ù† Ø®Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.  \n",
      "Ø¢Ù†Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ØªÛŒ Ø±Ú©Ø³ Ø±Ø§ Ù†ÙˆØ§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ù¾Ø´Ù… Ù†Ø±Ù… Ø§Ùˆ Ø±Ø§ Ù„Ù…Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ú†Ø´Ù…Ø§Ù† Ù‚Ù‡ÙˆÙ‡â€ŒØ§ÛŒ Ø§Ùˆ Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¯Ù†Ø¯Ø§Ù†Ù‡Ø§ÛŒ ØªÛŒØ² Ø§Ùˆ Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù†Ø¯.  \n",
      "\"ÙˆØ§Ùˆ Ø§Ùˆ ÙˆØ§Ù‚Ø¹Ø§ Ø®ÙÙ† Ø§Ø³Øª!\" Ø³Ù… Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯.  \n",
      "\"Ù…Ù…Ù†ÙˆÙ† Ú©Ù‡ Ø¨Ù‡ Ù…Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø¯ÛŒØ¯ Ø§Ùˆ Ø±Ø§ Ù†ÙˆØ§Ø²Ø´ Ú©Ù†ÛŒÙ….\" Ø¨Ù† Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯.  \n",
      "\"Ø®ÙˆØ§Ù‡Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù…\" Ù…Ø­Ø§ÙØ¸ Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯. \"Ø±Ú©Ø³ Ø§Ø² Ø´Ù…Ø§ Ø®ÙˆØ´Ø´ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª. Ø´Ù…Ø§ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¤Ø¯Ø¨ Ù‡Ø³ØªÛŒØ¯.\"  \n",
      "Ø³Ù… Ùˆ Ø¨Ù† Ø§Ø² Ù…Ø­Ø§ÙØ¸ Ùˆ Ø±Ú©Ø³ Ø®Ø¯Ø§Ø­Ø§ÙØ¸ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø³Ù…Øª Ø³Ø±Ø³Ø±Ù‡ Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ø¯. Ø¢Ù†Ù‡Ø§ Ø®ÙˆØ´Ø­Ø§Ù„ Ù‡Ø³ØªÙ†Ø¯. Ø¢Ù†Ù‡Ø§ ÛŒÚ© Ø¯ÙˆØ³Øª Ø¬Ø¯ÛŒØ¯ Ø¯Ø§Ø±Ù†Ø¯.  ÛŒÚ© Ø±ÙˆØ²ØŒ Ø¯Ø®ØªØ±Ú©ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù…ÛŒ Ø¨Ù‡ Ø´Ø¯Øª Ù…Ø´ØªØ§Ù‚ Ù¾ÙˆØ´ÛŒØ¯Ù† Ù„Ø¨Ø§Ø³ Ø¬Ø¯ÛŒØ¯Ø´ Ø¨ÙˆØ¯. Ù„Ø¨Ø§Ø³ÛŒ Ø²ÛŒØ¨Ø§ Ø¨Ø§ Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ Ø¨ÙˆØ¯. Ø§Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¯ÙˆØ³ØªØ´ Ø¨Ù† Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯.  \n",
      "Ø§Ù…ÛŒ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ù† Ø±ÙØª Ùˆ Ú¯ÙØª \"Ø¨Ù‡ Ù„Ø¨Ø§Ø³ Ø¬Ø¯ÛŒØ¯Ù… Ù†Ú¯Ø§Ù‡ Ú©Ù† Ø¨Ù†! Ù…Ú¯Ø± Ù†ÛŒØ³Øª Ø²ÛŒØ¨Ø§ØŸ\" Ø¨Ù† Ø¨Ù‡ Ù„Ø¨Ø§Ø³ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ú¯ÙØª \"Ø¨Ù„Ù‡ØŒ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø²ÛŒØ¨Ø§Ø³Øª Ø§Ù…ÛŒ. Ù…Ù† Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒØ´ Ø±Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù….\"  \n",
      "Ø¢Ù†â€ŒÙ‡Ø§ ØªÙ…Ø§Ù… Ø±ÙˆØ² Ø±Ø§ Ø¨ÛŒØ±ÙˆÙ† Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù†Ø¯ØŒ Ø¯ÙˆÛŒØ¯Ù† Ùˆ Ù¾Ø±ÛŒØ¯Ù†. Ø§Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù…ÛŒ Ø§ÙØªØ§Ø¯ Ùˆ Ù„Ø¨Ø§Ø³Ø´ Ú©Ø«ÛŒÙ Ø´Ø¯. Ø§Ùˆ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ú¯Ø±ÛŒÙ‡ Ú©Ø±Ø¯Ù† Ú©Ø±Ø¯. Ø¨Ù† Ú¯ÙØª \"Ù…ØªØ£Ø³ÙÙ… Ø§Ù…ÛŒ. Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³ØªÙ… Ù„Ø¨Ø§Ø³Øª Ú©Ø«ÛŒÙ Ø´ÙˆØ¯.\"  \n",
      "Ø§Ù…ÛŒ Ù¾Ø´ÛŒÙ…Ø§Ù† Ø´Ø¯ Ú©Ù‡ Ø¢Ù†â€ŒÙ‚Ø¯Ø± Ø¨Ø§ Ù„Ø¨Ø§Ø³ Ø¬Ø¯ÛŒØ¯Ø´ Ø´Ø¯ÛŒØ¯ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø§Ùˆ Ú¯ÙØª \"Ù…Ø´Ú©Ù„ÛŒ Ù†ÛŒØ³Øª Ø¨Ù†. Ø¨Ø§Ø± Ø¯ÛŒÚ¯Ø± Ù…Ø±Ø§Ù‚Ø¨ Ø®ÙˆØ§Ù‡Ù… Ø¨ÙˆØ¯ ÙˆÙ‚ØªÛŒ Ù„Ø¨Ø§Ø³ Ø¬Ø¯ÛŒØ¯Ù… Ø±Ø§ Ù…ÛŒâ€ŒÙ¾ÙˆØ´Ù….\"  \n",
      "Ø§Ø² Ø¢Ù† Ø±ÙˆØ² Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø§Ù…ÛŒ ÛŒØ§Ø¯ Ú¯Ø±ÙØª Ú©Ù‡ ÙˆÙ‚ØªÛŒ Ù„Ø¨Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ÛŒØ´ Ø±Ø§ Ù…ÛŒâ€ŒÙ¾ÙˆØ´Ø¯ØŒ Ù…Ø±Ø§Ù‚Ø¨â€ŒØªØ± Ø¨Ø§Ø´Ø¯. Ùˆ Ø§Ùˆ Ùˆ Ø¨Ù† Ù‡Ù…Ú†Ù†Ø§Ù† Ø¨Ø§ Ù‡Ù… Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ Ùˆ Ø®ÙˆØ´ Ù…ÛŒâ€ŒÚ¯Ø°Ø±Ø§Ù†Ø¯Ù†Ø¯.\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([df_train[i] for i in range(20)]))  # to see some data in persian so i could better decide the preprocessing i have to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "import re\n",
    "\n",
    "# Persian char maps\n",
    "ARABIC_TO_PERSIAN = {\n",
    "    \"ÙŠ\": \"ÛŒ\",\n",
    "    \"Ùƒ\": \"Ú©\",\n",
    "    \"Ø©\": \"Ù‡\",\n",
    "    \"Û€\": \"Ù‡\"\n",
    "}\n",
    "\n",
    "DIACRITICS_PATTERN = re.compile(r\"[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]+\")\n",
    "\n",
    "def replace_chars(text, mapping):\n",
    "    for src, target in mapping.items():\n",
    "        text = text.replace(src, target)\n",
    "    return text\n",
    "\n",
    "def normalize_zwnj(text):\n",
    "    # normalize Arabic ZWJ to Persian ZWNJ\n",
    "    text = text.replace(\"\\u200d\", \"\\u200c\")\n",
    "\n",
    "    # common Persian patterns\n",
    "    text = re.sub(r\"\\bÙ…ÛŒ\\s+\", \"Ù…ÛŒâ€Œ\", text)\n",
    "    text = re.sub(r\"\\bÙ†Ù…ÛŒ\\s+\", \"Ù†Ù…ÛŒâ€Œ\", text)\n",
    "\n",
    "    # Plural Ù‡Ø§\n",
    "    text = re.sub(r\"\\s+Ù‡Ø§\\b\", \"â€ŒÙ‡Ø§\", text)\n",
    "\n",
    "    # Possessives\n",
    "    text = re.sub(r\"\\s+(Ø§Ù…|Ø§Øª|Ø§Ø´|Ø§ÛŒÙ…|Ø§ÛŒØ¯|Ø§Ù†Ø¯)\\b\", r\"â€Œ\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    # unify punctuation to Persian-friendly\n",
    "    text = text.replace(\";\", \"Ø›\")\n",
    "    text = text.replace(\",\", \"ØŒ\") if \"ØŒ\" in text else text.replace(\",\", \"ØŒ\")\n",
    "    text = text.replace(\"?\", \"ØŸ\")\n",
    "\n",
    "    # normalize ellipsis\n",
    "    text = text.replace(\"...\", \"â€¦\")\n",
    "\n",
    "    # normalize quotes\n",
    "    text = text.replace(\"â€œ\", \"Â«\").replace(\"â€\", \"Â»\")\n",
    "    text = text.replace(\"\\\"\", \"Â»\")  # fallback for quotes\n",
    "    text = re.sub(r\"[Â«Â»]+\", lambda m: \"Â«\" if m.group()==m.group()[0] else \"Â»\", text)\n",
    "\n",
    "    # remove spaces before punctuation\n",
    "    text = re.sub(r\"\\s+([ØŒØ›:!ØŸ.])\", r\"\\1\", text)\n",
    "\n",
    "    # enforce one space after punctuation\n",
    "    text = re.sub(r\"([ØŒØ›:!ØŸ.])([^ \\nÂ»])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_digits(text):\n",
    "    # convert English digits 0-9 â†’ Persian digits\n",
    "    return text.translate(str.maketrans(\"0123456789\", \"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹\"))\n",
    "\n",
    "def cleanup_whitespace(text):\n",
    "    # collapse multiple spaces + trim\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    return DIACRITICS_PATTERN.sub(\"\", text)\n",
    "\n",
    "def preprocess_farsi(\n",
    "    text: str,\n",
    "    *,\n",
    "    normalize_chars=True,\n",
    "    normalize_zwnj_flag=True,\n",
    "    normalize_punc=True,\n",
    "    unify_digits=True,\n",
    "    strip_diacritics=True,\n",
    "    clean_space=True,\n",
    "):\n",
    "    if normalize_chars:\n",
    "        text = replace_chars(text, ARABIC_TO_PERSIAN)\n",
    "\n",
    "    if strip_diacritics:\n",
    "        text = remove_diacritics(text)\n",
    "\n",
    "    if normalize_zwnj_flag:\n",
    "        text = normalize_zwnj(text)\n",
    "\n",
    "    if normalize_punc:\n",
    "        text = normalize_punctuation(text)\n",
    "\n",
    "    if unify_digits:\n",
    "        text = normalize_digits(text)\n",
    "\n",
    "    if clean_space:\n",
    "        text = cleanup_whitespace(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We did the Preprocessing on both the validation and train set.\n",
      "0    ÛŒÚ©â€ŒØ±ÙˆØ² ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ø§Ø³Ù… Ø¨Ù† Ø¨ÙˆØ¯. Ø¨Ù† Ø¯ÙˆØ³Øª...\n",
      "1    ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ ÛŒÚ© Ø³Ù…ÙˆØ± Ø¢Ø¨ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´...\n",
      "2    ÛŒÚ© Ø±ÙˆØ² Ù¾Ø³Ø± Ú©ÙˆÚ†Ú©ÛŒ Ø¨Ù‡ Ø§Ø³Ù… ØªÛŒÙ… Ø¨Ù‡ Ù¾Ø§Ø±Ú© Ø±ÙØª. Ø§Ùˆ ÛŒÚ©...\n",
      "3    ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ù…Ù‡Ø±Ø¨Ø§Ù† Ø¨Ù‡ Ù†Ø§Ù… Ø¨Ø§Ø¨ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©...\n",
      "4    ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ Ø¯Ø± ÛŒÚ© Ø®Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú©ØŒ ÛŒÚ© Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ù†Ø§...\n",
      "Name: Persian, dtype: object\n",
      "0    ØªØ±Ø¬Ù…Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ: Ù…Ø¬Ø¨ÙˆØ± Ù†ÛŒØ³ØªÛŒ Ø§Ø² Ø³Ú¯ Ø¨Ù„Ù†Ø¯ ØµØ¯Ø§ ØªØ±Ø³...\n",
      "1    ÛŒÚ© Ø±ÙˆØ² Ø¯Ø± ÛŒÚ© Ø¬Ø§ÛŒ Ú¯Ø±Ù… Ùˆ Ø¢ÙØªØ§Ø¨ÛŒØŒ ÛŒÚ© Ú†Ø§Ù„Ù‡ Ø¨Ø²Ø±Ú¯ ÙˆØ¬...\n",
      "2    Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ Ø¯Ø®ØªØ± Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÙˆØ³ÛŒ Ø¨Ùˆ...\n",
      "3    Ø¨Ø§ ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ: ÛŒÚ© ØµØ¨Ø­ØŒ Ú¯Ø±Ø¨Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ù†Ø§Ù… ...\n",
      "4    ÛŒÚ© Ø±ÙˆØ² ÛŒÚ© Ù¾Ø§Ø¯Ø´Ø§Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ Ù‚ÙˆÛŒ Ú©Ù‡ Ø¨Ø± Ù¾Ø§Ø¯Ø´Ø§Ù‡ÛŒâ€ŒØ§Ø´ Ø­...\n",
      "Name: Persian, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train_preprocessed = df_train.apply(preprocess_farsi)\n",
    "\n",
    "df_val_preprocessed = df_val.apply(preprocess_farsi)\n",
    "\n",
    "print(\"We did the Preprocessing on both the validation and train set.\")\n",
    "print(df_train_preprocessed.head())\n",
    "print(df_val_preprocessed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE merges: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [24:33<00:00,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer(num_merges=10000)\n",
    "\n",
    "train_words = []\n",
    "for text in df_train_preprocessed:\n",
    "    train_words.extend(text.split())\n",
    "\n",
    "tokenizer.train(train_words)\n",
    "print(\"Tokenizer training done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø±ÙˆØ²',\n",
       " 'ÛŒ</w>',\n",
       " 'ÛŒÚ©',\n",
       " '</w>',\n",
       " 'Ù…Ø±Ø¯</w>',\n",
       " 'Ø«Ø±Ùˆ',\n",
       " 'ØªÙ…',\n",
       " 'Ù†Ø¯',\n",
       " 'ØŒ</w>',\n",
       " 'Ù¾Ø³Ø±',\n",
       " '</w>',\n",
       " 'Ø¨Ú†',\n",
       " 'Ù‡</w>',\n",
       " 'Ú©ÙˆÚ†Ú©',\n",
       " 'Ø´</w>',\n",
       " 'Ø±Ø§',\n",
       " '</w>',\n",
       " 'Ø¨',\n",
       " 'Ù€',\n",
       " 'Ù‡</w>',\n",
       " 'Ø¯Ù‡',\n",
       " '</w>',\n",
       " 'Ø¨Ø±Ø¯</w>',\n",
       " 'ØªØ§',\n",
       " '</w>',\n",
       " 'Ø¨',\n",
       " 'Ù€',\n",
       " 'Ù‡</w>',\n",
       " 'Ø§Ùˆ',\n",
       " '</w>',\n",
       " 'Ù†Ø´',\n",
       " 'Ø§Ù†',\n",
       " '</w>',\n",
       " 'Ø¯Ù‡Ø¯</w>',\n",
       " 'Ù…Ø±',\n",
       " 'Ø¯Ù…ÛŒ',\n",
       " '</w>',\n",
       " 'Ú©Ù‡',\n",
       " '</w>',\n",
       " 'Ø¯Ø±',\n",
       " '</w>',\n",
       " 'Ø¢Ù†',\n",
       " 'Ø¬Ø§',\n",
       " '</w>',\n",
       " 'Ø²Ù†',\n",
       " 'Ø¯',\n",
       " 'Ú¯ÛŒ',\n",
       " '</w>',\n",
       " 'Ù…ÛŒ\\u200cÚ©Ù†',\n",
       " 'Ù†Ø¯',\n",
       " 'ØŒ</w>',\n",
       " 'Ú†',\n",
       " 'Ù‚Ø¯',\n",
       " 'Ø±</w>',\n",
       " 'ÙÙ‚',\n",
       " 'ÛŒØ±</w>',\n",
       " 'Ù‡Ø³Øª',\n",
       " 'Ù†Ø¯',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmaple = \"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "\n",
    "tokenizer.tokenize(exmaple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124411/124411 [3:52:27<00:00,  8.92it/s]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27630/27630 [51:14<00:00,  8.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization done!\n",
      "0    [ÛŒÚ©, â€Œ, Ø±ÙˆØ², </w>, ÛŒÚ©, </w>, Ù¾Ø³Ø±, Ø¨Ú†, Ù‡</w>, Ú©...\n",
      "1    [ÛŒÚ©, </w>, Ø±ÙˆØ², ÛŒØŒ</w>, ÛŒÚ©, </w>, Ø³, Ù…Ùˆ, Ø±</w>...\n",
      "2    [ÛŒÚ©, </w>, Ø±ÙˆØ², </w>, Ù¾Ø³Ø±, </w>, Ú©ÙˆÚ†Ú©, ÛŒ</w>, ...\n",
      "Name: Persian, dtype: object\n",
      "0    [ØªØ±Ø¬Ù…, Ù‡</w>, Ø¨Ù‡, </w>, ÙØ§, Ø±Ø³ÛŒ, :</w>, Ù…Ø¬, Ø¨Ùˆ...\n",
      "1    [ÛŒÚ©, </w>, Ø±ÙˆØ², </w>, Ø¯Ø±, </w>, ÛŒÚ©, </w>, Ø¬Ø§, ...\n",
      "2    [Ø¨Ø§, Ø±ÛŒ, </w>, Ø±ÙˆØ², ÛŒ</w>, Ø±ÙˆØ²Ú¯, Ø§Ø±, ÛŒ</w>, Ø¯Ø®...\n",
      "Name: Persian, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We are now doing the tokenization of the rows in the preprocessed dataframe\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_train_tokenized = df_train_preprocessed.progress_apply(lambda s: tokenizer.tokenize(s))\n",
    "df_val_tokenized = df_val_preprocessed.progress_apply(lambda s: tokenizer.tokenize(s))\n",
    "\n",
    "print(\"Tokenization done!\")\n",
    "\n",
    "print(df_train_tokenized.head(3))\n",
    "print(df_val_tokenized.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "Ú†Ù†Ø¯ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù… Ù…Ø®ØµÙˆØµØ§ Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª Ùˆ Ø¨Ø§ Ø¹Ø±Ø¨ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ±Ú©ÛŒØ¨ Ø´ÙˆØ¯ Ùˆ Ø¨Ù‡ Ø¯Ù„Ø§ÛŒÙ„ Ù†ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ù‡Ø§ Ùˆ Ø®Ø§ØµÛŒØª Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ù†Ø¬ÙˆÙ‡ Ù†Ú¯Ø§Ø±Ø´ Ø¢Ù† Ø®ÛŒÙ„ÛŒ Ù…Ø±Ø­Ù„Ù‡ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù‡Ù…ÛŒØª ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ø¯Ø§Ø±Ø¯<br>\n",
    "Ø¨Ù‡ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ Ø¯Ø± Pre-Process Ø­ØªÙ…Ø§ Ø¨Ø§ÛŒØ¯ Ø±Ø¹Ø§ÛŒØª Ú©Ù†Ù… Ø§Ø´Ø§Ø±Ù‡ Ù…ÛŒÚ©Ù†Ù…:<br>\n",
    "1. Ù†ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ù‡Ø§ Ø¯Ø± ÙØ§Ø±Ø³ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù‡Ù…Ù‡ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ú©Ù†Ù… Ùˆ Ø¬Ø§Ù‡Ø§ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù†ÛŒÙ… ÙØ§ØµÙ„Ù‡ Ø±Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± Ù…Ø®ØµÙˆØµ Ø®ÙˆØ¯ Ù¾Ø± Ú©Ù†Ù….<br>\n",
    "2. Ø¨Ø±Ø®ÛŒ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÛŒ Ù…Ø§Ù†Ù†Ø¯ ÛŒ Ú©  Ùˆ Ú†Ù†Ø¯ Ø­Ø±Ù Ø¯ÛŒÚ¯Ø± Ú©Ù‡ Ø¹Ø±Ø¨ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ø§Ù…Ø§ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ù†Ø¯.<br>\n",
    "3. Ø¨Ø§ÛŒØ¯ Ø­Ø±ÙˆÙ ØµØ¯Ø§Ø¯Ø§Ø± Ø¨Ø§Ù„Ø§ÛŒ Ú©Ù„Ø§Ù…Øª Ø±Ø§ Ø­Ø°Ù Ú©Ù†Ù… Ø²ÛŒØ±Ø§ Ø¨Ø§Ø² ÙØ§Ø±Ø³ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø¹Ø±Ø¨ÛŒ Ù†Ø¨Ø§ÛŒØ¯ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø¢Ù† Ø¨Ø§Ø´Ø¯ Ùˆ Ú†ÙˆÙ† Ø¯Ø± Ø§Ú©Ø«Ø± Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ Ù…Ø§ Ø­Ø±ÙˆÙ ØµØ¯Ø§Ø¯Ø§Ø± Ø±Ø§ Ù†Ø¯Ø§Ø±ÛŒÙ….<br>\n",
    "4. Ø´Ù…Ø§Ø±Ù‡ Ù‡Ø§ÛŒ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ø±Ù…Ø§Ù„ Ú©Ù†Ù…. Ø§Ú¯Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ùˆ ÛŒØ§ Ø¹Ø±Ø¨ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯ Ø­Ø°Ù Ú©Ù†Ù….<br>\n",
    "5. Ø¨Ø±Ø®ÛŒ Ø¹Ù„Ø§Ø¹Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ù†Ø³ÛŒØ³ØªÙ†Øª Ú©Ù†Ù… Ùˆ Ø¹Ù…Ù„Ø§ Ú†Ù†Ø¯ ØªØ§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù†ÙˆØ¹ ÙˆØ§Ø­Ø¯ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ù…<br>\n",
    "6. Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù‡Ø§ Ø±Ø§ Ù‡Ù†Ø¯Ù„ Ú©Ù†ÛŒÙ….<br>\n",
    "7. Ø¨Ø§ÛŒØ¯ <<>> Ùˆ \"\" Ø±Ø§ ÛŒÚ©ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒÙ… <br>\n",
    "8. Ø§ÛŒØ§ Ú©Ù„Ù…Ø§Øª ØºÛŒØ± Ø±Ø³Ù…ÛŒ Ø±Ø§ Ú©Ù†Ø§Ø± Ú©Ù„Ù…Ø§Øª Ø±Ø³Ù…ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒÙ… ÛŒØ§ Ø®ÛŒØ± Ú©Ù‡ Ø¨Ù†Ø¸Ø± Ø·Ø¨Ù‚ Ø§ÛŒÙ†Ú©Ù‡ Ø¯ÛŒØªØ§ Ø¯Ø§Ø³ØªØ§Ù† Ø§Ø³Øª Ø¨Ù‡ØªØ± Ø§Ø³Øª Ù†Ú¯Ù‡ Ø¯Ø§Ø±Ù….<br>\n",
    "9. Ù…ÙˆØ±Ø¯ Ø¯ÛŒÚ¯Ø± Ù‡Ù… Ø±Ø§Ø¬Ø¨ Ú†Ù†Ø¯ ØªØ§ Ø¹Ù„Ø§Ù…Øª ØªØ¹Ø¬Ø¨ ÛŒØ§ Ø³ÙˆØ§Ù„ Ù¾Ø´Øª Ù‡Ù… Ø¨ÛŒØ§ÛŒÙ†Ø¯. Ù…Ø¹Ù…ÙˆÙ„Ø§ Ù…Ø§ Ø¨Ù‡ØªØ± Ø§Ø³Øª ÛŒÚ©ÛŒ Ø¢Ù† Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒÙ….\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ N-gram</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ N-gram Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ù‚Ø¨Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ \n",
    "<span dir=\"ltr\"> 2-gram, 4-gram, 8-gram</span>\n",
    "Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ 100 ØªÙˆÚ©Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ùˆ Ú©ÛŒÙÛŒØª Ù…ØªÙˆÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ Ù‡Ù… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªÙØ§ÙˆØª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² Ù†Ø¸Ø± Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ Ùˆ Ø±ÙˆØ§Ù†ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ N-gram Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§ÙØªÙ‡\n",
    "<br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ 100 ØªÙˆÚ©Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "# The smoothing is only implemented for the N=4 \n",
    "\n",
    "class NGramLM:\n",
    "\n",
    "    LAMBDA_BACKOFF = 0.4 \n",
    "    INTERPOLATION_WEIGHTS = (0.4, 0.3, 0.2, 0.1) \n",
    "    \n",
    "    NONE_SMOOTHING = 'none'\n",
    "    LAPLACE = 'laplace'\n",
    "    BACKOFF = \"backoff\"\n",
    "    INTERPOLATION = \"interpolation\"\n",
    "    \n",
    "    def __init__(self, n: int = 4, smoothing: str = \"none\"):\n",
    "        \"\"\"Initialize N-gram model with order n.\"\"\"\n",
    "        self.n = n  # the order of the LM (bigram, trigram, ...)\n",
    "        self.smoothing = smoothing\n",
    "        self.vocab = set()\n",
    "        \n",
    "        # Count dictionaries\n",
    "        self.ngram_counts = {}\n",
    "        self.context_counts = {}\n",
    "\n",
    "        # Probability dictionary\n",
    "        self.probabilities = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.bos_token = \"<bos>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "\n",
    "    def build_vocab(self, tokenized_texts):\n",
    "        \"\"\"Populate vocabulary set from tokenized train data.\"\"\"\n",
    "        for sentence in tokenized_texts:\n",
    "            for token in sentence:\n",
    "                self.vocab.add(token)\n",
    "        # ensure special tokens are present\n",
    "        self.vocab.update([self.bos_token, self.eos_token, self.unk_token])\n",
    "        print(f\"Vocabulary size: {len(self.vocab)} tokens\")\n",
    "        return self.vocab\n",
    "        # Add (nâˆ’1) BOS tokens at the start and one EOS at the end\n",
    "\n",
    "    def count_ngrams(self, tokenized_texts):\n",
    "        \"\"\"Count all N-grams and (Nâˆ’1)-gram contexts for any n.\"\"\"\n",
    "        for sentence in tokenized_texts:\n",
    "            # Add (nâˆ’1) BOS and one EOS token\n",
    "            sentence = [self.bos_token] * (self.n - 1) + sentence + [self.eos_token]\n",
    "            \n",
    "            # Slide a window of size n across the sentence\n",
    "            for i in range(len(sentence) - self.n + 1):\n",
    "                ngram = tuple(sentence[i:i + self.n])\n",
    "                context = tuple(sentence[i:i + self.n - 1])\n",
    "\n",
    "                # Update n-gram counts\n",
    "                self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1\n",
    "\n",
    "                # Update context counts\n",
    "                self.context_counts[context] = self.context_counts.get(context, 0) + 1\n",
    "\n",
    "    def calculate_probabilities(self):\n",
    "        \"\"\"Convert counts to probabilities.\n",
    "        - If n != 4 and smoothing != 'none', fall back to 'none'.\n",
    "        - For n == 4:\n",
    "            * laplace:    (c4 + 1) / (C3 + |V|)\n",
    "            * interpolation: 0.4*P4 + 0.3*P3 + 0.2*P2 + 0.1*P1\n",
    "            * backoff: P4 else 0.4*P3 else 0.4**2*P2 else 0.4**3*P1\n",
    "        \"\"\"\n",
    "        # reset before recomputing\n",
    "        self.probabilities = {}\n",
    "\n",
    "        use_plain = (self.smoothing == \"none\") or (self.n != 4)\n",
    "        if use_plain:\n",
    "            probs = self.probabilities\n",
    "            ctx = self.context_counts\n",
    "            for ngram, c in self.ngram_counts.items():\n",
    "                C = ctx.get(ngram[:-1], 0)\n",
    "                probs[ngram] = (c / C) if C > 0 else 0.0\n",
    "            return probs\n",
    "\n",
    "        # smoothing != none and n == 4\n",
    "        # 1) aggregate lower orders in one pass\n",
    "        tri_counts, tri_ctx = {}, {}\n",
    "        bi_counts, bi_ctx = {}, {}\n",
    "        uni_counts = {}\n",
    "        total_unigrams = 0\n",
    "\n",
    "        for (h3, h2, h1, w), c4 in self.ngram_counts.items():\n",
    "            k3 = (h2, h1, w); tri_counts[k3] = tri_counts.get(k3, 0) + c4\n",
    "            c3k = (h2, h1);   tri_ctx[c3k]     = tri_ctx.get(c3k, 0) + c4\n",
    "\n",
    "            k2 = (h1, w);     bi_counts[k2]    = bi_counts.get(k2, 0) + c4\n",
    "            c2k = (h1,);      bi_ctx[c2k]      = bi_ctx.get(c2k, 0) + c4\n",
    "\n",
    "            uni_counts[w]     = uni_counts.get(w, 0) + c4\n",
    "            total_unigrams   += c4\n",
    "\n",
    "        # 2) precompute MLE tables P4,P3,P2,P1 (dicts)\n",
    "        P4 = {}\n",
    "        P3 = {}\n",
    "        P2 = {}\n",
    "        P1 = {}\n",
    "\n",
    "        ctx4 = self.context_counts\n",
    "        for (h3, h2, h1, w), c4 in self.ngram_counts.items():\n",
    "            C3 = ctx4.get((h3, h2, h1), 0)\n",
    "            P4[(h3, h2, h1, w)] = (c4 / C3) if C3 > 0 else 0.0\n",
    "\n",
    "        for (h2, h1, w), c3 in tri_counts.items():\n",
    "            C2 = tri_ctx.get((h2, h1), 0)\n",
    "            P3[(h2, h1, w)] = (c3 / C2) if C2 > 0 else 0.0\n",
    "\n",
    "        for (h1, w), c2 in bi_counts.items():\n",
    "            C1 = bi_ctx.get((h1,), 0)\n",
    "            P2[(h1, w)] = (c2 / C1) if C1 > 0 else 0.0\n",
    "\n",
    "        inv_total = (1.0 / total_unigrams) if total_unigrams > 0 else 0.0\n",
    "        if inv_total:\n",
    "            for w, c1 in uni_counts.items():\n",
    "                P1[w] = c1 * inv_total\n",
    "        else:\n",
    "            # keep empty -> zeros\n",
    "            pass\n",
    "\n",
    "        probs = self.probabilities\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        if self.smoothing == NGramLM.LAPLACE:\n",
    "            # only 4-gram add-1 as requested\n",
    "            for (h3, h2, h1, w), c4 in self.ngram_counts.items():\n",
    "                C3 = ctx4.get((h3, h2, h1), 0)\n",
    "                probs[(h3, h2, h1, w)] = ((c4 + 1) / (C3 + vocab_size)) if C3 > 0 else (1 / max(vocab_size, 1))\n",
    "            return probs\n",
    "\n",
    "        if self.smoothing == NGramLM.INTERPOLATION:\n",
    "            # weights fixed: 0.4, 0.3, 0.2, 0.1\n",
    "            for (h3, h2, h1, w) in self.ngram_counts.keys():\n",
    "                p = (self.INTERPOLATION_WEIGHTS[0] * P4.get((h3, h2, h1, w), 0.0) +\n",
    "                    self.INTERPOLATION_WEIGHTS[1] * P3.get((h2, h1, w), 0.0) +\n",
    "                    self.INTERPOLATION_WEIGHTS[2] * P2.get((h1, w), 0.0) +\n",
    "                    self.INTERPOLATION_WEIGHTS[3] * P1.get(w, 0.0))\n",
    "                probs[(h3, h2, h1, w)] = p\n",
    "            return probs\n",
    "\n",
    "        if self.smoothing == NGramLM.BACKOFF:\n",
    "            # geometric factor 0.4 per backoff step\n",
    "            for (h3, h2, h1, w) in self.ngram_counts.keys():\n",
    "                p4 = P4.get((h3, h2, h1, w), 0.0)\n",
    "                if p4 > 0.0:\n",
    "                    probs[(h3, h2, h1, w)] = p4\n",
    "                    continue\n",
    "                p3 = P3.get((h2, h1, w), 0.0)\n",
    "                if p3 > 0.0:\n",
    "                    probs[(h3, h2, h1, w)] = NGramLM.LAMBDA_BACKOFF * p3\n",
    "                    continue\n",
    "                p2 = P2.get((h1, w), 0.0)\n",
    "                if p2 > 0.0:\n",
    "                    probs[(h3, h2, h1, w)] = (NGramLM.LAMBDA_BACKOFF ** 2) * p2\n",
    "                    continue\n",
    "                p1 = P1.get(w, 0.0)\n",
    "                probs[(h3, h2, h1, w)] = (NGramLM.LAMBDA_BACKOFF ** 3) * p1\n",
    "            return probs\n",
    "\n",
    "        raise ValueError(f\"Smoothing method '{self.smoothing}' is not recognized.\")\n",
    "\n",
    "\n",
    "    def add_special_tokens(self, tokens):\n",
    "        \"\"\"Insert (nâˆ’1) BOS tokens and one EOS token around a tokenized sentence.\"\"\"\n",
    "        self.vocab.update([self.bos_token, self.eos_token])\n",
    "        \n",
    "        return [self.bos_token] * (self.n - 1) + tokens + [self.eos_token]\n",
    "\n",
    "    def predict_next(self, context, use_sampling_weighted=False, temperature: float = 1.0):\n",
    "        \"\"\"Predict the next token given the last N-1 tokens.\"\"\"\n",
    "\n",
    "        if not isinstance(context, tuple):\n",
    "            context = tuple(context)\n",
    "        context = context[-(self.n - 1):]\n",
    "\n",
    "        # gather candidates that match this exact context at order n\n",
    "        candidates = {\n",
    "            ngram[-1]: prob\n",
    "            for ngram, prob in self.probabilities.items()\n",
    "            if ngram[:-1] == context\n",
    "        }\n",
    "\n",
    "        if not candidates:\n",
    "            return self.unk_token\n",
    "\n",
    "        # deterministic path (argmax)    \n",
    "        if not use_sampling_weighted:\n",
    "            return max(candidates, key=candidates.get)\n",
    "\n",
    "        # sampling path with temperature\n",
    "        tokens, probs = zip(*candidates.items())\n",
    "        probs = [p if p > 0.0 else 1e-9 for p in probs]\n",
    "\n",
    "        if temperature is None or temperature <= 0:\n",
    "            # fall back to greedy if bad temperature\n",
    "            return max(candidates, key=candidates.get)\n",
    "\n",
    "        scaled = [p ** (1.0 / temperature) for p in probs]\n",
    "        total = sum(scaled)\n",
    "        scaled = [p / total for p in scaled]\n",
    "\n",
    "        return random.choices(tokens, scaled)[0]\n",
    "    \n",
    "    def detokenize_bpe(self, tokens):\n",
    "        words = []\n",
    "        current = []\n",
    "        for t in tokens:\n",
    "            if t == \"</w>\":\n",
    "                if current:\n",
    "                    words.append(\"\".join(current))\n",
    "                    current = []\n",
    "            elif t in (self.bos_token, self.eos_token):\n",
    "                continue\n",
    "            else:\n",
    "                current.append(t)\n",
    "        if current:\n",
    "            words.append(\"\".join(current))\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "    def generate(self, length=30, max_in_len=True, use_sampling_weighted=False, temperature: float = 1.0):\n",
    "        context = [self.bos_token] * (self.n - 1)\n",
    "        generated = []\n",
    "\n",
    "        for _ in range(length):\n",
    "            next_token = self.predict_next(\n",
    "                context,\n",
    "                use_sampling_weighted=use_sampling_weighted,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "\n",
    "            if next_token == self.eos_token and max_in_len:\n",
    "                break\n",
    "\n",
    "            generated.append(next_token)\n",
    "            context = (context + [next_token])[-(self.n - 1):]\n",
    "\n",
    "        # if output looks like BPE â†’ detokenize, else just join\n",
    "        if any(tok == \"</w>\" for tok in generated):\n",
    "            return self.detokenize_bpe(generated)\n",
    "        return \" \".join(tok for tok in generated if tok not in (self.bos_token, self.eos_token))\n",
    "\n",
    "\n",
    "\n",
    "    def get_ngram_probability(self, ngram: tuple):\n",
    "        \"\"\"Return the probability of an N-gram.\n",
    "\n",
    "        Prefers precomputed probabilities (from calculate_probabilities).\n",
    "        Falls back to MLE from counts if available. Otherwise returns 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        # Coerce to tuple and validate length\n",
    "        if not isinstance(ngram, tuple):\n",
    "            ngram = tuple(ngram)\n",
    "        if len(ngram) != self.n:\n",
    "            # For now we only support exact order-n queries\n",
    "            return 0.0\n",
    "\n",
    "        # 1) If precomputed, return it\n",
    "        if ngram in self.probabilities:\n",
    "            return self.probabilities[ngram]\n",
    "\n",
    "        # 2) Fallback: compute MLE from counts if we can\n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        if count == 0:\n",
    "            return 0.0\n",
    "\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return count / context_count\n",
    "\n",
    "    def perplexity(self, tokenized_texts):\n",
    "        \"\"\"Compute perplexity on tokenized sentences (order-n model).\"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_predicted = 0\n",
    "\n",
    "        for sentence in tokenized_texts:\n",
    "            # pad with (n-1) BOS and one EOS\n",
    "            sent = [self.bos_token] * (self.n - 1) + sentence + [self.eos_token]\n",
    "\n",
    "            # slide an n-gram window over the sentence\n",
    "            for i in range(self.n - 1, len(sent)):\n",
    "                ngram = tuple(sent[i - self.n + 1 : i + 1])\n",
    "\n",
    "                p = self.get_ngram_probability(ngram)\n",
    "                if p <= 0.0:\n",
    "                    return float(\"inf\")  # no smoothing â†’ zero prob â†’ infinite perplexity\n",
    "\n",
    "                total_log_prob += math.log(p)\n",
    "                total_predicted += 1\n",
    "\n",
    "        if total_predicted == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        # cross-entropy (base e), then perplexity\n",
    "        avg_neg_log = - total_log_prob / total_predicted\n",
    "        return math.exp(avg_neg_log)\n",
    "    \n",
    "    def _iter_tokenized(self, tokenized_texts, col: str | None = None):\n",
    "        \"\"\"Internal: yield token lists from iterable/Series/DataFrame.\"\"\"\n",
    "        # pandas Series of token lists\n",
    "        try:\n",
    "            import pandas as pd  # local import to avoid hard dependency at import time\n",
    "            is_series = isinstance(tokenized_texts, pd.Series)\n",
    "            is_df = isinstance(tokenized_texts, pd.DataFrame)\n",
    "        except Exception:\n",
    "            is_series = False\n",
    "            is_df = False\n",
    "\n",
    "        if is_series:\n",
    "            for row in tokenized_texts:\n",
    "                yield row\n",
    "            return\n",
    "\n",
    "        if is_df:\n",
    "            if not col:\n",
    "                raise ValueError(\"When passing a DataFrame, you must provide the 'col' name containing token lists.\")\n",
    "            for row in tokenized_texts[col]:\n",
    "                yield row\n",
    "            return\n",
    "\n",
    "        # Generic iterable of token lists\n",
    "        for row in tokenized_texts:\n",
    "            yield row\n",
    "    \n",
    "    def train(self, tokenized_texts, *, col: str | None = None):\n",
    "        \"\"\"One-shot training pipeline.\"\"\"\n",
    "        iterator = list(self._iter_tokenized(tokenized_texts, col=col))\n",
    "\n",
    "        self.build_vocab(iterator)\n",
    "        self.count_ngrams(iterator)\n",
    "        self.calculate_probabilities()\n",
    "        return self\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model parameters for later use.\"\"\"\n",
    "        dirpath = os.path.dirname(path)\n",
    "        if dirpath:\n",
    "            os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "        model_data = {\n",
    "            \"n\": self.n,\n",
    "            \"vocab\": list(self.vocab),\n",
    "            \"ngram_counts\": self.ngram_counts,\n",
    "            \"context_counts\": self.context_counts,\n",
    "            \"probabilities\": self.probabilities,\n",
    "        }\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved successfully to {path}\")\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load saved model parameters.\"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.n = model_data[\"n\"]\n",
    "        self.vocab = set(model_data[\"vocab\"])\n",
    "        self.ngram_counts = model_data[\"ngram_counts\"]\n",
    "        self.context_counts = model_data[\"context_counts\"]\n",
    "        self.probabilities = model_data[\"probabilities\"]\n",
    "\n",
    "        print(f\"Model loaded successfully from {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3246 tokens\n",
      "Model saved successfully to .\\saved_models\\lm_2gram_model.pkl\n",
      "\n",
      "Generated text (2-gram model):\n",
      "\n",
      "Ø¨Ø§Ù‚Ù„Ú©Ø®ÙˆØ§Ù† Ø¨Ø±Ø§ÛŒ Ú©ÙˆÚ†Ú© Ú©Ù‡ ÙÙ‚Ø·Ø¨Ø§ Ø§Ø³Ø¨Ø§Ø¨Ø¨Ø§Ø²ÛŒØ²ÛŒ Ø§Ø² Ø¨Ù¾Ø²Ø¯.Ø¢Ù†â€ŒØ®ÙˆØ±Ú†Ù‡ Ø¨Ø²Ø±Ú¯ØŒÙ¾Ø³Ø± Ø±ÙˆØ²ØŒØ¢Ù†â€ŒÙ‡Ø§\n",
      "Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±ÙˆÛŒÙ…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ ØªØ´ Ù†Ù…ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ÙˆÙ‚ØªÛŒÙ…Ø®ÛŒÙ„ÛŒ ØªØ§Ù…Ø²Ø¯Ù†Ø¢ÙØª Ø´Ø¯.Ù†Ø§Ú¯Ù‡Ø§Ù†Ù‡â€ŒØ¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§\n",
      "Ù†Ú¯Ù‡Ù†Ø¯.Ù„ÛŒÙ„ÛŒ ÙˆØ¨Ù‡â€ŒÚ¯Ø±Ø¯Ø§Ù† Ø¯ÛŒØ¯ Ø¯Ø± Ú©Ù…Ø¯Ø¯ÙˆØ³ØªØ´ØŒÙ¾Ø±Ù†\n"
     ]
    }
   ],
   "source": [
    "# 2Gram\n",
    "lm_2gram = NGramLM(n=2)\n",
    "\n",
    "lm_2gram.train(df_train_tokenized)\n",
    "\n",
    "save_2gram_path = os.path.join(\".\", \"saved_models\", \"lm_2gram_model.pkl\")\n",
    "lm_2gram.save(save_2gram_path)\n",
    "generated_text = lm_2gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\nGenerated text (2-gram model):\\n\")\n",
    "print(textwrap.fill(clean, width=80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3246 tokens\n",
      "Model saved successfully to .\\saved_models\\lm_4gram_model.pkl\n",
      "\n",
      "Generated text (4-gram model):\n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ² Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯.\n"
     ]
    }
   ],
   "source": [
    "# 4Gram\n",
    "lm_4gram = NGramLM(n=4)\n",
    "\n",
    "lm_4gram.train(df_train_tokenized)\n",
    "\n",
    "save_4gram_path = os.path.join(\".\", \"saved_models\", \"lm_4gram_model.pkl\")\n",
    "lm_4gram.save(save_4gram_path)                                           \n",
    "generated_text = lm_4gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\nGenerated text (4-gram model):\\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3246 tokens\n",
      "Model saved successfully to .\\saved_models\\lm_8gram_model.pkl\n",
      "\n",
      "Generated text (8-gram model):\n",
      "\n",
      "Ø¨Ø§Ø±ÛŒØŒØ±ÙˆØ²ÛŒØ±ÙˆØ²Ú¯Ø§Ø±ÛŒØ®Ø§Ù†Ù…ÛŒ Ø¨Ø³ÛŒØ§Ø±Ù…Ø´ØªØ§Ù‚Ø¨Ø§Ø²ÛŒØ¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´Ø±ÙØª.Ø¢Ù†â€ŒÙ‡Ø§ Ù‡Ù…Ù‡â€ŒÛŒÚ©â€ŒØ¯Ø§ÛŒØ±Ù‡ Ø¨Ø²Ø±Ú¯\n",
      "ÙˆØ¢Ø¨ÛŒØ¯Ø§Ø´Øª.ØªØ§Ù…Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø§ Ø§Ø³Ø¨Ø§Ø¨Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒØ´Ø§Ù†Ø±Ø§ Ø¨Ù‡ Ø¬Ø¹Ø¨Ù‡\n",
      "Ø§Ø³Ø¨Ø§Ø¨Ø¨Ø§Ø²ÛŒÙ‡Ø§ÛŒØ´Ø§Ù†Ø¨Ø§Ø²ÛŒÚ©Ù†Ù†Ø¯.Ø¢Ù†â€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ Ù†Ø§Ø±Ø§Ø­ØªØ´Ø¯Ù†Ø¯.Ø§Ù…Ø§Ù†Ø§Ú¯Ù‡Ø§Ù† Ø§Ø³Ù¾Ø§ÛŒÚ© Ø¨Ù‡\n"
     ]
    }
   ],
   "source": [
    "# 8Gram\n",
    "lm_8gram = NGramLM(n=8)\n",
    "\n",
    "lm_8gram.train(df_train_tokenized)\n",
    "\n",
    "save_8gram_path = os.path.join(\".\", \"saved_models\", \"lm_8gram_model.pkl\")\n",
    "lm_8gram.save(save_8gram_path)\n",
    "generated_text = lm_8gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\nGenerated text (8-gram model):\\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "Ø¯Ø± 2gram Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¨ÛŒ Ù…Ø§ Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ø§ÛŒ Ù‡Ø§ÛŒ Ù¾Ø´Øª Ù‡Ù… Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ ØªØ§ Ø­Ø¯ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø§Ù…Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ù„ÛŒ Ú©Ø§Ù†ØªÚ©Ø³Øª Ø¬Ù…Ù„Ù‡ Ùˆ \n",
    "Ú¯Ø±Ø§Ù…Ø± Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ú©Ù„ÛŒ Ùˆ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª.<br>\n",
    "Ø¯Ø± Ø­Ø§Ù„Øª 8gram Ù‡Ù… Ø¬Ù…Ù„Ù‡ ÛŒØ§ Ø²ÙˆØ¯ ØªÙ…Ø§Ù… Ù…ÛŒØ´ÙˆØ¯ ÛŒØ§ Ø¨Ø¹Ù„Øª Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù† ØªÙ…Ø§Ù…ÛŒ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† generate Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯. Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ù‡Ù… Ø¬Ù…Ù„Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø§Ø®Ù„ Ù…ØªÙ† Ø§Ù…ÙˆØ²Ø´ÛŒ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒÚ©Ù†Ø¯.<br>\n",
    "Ø§Ù…Ø§ Ø¨Ù‡ Ù†Ø³Ø¨Øª 4gram Ø¹Ù„Ù…Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ùˆ Ù…ØªÙˆØ§Ø²Ù† ØªØ±ÛŒ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ùˆ Ù†ÙˆØ´ØªÙ† Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø±Ø¯. Ù‡Ù… context Ù…Ù†Ø§Ø³Ø¨ ØªØ±ÛŒ Ø¯ÛŒØ¯Ù‡ Ø§Ø³Øª Ùˆ Ù‡Ù…Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ ØµÙØ± Ú©Ù…ØªØ± Ø¨Ø±Ø®ÙˆØ±Ø¯ Ù…ÛŒÚ©Ù†Ø¯.<br> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù…Ø¹ÛŒØ§Ø± Perplexity</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "Ø¯Ø§Ø¯Ú¯Ø§Ù† validation ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² N-gram Ù‡Ø§ÛŒ Ø®ÙˆØ¯ØŒ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¨Ú¯ÙˆÛŒÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ validation\n",
    "<br>\n",
    "- Ù†ØªÛŒØ¬Ù‡ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø¯Ø± Ù‡Ø± N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fair_mean_ppl(df_val_tokenized, lm, is_inf_counted=True, replace_inf=False):\n",
    "    import math\n",
    "    pps = []\n",
    "    inf_count = 0\n",
    "\n",
    "    INF_CAP = 1e6  # static large value to represent infinite perplexity\n",
    "\n",
    "    for sent in df_val_tokenized:\n",
    "        pp = lm.perplexity([sent])\n",
    "\n",
    "        if math.isinf(pp):\n",
    "            inf_count += 1\n",
    "            if not is_inf_counted:\n",
    "                continue\n",
    "\n",
    "            if replace_inf:\n",
    "                pp = INF_CAP  \n",
    "            else:\n",
    "                pps.append(float(\"inf\"))\n",
    "                continue\n",
    "\n",
    "        log_ppl = math.log(pp)\n",
    "        pps.append(log_ppl)\n",
    "\n",
    "    if not pps:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    # geometric mean of per-sentence perplexities\n",
    "    if any(math.isinf(x) for x in pps):\n",
    "        true_avg_pp = float(\"inf\")\n",
    "    else:\n",
    "        true_avg_pp = math.exp(sum(pps) / len(pps))\n",
    "\n",
    "    if not is_inf_counted:\n",
    "        msg = f\"Skipped {inf_count} sentences with inf perplexity.\"\n",
    "    elif replace_inf:\n",
    "        msg = f\"Replaced {inf_count} inf perplexities with {INF_CAP}.\"\n",
    "    else:\n",
    "        msg = f\"Included {inf_count} sentences with inf perplexity in mean.\"\n",
    "\n",
    "    print(msg)\n",
    "    return true_avg_pp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included 11765 sentences with inf perplexity in mean.\n",
      "Average perplexity for the validation set: inf\n",
      "\n",
      "Replaced 11765 inf perplexities with 1000000.0.\n",
      "Average perplexity for the validation set: 1925.2561001614085\n",
      "\n",
      "Skipped 11765 sentences with inf perplexity.\n",
      "Average Perplexity without INF counting: 18.65\n"
     ]
    }
   ],
   "source": [
    "# I have previously held out the validation data and i have preprocessed and tokenized it and its in the df_val_tokenized\n",
    "\n",
    "# row by row\n",
    "average_perplexity_2gram = fair_mean_ppl(df_val_tokenized, lm_2gram, is_inf_counted=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_2gram}\")\n",
    "print()\n",
    "average_perplexity_2gram_replace = fair_mean_ppl(df_val_tokenized, lm_2gram, is_inf_counted=True, replace_inf=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_2gram_replace}\")\n",
    "print()\n",
    "average_perplexity_2gram_without =  fair_mean_ppl(df_val_tokenized, lm_2gram, is_inf_counted=False)\n",
    "print(f\"Average Perplexity without INF counting: {average_perplexity_2gram_without:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included 27453 sentences with inf perplexity in mean.\n",
      "Average perplexity for the validation set: inf\n",
      "\n",
      "Replaced 27453 inf perplexities with 1000000.0.\n",
      "Average perplexity for the validation set: 924364.9932679189\n",
      "\n",
      "Skipped 27453 sentences with inf perplexity.\n",
      "Average Perplexity without INF counting: 4.66\n"
     ]
    }
   ],
   "source": [
    "average_perplexity_4gram = fair_mean_ppl(df_val_tokenized, lm_4gram, is_inf_counted=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_4gram}\")\n",
    "print()\n",
    "average_perplexity_4gram_replace = fair_mean_ppl(df_val_tokenized, lm_4gram, is_inf_counted=True, replace_inf=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_4gram_replace}\")\n",
    "print()\n",
    "average_perplexity_4gram_without =  fair_mean_ppl(df_val_tokenized, lm_4gram, is_inf_counted=False)\n",
    "print(f\"Average Perplexity without INF counting: {average_perplexity_4gram_without:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included 27603 sentences with inf perplexity in mean.\n",
      "Average perplexity for the validation set: inf\n",
      "\n",
      "Replaced 27603 inf perplexities with 1000000.0.\n",
      "Average perplexity for the validation set: 987122.1144265482\n",
      "\n",
      "Skipped 27603 sentences with inf perplexity.\n",
      "Average Perplexity without INF counting: 1.74\n"
     ]
    }
   ],
   "source": [
    "average_perplexity_8gram = fair_mean_ppl(df_val_tokenized, lm_8gram, is_inf_counted=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_8gram}\")\n",
    "print()\n",
    "average_perplexity_8gram_replace = fair_mean_ppl(df_val_tokenized, lm_8gram, is_inf_counted=True, replace_inf=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_8gram_replace}\")\n",
    "print()\n",
    "average_perplexity_8gram_without =  fair_mean_ppl(df_val_tokenized, lm_8gram, is_inf_counted=False)\n",
    "print(f\"Average Perplexity without INF counting: {average_perplexity_8gram_without:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "Ø¯Ø± Ù‡Ù…Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ù…Ù† Ø¢Ù…Ø¯Ù… Ùˆ PPL Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª row-wised Ú¯Ø±ÙØªÙ… Ùˆ Ø¨Ø¹Ø¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú¯Ø±ÙØªÙ… Ø§Ù„Ø¨ØªÙ‡ Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ²Ù† Ø¯Ø§Ø± Ø·Ø¨Ù‚ Ø·ÙˆÙ„ Ø¬Ù…Ù„Ù‡ Ú©Ù‡ Ø§Ù„Ø¨ØªÙ‡ Ø¯Ø± Ù‡Ù…Ù‡ Ø­Ø§Ù„Ø§Øª Ú†ÙˆÙ† Ø¯Ø± Ø¢Ù†Ù‡Ø§ Ø¨ÛŒ Ù†Ù‡Ø§ÛŒØª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Ø¨Ø¹Ù†ÛŒ Ø­Ø§Ù„ØªÛŒ Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ø§Ø­ØªÙ…Ø§Ù„ Ø¢Ù† Ùˆ Ø¬Ø§ÛŒ Ø¢Ù† ØµÙØ± Ù‚Ø±Ø§Ø± Ú©Ø±ÙØªÙ‡ Ø¨ÙˆØ¯ Ù¾Ø³ Ø¯Ø± Ø¯Ø¬ÙˆØ§Ø¨ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§ inf Ø¯ÛŒØ¯ÛŒÙ… ÛŒØ¹Ù†ÛŒ ppl Ø¨ÛŒ Ù†Ù‡Ø§ÛŒØª.<br>\n",
    "Ø­Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ Ù‡Ø±Ø­Ø§Ù„ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ Ø¢Ù…Ø¯Ù… Ùˆ Ø³Ø·Ø± Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒ Ù†Ù‡Ø§ÛŒØª Ù…ÛŒØ´ÙˆÙ†Ø¯ Ø±Ø§ Ø­Ø°Ù Ú©Ø±Ø¯Ù… Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø´ØªÛŒÙ… Ø¹Ø¯Ø¯ ppl Ø¯Ø± 8Ù„Ù‚Ø´Ø¦ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ùˆ Ø¨Ø¹Ø¯ Ø¯Ø±4 Ùˆ Ø¨Ø¹Ø¯ Ø¯Ø± 2gram Ú©Ù‡  Ù…Ù†Ø·Ù‚ÛŒ Ù‡Ù… Ù‡Ø³Øª.<br>\n",
    "Ø­Ø§Ù„Øª Ø¯ÛŒÚ¯Ø±ÛŒ Ù‡Ù… Ú©Ù‡ Ø¨ÛŒØ§ÛŒÛŒÙ… Ø¨Ù‡ Ø¬Ø§ÛŒ inf ÛŒÙ‡ Ø¹Ø¯Ø¯ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ú¯Ø°Ø§Ø±ÛŒÙ… ÙˆÙ„ÛŒ Ø¯Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨ÛŒØ§ÙˆØ±ÛŒÙ… Ù‡Ù… Ø¢ÙˆØ±Ø¯Ù… Ú©Ù‡ Ø¨Ø§Ø² Ø·Ø¨Ù‚ Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ù‡ Ø¹Ù„Øª Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨ÙˆØ¯Ù† n Ù…Ø§ ØªØ¹Ø¯Ø§Ø¯ inf Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†ppl Ø§Ù†Ù‡Ø§ Ù‡Ù… Ø¨ÛŒØ´ØªØ± Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ú†Ù‡Ø§Ø± Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ù†ÛŒØ¯ Ú©Ø¯Ø§Ù… Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ù…Ø­ØªÙ…Ù„â€ŒØªØ±Ù†Ø¯ Ú©Ù‡ ØªÙˆØ³Ø· ÛŒÚ© <span dir=\"ltr\">4-gram</span> Ú©Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡â€ŒØ§Ø³ØªØŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø§ÙˆÙ„ = Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¯Ø± Ù…Ø§Ø³Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ Ø¬Ø²Ø± Ùˆ Ù…Ø¯ Ø¢Ø¨ Ø±Ø§ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ù†Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯ÙˆÙ… = Ø¬ÛŒÙ„ Ùˆ ØªØ§Ù… Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø§Ù…Ø§Ù† Ùˆ Ø¨Ø§Ø¨Ø§ Ø¨Ù‡ Ø³Ø§Ø­Ù„ Ø±ÙØªÙ†Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø³ÙˆÙ… = ØªØ§Ù… Ø¨Ø·Ø±ÛŒâ€Œ Ù†ÙˆØ´Ø§Ø¨Ù‡â€Œ Ø±Ø§ ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¯Ø§Ø®Øª Ùˆ Ø¨Ù‡ Ø³Ù…Øª Ø§Ùˆ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ú†Ù‡Ø§Ø±Ù… = Ø¨Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ù…Ù†Ø²Ù„ Ù†Ù‚Ø§Ø´ÛŒ Ú©Ù†Ø¯ Ùˆ Ø¨Ø§ Ù¾Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø¸Ø±Ù‡ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ø¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¬Ù…Ù„Ù‡ 1: perp = inf\n",
      "Ø¬Ù…Ù„Ù‡ 2: perp = inf\n",
      "Ø¬Ù…Ù„Ù‡ 3: perp = inf\n",
      "Ø¬Ù…Ù„Ù‡ 4: perp = inf\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¯Ø± Ù…Ø§Ø³Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ Ø¬Ø²Ø± Ùˆ Ù…Ø¯ Ø¢Ø¨ Ø±Ø§ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ù†Ø¯\",\n",
    "    \"Ø¬ÛŒÙ„ Ùˆ ØªØ§Ù… Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø§Ù…Ø§Ù† Ùˆ Ø¨Ø§Ø¨Ø§ Ø¨Ù‡ Ø³Ø§Ø­Ù„ Ø±ÙØªÙ†Ø¯\",\n",
    "    \"ØªØ§Ù… Ø¨Ø·Ø±ÛŒâ€Œ Ù†ÙˆØ´Ø§Ø¨Ù‡â€Œ Ø±Ø§ ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¯Ø§Ø®Øª Ùˆ Ø¨Ù‡ Ø³Ù…Øª Ø§Ùˆ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯\",\n",
    "    \"Ø¨Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ù…Ù†Ø²Ù„ Ù†Ù‚Ø§Ø´ÛŒ Ú©Ù†Ø¯ Ùˆ Ø¨Ø§ Ù¾Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø¸Ø±Ù‡ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ø¯\",\n",
    "]\n",
    "\n",
    "tokenized_sents = [tokenizer.tokenize(s) for s in sentences]\n",
    "\n",
    "perps = []\n",
    "for sent_tokens in tokenized_sents:\n",
    "    ppl = lm_4gram.perplexity([sent_tokens])  # our method expects a list of token-lists\n",
    "    perps.append(ppl)\n",
    "\n",
    "for i, (s, p) in enumerate(zip(sentences, perps), 1):\n",
    "    print(f\"Ø¬Ù…Ù„Ù‡ {i}: perp = {p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "Ø¨Ù‡ Ù„Ø­Ø§Ø¸ ØªØ­Ù„ÛŒÙ„ Ø®Ø§Ù… Ø¨Ø§ÛŒØ¯ Ú¯ÙØª Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø¬Ù…Ù„Ù‡ Ø§ÙˆÙ„ Ø±Ø§ ÛŒÚ© 4gram ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.<br>\n",
    "Ø¨Ø§ÛŒØ¯ Ú¯ÙØª Ú©Ù‡ Ú©Ù‡ Ø¬Ù…Ù„Ù‡ Ø¯ÙˆÙ… Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ù„Ø­Ø§Ø¸ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù‡Ø± Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ú©Ù†Ø§Ø± Ù‡Ù… Ø¨ÛŒØ´ØªØ± Ø¨Ø§ Ù‡Ù… Ø§Ø±ØªØ¨Ø§Ø· Ø¯Ø§Ø±Ù†Ø¯ ØªØ§ Ø¬Ù…Ù„Ù‡ Ø¨Ø§Ù‡Ù… Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø§Ø² ÛŒÚ© Ngram Ø¨Ø§ n < 4 ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.<br>\n",
    "Ø±Ø§Ø¬Ø¨ Ø¬Ù…Ù„Ù‡ Ø³ÙˆÙ… Ùˆ Ú†Ù‡Ø§Ø±Ù… Ù‡Ù… Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ø±ØªØ¨Ø§Ø· ØªÙ…Ø§Ù… ØªÛŒÚ©Ù‡ Ù‡Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø¨Ø§Ù‡Ù… Ø¨Ù†Ø¸Ø± Ù…ÛŒ Ø¢ÛŒØ¯ ÛŒØ§ Ø§Ø² Ø·Ø±ÛŒÙ‚ ÛŒÚ© Ngram Ø¨Ø§ n Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² 4 ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ ØªÙˆØ³Ø· ÛŒÚ© llm Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¹Ù…Ù„Ø§ Ú©Ù„Ù…Ø§Øª Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø¬Ù…Ù„Ù‡ Ú©Ø§Ù…Ù„Ø§ Ø§Ø² Ú©Ø§Ù†ØªÚ©Ø³Øª Ú©Ù„Ù…Ø§Øª Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ø¬Ù…Ù„Ù‡ Ø®Ø¨Ø± Ø¯Ø§Ø±Ù†Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ (Smoothing) Ú©Ù‡ Ø¯Ø± Ø¯Ø±Ø³ Ø®ÙˆØ§Ù†Ø¯Ù‡â€ŒØ§ÛŒØ¯ (Laplace, Interpolation, Backoff) Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ÛŒÚ© Ù…Ø¯Ù„\n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "(Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ù…Ø¯Ù„ \n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "Ø¢Ù…ÙˆØ²Ø´â€ŒÛŒØ§ÙØªÙ‡ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.)\n",
    "<br>\n",
    "Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Interpolation Ù…Ù‚Ø§Ø¯ÛŒØ± Î» Ø±Ø§ 0.4, 0.3, 0.2, 0.1 Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯. Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª:\n",
    "Pâ€‹(wâˆ£h3â€‹,h2â€‹,h1â€‹)=0.4Pâ€‹(wâˆ£h3â€‹,h2â€‹,h1â€‹)+0.3Pâ€‹(wâˆ£h2â€‹,h1â€‹)+0.2Pâ€‹(wâˆ£h1â€‹)+0.1Pâ€‹(w)\n",
    "<br>\n",
    "Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Backoff Ù…Ù‚Ø¯Ø§Ø± Î» Ø±Ø§ 0.4 Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø± ÛŒÚ© Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ 100 ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªÙØ§ÙˆØª Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Ù†Ø¸Ø± Ø±ÙˆØ§Ù†ÛŒ Ùˆ ØªÙ†ÙˆØ¹ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¯Ø± Ù†Ù‡Ø§ÛŒØªØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± ÛŒÚ© Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ØŒ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø±Ø§ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ validation - Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø³ÙˆÙ… Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ - Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù…Ù‚Ø§Ø¯ÛŒØ± Perplexity Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø¯Ù„ ØºÛŒØ±Ù‡Ù…ÙˆØ§Ø± (Unsmoothed) Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø¨Ú¯ÙˆÛŒÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ø°Ú©Ø± Ø´Ø¯Ù‡\n",
    "<br>\n",
    "- Ù…Ø¹ÛŒØ§Ø± Perplexity Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ø°Ú©Ø± Ø´Ø¯Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3246 tokens\n",
      "\n",
      "generated text: \n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ²ÛŒØŒÛŒÚ© Ú©Ø¯ÙˆÛŒØ¨Ø²Ø±Ú¯ ÙˆÚ¯Ø±Ø¯ Ø¨ÙˆØ¯.ØªÙˆÙ¾ Ø®ÛŒÙ„ÛŒ Ù¾Ù‡Ù†ÙˆÙ…Ø­Ú©Ù… Ø¨ÙˆØ¯.ØªÙˆÙ¾ Ø´Ú©Ø³Øª ÙˆØ¨Ù‡ Ù‡Ø± Ú©Ø¯ÙˆÙ…Ø§Ø²\n",
      "Ø¯ÙˆØ³ØªØ§Ø´Ú†Ø§ÛŒØªÙˆ ÙÙ†Ø¬ÙˆÙ†Ø§ÛŒÚ©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø±ÛŒØ®Øª.Ø¯Ø± Ø§ÙˆÙ† Ù…ÛŒÙ‡Ù…ÙˆÙ†ÛŒ Ú†Ø§ÛŒÛŒ Ø´Ø±Ú©ØªÚ©Ø±Ø¯Ù†Ø¯.Ú©Ø§Ø±ÛŒ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ\n",
      "Ù¾Ø±ÛŒâ€ŒØ§Ø´Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯.Ø¢Ù† Ø¨Ø±Ú¯ Ø¨Ù‡ ÛŒÚ© Ù¾Ø±ÙˆØ§Ù†Ù‡\n"
     ]
    }
   ],
   "source": [
    "# I have done most of smoothing coding and i have put it into the class of the Ngram that we have, so here we are just using it\n",
    "# The smoothing is only implemented for the N=4\n",
    "\n",
    "# 4Gram with smoothign Laplace(add-one)\n",
    "lm_4gram_laplace = NGramLM(n=4, smoothing=NGramLM.LAPLACE)\n",
    "\n",
    "lm_4gram_laplace.train(df_train_tokenized)\n",
    "\n",
    "save_4gram_laplace_path = os.path.join(\".\", \"saved_models\", \"lm_4gram_laplace_model.pkl\")\n",
    "lm_4gram_laplace.save(save_4gram_laplace_path)\n",
    "\n",
    "generated_text = lm_8gram.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3246 tokens\n",
      "\n",
      "generated text: \n",
      "\n",
      "ÛŒÚ© Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´Ù‡Ø± Ø¯Ùˆ ÙˆØ§Ú¯Ù†Ø´Ø§Ù†Ø¯Ø± Ø¢Ø¨ Ú¯Ø±Ù… Ø´Ø¯Ù‡Ø¨ÙˆØ¯Ù†Ø¯ ÙˆØ®ÙˆØ´Ø­Ø§Ù„ÛŒ Ú©Ù‡ ØªÙˆÙ¾ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ±Ù†Ø¯.Ø¢Ù†â€ŒÙ‡Ø§\n",
      "Ø¨Ù¾Ø±Ø§Ù†Ø¯.Ù…Ø§Ù…Ø§Ù†Ù…ÛŒâ€ŒØ§ÙØªØ¯.ÙˆØ§Ú¯Ø±Ú†Ù‡ Ú©Ø§Ø±Ø±Ø§ ØªØ±Ú©Ú©Ù†Ø¯.Ù„ÛŒÙ„ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯.ÛŒÚ© Ù‚ÙˆØ±Ø¨Ø§ØºÙ‡Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³Øª\n",
      "Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø¹Ø´Ù‚ Ù¾Ø± Ø§Ø² Ù†ÙˆØ§Ø®ØªÙ† ØªØ±ÙˆÙ…\n"
     ]
    }
   ],
   "source": [
    "lm_4gram_backoff = NGramLM(n=4, smoothing=NGramLM.BACKOFF)\n",
    "\n",
    "lm_4gram_backoff.train(df_train_tokenized)\n",
    "\n",
    "save_4gram_backoff_path = os.path.join(\".\", \"saved_models\", \"lm_4gram_backoff_model.pkl\")\n",
    "lm_4gram_backoff.save(save_4gram_backoff_path)\n",
    "\n",
    "generated_text = lm_4gram_backoff.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to .\\saved_models\\lm_4gram_interpollation_model.pkl\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3246 tokens\n",
      "\n",
      "generated text: \n",
      "\n",
      "Ø±ÙˆØ²ÛŒÚ©Ù‡ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒØ±ÙˆÛŒØµÙˆØ±Øªâ€ŒØ´Ø§Ù†Ø±Ø§ Ø¨Ø´Ù†ÙˆÛŒ.Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ù†Ø§Ù… ØªÛŒÙ…ÛŒ Ú¯ÙØª:Â«ØªÙˆØ¨Ø§ÛŒØ¯Ø¯Ø±Ø³Øª Ú©Ø±Ø¯Ù†ÙØ§Ø±ØºØ§Ø²\n",
      "ØªØ¹Ø¬Ø¨Ø´ØŒØ³Ø§Ù…Ø¯Ø± Ø³Ø§Ø®ØªÛŒÙ…ØŒÙˆÚ†Ù†Ø¯ØªØ§ Ø±Ø§Ù‡Ù…ÛŒâ€ŒØ±ÙØªØŒÙˆØ¯ÛŒÚ¯Ø±Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³ØªÙ…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ú©Ù…Ú©Ù… Ø³Ø± Ø¨Ú¯Ø°Ø§Ø±Ù†Ø¯.\n"
     ]
    }
   ],
   "source": [
    "lm_4gram_interpolation = NGramLM(n=4, smoothing=NGramLM.INTERPOLATION)\n",
    "\n",
    "lm_4gram_interpolation.train(df_train_tokenized)\n",
    "\n",
    "save_4gram_interpollation_path = os.path.join(\".\", \"saved_models\", \"lm_4gram_interpollation_model.pkl\")\n",
    "lm_4gram_interpolation.save(save_4gram_interpollation_path)\n",
    "\n",
    "generated_text = lm_4gram_interpolation.generate(length=100, max_in_len=True, use_sampling_weighted=True)\n",
    "\n",
    "clean = (\n",
    "    generated_text\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity (4-gram Laplace): 17.86\n",
      "Average Perplexity (4-gram Backoff): 4.75\n",
      "Average Perplexity (4-gram Interpolation): 6.24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "average_perplexity_2gram = fair_mean_ppl(df_val_tokenized, lm_2gram, is_inf_counted=True)\n",
    "print(f\"Average perplexity for the validation set: {average_perplexity_2gram}\")\n",
    "# Laplace\n",
    "ppl_laplace = fair_mean_ppl(df_val_tokenized, lm_4gram_laplace, is_inf_counted=True)\n",
    "print(f\"Average Perplexity (4-gram Laplace): {ppl_laplace:.2f}\")\n",
    "\n",
    "# Backoff\n",
    "ppl_backoff = fair_mean_ppl(df_val_tokenized, lm_4gram_backoff, is_inf_counted=True)\n",
    "print(f\"Average Perplexity (4-gram Backoff): {ppl_backoff:.2f}\")\n",
    "\n",
    "# Interpolation\n",
    "ppl_interp = fair_mean_ppl(df_val_tokenized, lm_4gram_interpolation, is_inf_counted=True)\n",
    "print(f\"Average Perplexity (4-gram Interpolation): {ppl_interp:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "Ø¨Ø±Ø§ÛŒ Ù„Ø§Ù¾Ù„Ø§Ø³:<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„â€ŒØªØ± Ùˆ Ø·Ø¨ÛŒØ¹ÛŒâ€ŒØªØ±Ù†Ø¯Ø› Ø³Ø§Ø®ØªØ§Ø± Ù†Ø­ÙˆÛŒ Ø¯Ø±Ø³ØªÛŒ Ø¯Ø§Ø±Ù†Ø¯. Ø¨Ø¯Ù„ÛŒÙ„ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø² Ù„Ø§Ù¾Ù„Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ… Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù‡Ù… Ù…ÛŒØ´ÙˆØ¯ Ø§Ù†Ø¯Ú©ÛŒ ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª Ø¸Ø§Ù‡Ø± Ø´ÙˆØ¯ Ùˆ Ú†ÙˆÙ† Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ø§ Ø±Ø§ Ø¨Ù‡Ø¹Ù„Ø§ÙˆÙ‡ ÛŒÚ© Ú©Ø±Ø¯Ù‡ ØµÙˆØ±Øª Ø¢Ù† Ø±Ø§ ØªÙ‚Ø±ÛŒØ¨Ø§ Ø§ÛŒÙ…Ù† ØªØ± Ù…ÛŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¹Ù…Ù„Ø§ Ù…ØªÙ† ÛŒÚ© Ø¯Ø³Øª Ùˆ Ø§ÛŒÙ…Ù† Ø§Ù…Ø§ ØªÚ©Ø±Ø§Ø±ÛŒ Ù…ÛŒØ´ÙˆØ¯.<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¨Ú© Ø¢Ù :<br>\n",
    "ØªÙ†ÙˆØ¹ Ú©Ù„Ù…Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª Ø§Ù…Ø§ Ø¨Ù‡ Ù„Ø­Ø§Ø¸ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ø¬Ù…Ù„Ù‡ Ø¶Ø¹ÛŒÙ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ù‡Ù… Ø§ÛŒÙ† Ø§Ø³Øª ÙˆÙ‚ØªÛŒ ØªØ±ØªÛŒØ¨ÛŒ Ø±Ø§ Ù†Ù…ÛŒØ¨ÛŒÙ†Ø¯ Ø¨Ù‡ Ø­Ø§Ù„Øª Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ ØªØ± Ø¨Ø±Ù…ÛŒÚ¯Ø±Ø¯Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒØ´ÙˆØ¯ Ø§Ù†Ø³Ø¬Ø§Ù… Ø¬Ù…Ù„Ù‡ Ú©Ù…ØªØ± Ø¨Ø´ÙˆØ¯.<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†ØªØ±Ù¾ÙˆÙ„ÛŒØ´Ù†:<br>\n",
    "ØªÙ†ÙˆØ¹ Ø¨Ø§Ù„Ø§ Ø¯Ø± Ú©Ù„Ù…Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ùˆ Ù„Ø­Ù† Ú¯ÙØªØ§Ø±ÛŒ Ø¯Ø§Ø±Ø¯ Ù…ØªÙ† Ø§Ù…Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ø­ÙˆÛŒ Ùˆ Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ ÛŒØ§ Ú¯Ø±Ø§Ù…Ø±ÛŒ Ø²ÛŒØ§Ø¯ Ø®ÙˆØ¨ Ø¹Ù…Ù„ Ù†Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.<br>\n",
    "Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù„Ø§Ù¾Ù„Ø§Ø³ Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨ÙˆØ¯Ù†Ø¯ Ø±Ø§ Ú©Ù…ØªØ± Ú©Ø±Ø¯ÛŒÙ… Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ ppl Ø¯Ø± Ú©Ù„ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ø§ØªÛŒ Ú©Ù‡ Ø¯Ø±Ø³Øª Ù‡Ø³ØªÙ†Ø¯ Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø­Ø§Ù„Ø§Øª Ø¯ÛŒÚ¯Ø± Ø¨Ù‡ Ù…Ø§ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒØ´ÙˆØ¯ .<br>\n",
    "Ø¯Ø± Ø­Ø§Ù„Øª Ø¨Ú© Ø¢Ù\n",
    "Ø¯Ø± Ø¹Ù…Ù„ØŒ Ù…Ø¯Ù„ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø±Ø§ Ø¨Ù‡ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ log-probÙ‡Ø§ Ø¨Ø²Ø±Ú¯â€ŒØªØ± (ÛŒØ¹Ù†ÛŒ Ù…Ù†ÙÛŒÙ Ú©ÙˆÚ†Ú©ØªØ±) Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.<br>\n",
    "Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø±ÙˆØ´ interpolation Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ²Ù† Ø¯Ø§Ø± Ø§Ø­ØªÙ…Ø§Ù„ Ù…ÛŒØ¯Ù‡Ø¯ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ú© Ø¢Ù Ø§Ù†Ù‚Ø¯Ø± Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø§ Ø¨Ø§Ù„Ø§ Ù…ÛŒØ¨ÛŒÙ†Ø¯ Ùˆ Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø¯Ø± Ú©Ù„ Ú†ÙˆÙ† Ø¯Ø±Ø­Ø§Ù„ Ú¯Ø±ÙØªÙ† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù† Ø¯Ø§Ø± Ø¨Ø§ Ø¶Ø±ÛŒØ¨ Ù‡Ø³ØªÛŒÙ… Ù…Ù†Ø¸Ù‚ÛŒ Ø§Ø³Øª Ø¹Ø¯Ø¯ ppl Ø¢Ù† Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ø¯ÛŒÚ¯Ø± Ø¨Ø§Ø´Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Temperature</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ§Ø«ÛŒØ± Temperature Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ ØªÙˆØ¶ÛŒØ­â€ŒØ¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ù‡ Ù‡Ù†Ú¯Ø§Ù… Sampling Ø§Ø² N-gramØŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù† Temperature ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒØ¯ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ú©Ù…ØªØ±ÛŒÙ† Temperature Ùˆ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† TemperatureØŒ Ø³Ù‡â€ŒØ¨Ø§Ø± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ Û²Û° ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯. (Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø­Ø§Ù„Øª Ø³Ù‡â€ŒØ¨Ø§Ø±) Ùˆ Ø³Ù¾Ø³ ØªØ§Ø«ÛŒØ± Temperature Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Temperature Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ† (Ø¨Ø±Ø§ÛŒ Ù‡Ø±ÛŒÚ© Û³ Ø¹Ø¯Ø¯ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text: \n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ² Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ\n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ² Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ\n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ² Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ…ÙˆØ³Ùˆ\n"
     ]
    }
   ],
   "source": [
    "lm = lm_4gram_interpolation\n",
    "\n",
    "generated_text1 = lm.generate(30, use_sampling_weighted=True, temperature=0)\n",
    "generated_text2 = lm.generate(30, use_sampling_weighted=True, temperature=0)\n",
    "generated_text3 = lm.generate(30, use_sampling_weighted=True, temperature=0) \n",
    "\n",
    "\n",
    "clean1 = (\n",
    "    generated_text1\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean1 = re.sub(r\"\\s+\", \" \", clean1).strip()\n",
    "\n",
    "clean2 = (\n",
    "    generated_text2\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean2 = re.sub(r\"\\s+\", \" \", clean2).strip()\n",
    "\n",
    "clean3 = (\n",
    "    generated_text3\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean3 = re.sub(r\"\\s+\", \" \", clean3).strip()\n",
    "\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean1, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean2, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean3, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text: \n",
      "\n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒØ¢Ù† Ø±Ø§ Ø¨Ø§ ØªÚ©Ø§Ù†Ø´Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡Ø±ÙˆÛŒØ¯Ø± Ø§ÛŒÙ† ÙˆØ¢Ù†\n",
      "\n",
      "Ø³Ø±Ø§ÙˆØ¨Ù† Ú¯ÙØª Ù¾Ø§Ø±Ú©Ú©Ø§Ø±Ú©Ù†Ø¯!\n",
      "\n",
      "Ø±ÙˆØ²ÛŒÛŒÚ© Ø§ÛŒØ¯Ù‡ Ø¨Ù‡â€ŒÙ†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ø±Ø§ Ú¯Ø±Ø¨Ù‡ Ù†Ø§Ù… Ù…ÛŒØ§Ù†ØªÙˆØ§Ù† ØªØ±Ø¬Ù…Ù‡Ú©Ø§Ù…Ù„\n"
     ]
    }
   ],
   "source": [
    "generated_text1 = lm.generate(30, use_sampling_weighted=True, temperature=1)\n",
    "generated_text2 = lm.generate(30, use_sampling_weighted=True, temperature=1)\n",
    "generated_text3 = lm.generate(30, use_sampling_weighted=True, temperature=1) \n",
    "\n",
    "\n",
    "clean1 = (\n",
    "    generated_text1\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean1 = re.sub(r\"\\s+\", \" \", clean1).strip()\n",
    "\n",
    "clean2 = (\n",
    "    generated_text2\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean2 = re.sub(r\"\\s+\", \" \", clean2).strip()\n",
    "\n",
    "clean3 = (\n",
    "    generated_text3\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean3 = re.sub(r\"\\s+\", \" \", clean3).strip()\n",
    "\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean1, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean2, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean3, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text: \n",
      "\n",
      "Ù…ØªØ£Ø³ÙÙ…ØŒÙ…Ù† Ù…Ø­Ø§ÙØ¸Ø¨ÙˆØ¯.Ø¨Ø±Ø§Ù†ÛŒØªØ§Ù…Ø®ÛŒÙ„ÛŒ Ø³Ø¨Ú©â€ŒØªØ± Ø´ÙˆÛŒ!ÙÙ‚Ø·Ø³Ø¹ÛŒØ¯Ø§Ø´Øª Ø¨Ù‡ Ù…Ø³\n",
      "\n",
      "ÛŒÚ©â€ŒØ±ÙˆØ²Ù‰ØŒÛŒÚ© Ø²Ù†Ø¨ÙˆØ±Ù…Ø´ØºÙˆÙ„ÛŒØŸÂ«Ù…Ø§Ø¯Ø±Ø´Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯.Ø³Ù¾Ø³ØªÛŒÙ…ÙˆØ­Ø±Ù ØªØ§Ù…Ù¾Ø´ÛŒÙ…ÙˆÙ†Ø´Ø¯ÛŒ.\n",
      "\n",
      "Ù…Ø¯ØªÙ‡Ø§ ÙˆØ§Ø±ØºÙˆØ§Ù†ÛŒØ¨Ù‡ Ø²ÙˆØ²Ù‡ ÛŒØ¨Ø²Ø±Ú¯ Ø®Ø±ÛŒØ¯ÛŒ Ø¨Ø§Ù„Ú¯Ø±Ø¯.Ù…Ù† ØªÙˆ Ú©Ù†Ø¬\n"
     ]
    }
   ],
   "source": [
    "generated_text1 = lm.generate(30, use_sampling_weighted=True, temperature=2)\n",
    "generated_text2 = lm.generate(30, use_sampling_weighted=True, temperature=2)\n",
    "generated_text3 = lm.generate(30, use_sampling_weighted=True, temperature=2) \n",
    "\n",
    "\n",
    "clean1 = (\n",
    "    generated_text1\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean1 = re.sub(r\"\\s+\", \" \", clean1).strip()\n",
    "\n",
    "clean2 = (\n",
    "    generated_text2\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean2 = re.sub(r\"\\s+\", \" \", clean2).strip()\n",
    "\n",
    "clean3 = (\n",
    "    generated_text3\n",
    "    .replace(\"</w>\", \"\")      # BPE/WordPiece end-of-word\n",
    "    .replace(\"<bos>\", \"\")     # our BOS\n",
    "    .replace(\"<eos>\", \"\")     # our EOS\n",
    ")\n",
    "clean3 = re.sub(r\"\\s+\", \" \", clean3).strip()\n",
    "\n",
    "print(\"\\ngenerated text: \\n\")\n",
    "print(textwrap.fill(clean1, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean2, width=80))\n",
    "print()\n",
    "print(textwrap.fill(clean3, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø´Ø´Ù…:</b><br>\n",
    "tempreture Ø¹Ù…Ù„Ø§ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³Øª Ø¨ÛŒÙ† 0 ØªØ§2 Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¹Ù…ÙˆÙ„ Ú©Ù‡ Ù‡Ø±Ø¬Ù‡ Ø¨Ù‡ ØµÙØ± Ù†Ø²Ø¯ÛŒÚ© ØªØ± Ø¨Ø§Ø´Ø¯ ÛŒØ¹Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚Ø§ Ù‡Ù…Ø§Ù† Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÙˆØ¯Ù†Ø¯ Ø±Ø§ Ø¨Ú¯Ø°Ø§Ø± Ùˆ Ù…Ø«Ù„Ø§ 0 ÛŒØ¹Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚Ø§ Ù‡Ù…Ø§Ù† Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªÛŒ Ú©Ù‡ Ø¨Ø¯Ø³Øª Ø¢Ù…Ø¯Ù†Ø¯ Ø±Ø§ Ø¨Ú¯Ø°Ø§Ø± Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¬Ù…Ù„Ø§Øª ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ ØªØ§ Ø­Ø¯ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ ØªØ± Ùˆ Ø¨ÛŒØ´ØªØ± Ø´Ø¨ÛŒÙ‡ Ø¯ÛŒØªØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù…ÛŒØ´ÙˆØ¯.<br>\n",
    "Ø§Ù…Ø§ Ù‡Ø±Ú†Ù‡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø¨Ù‡ 2 Ù†Ø²Ø¯ÛŒÚ© ØªØ± Ø¨Ø§Ø´Ø¯ Ù…Ø§ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ ØªØ± Ù…ÛŒÚ©Ù†ÛŒÙ… Ùˆ Ø¹Ù…Ù„Ø§ Ø¨Ù‡ Ú†ÛŒØ² Ù‡Ø§ÛŒ Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ú©Ù… Ø§Ø­ØªÙ…Ø§Ù„ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ø¬ÛŒØ² Ù‡Ø§ÛŒ Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨ÛŒØ´ØªØ± Ú©Ù…ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù† Ø±Ø§ Ú©Ù… Ù…ÛŒÚ©Ù†ÛŒÙ… Ø§Ù„Ø¨ØªÙ‡ Ú©Ù‡ ØªØ±ØªÛŒØ¨ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ù‡Ù… Ø¨Ø±ÛŒØ²Ø¯ Ùˆ Ø·Ø¨Ù‚ ÙØ±Ù…ÙˆÙ„ Ø¢Ù† Ù†Ù…ÛŒØ±ÛŒØ²Ø¯. Ø¯Ø± Ú©Ù„ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§Ø¹Ø« Ø´Ø¯Ù‡ Ø®Ù„Ø§Ù‚ÛŒØª Ø¯Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¨ÛŒØ´ØªØ± Ø´ÙˆØ¯ Ø§Ù…Ø§ Ø¨Ù‡ Ù‡Ù…Ø§Ù† Ù…ÛŒØ²Ø§Ù† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù„Ù…Ù‡ Ø§ÛŒ Ø¨ÛŒØ§ÛŒØ¯ Ú©Ù‡ Ø§Ù†Ù‚Ø¯Ø± Ø´Ø§ÛŒØ¯ Ø¯Ø± Ø¬Ø§ÛŒÚ¯Ø§Ù‡ÛŒ Ù†Ø¨ÙˆØ¯Ù‡ Ú©Ù‡ Ø¨ÛŒØ§ÛŒØ¯. (Ø§Ù„Ø¨ØªÙ‡ Ø§ÛŒÙ† Ø±Ø§ Ø¯Ø±Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ú©Ù‡ Ù…ÛŒØªÙˆØ§Ù† Ø§ÛŒÙ† tempreture Ø±Ø§ ØªØ§ Ø¨ÛŒÙ†Ù‡Ø§ÛŒØª Ù‡Ù… Ø¯Ø§Ø¯ Ù‡Ù… Ø¨Ù‡ ØµÙˆØ±Øª ØªØ¹ÙˆØ±ÛŒ Ùˆ Ø¯Ø± Ø¹Ù…Ù„ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¹Ø¯Ø¯ÛŒ Ø¨ÛŒÙ† 0 ØªØ§ 2 Ø¢Ù† Ø±Ø§ Ù…ÛŒÚ¯Ø°Ø§Ø±Ù†Ø¯.)<br>\n",
    "Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø§ Ù‡Ù… Ø¯Ù‚ÛŒÙ‚Ø§ Ø·Ø¨Ù‚ Ø§Ù†ØªØ¸Ø§Ø± Ù…Ø§ Ø¸Ø§Ù‡Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª . ÛŒØ¹Ù†ÛŒ Ø¯Ø± Ø­Ø§Ù„Øª tmep=0 Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ú¯Ø±ÙØªÙ† Ø¬Ù…Ù„Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨ÙˆØ¯ Ú©Ù‡ Ù‡Ù…ÛŒÙ†Ø·ÙˆØ± Ú©Ù‡ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ… Ø¬Ù…Ù„Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø§Ø±ÛŒÙ… ÙˆÙ„ÛŒ Ø¯Ø± Ø­Ø§Ù„Øª temp=2 Ù…Ø§ ØªÙ†ÙˆØ¹ Ú©Ø§Ù…Ù„ Ø¯Ø± Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø² Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ùˆ Ù†Ø­ÙˆÛŒ Ø¢Ù† Ú©Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px; text-align: right;\">\n",
    "    <p style=\"text-align: right;\" dir=\"rtl\"><strong dir=\"rtl\">Ù…Ù‡Ù„Øª ØªØ­ÙˆÛŒÙ„ :</strong> 10 Ø¢Ø¨Ø§Ù†</p>\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
