{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† Û²<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Logistic Regression and Naive Bayes</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ÙØ±Ù‡Ø§Ø¯ Ù†ØµØ±ÛŒ - Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">farhadnasri999@gmail.com - shigzv@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ù…Ù‡Ø¯ÛŒ Ù…Ø®ØªØ§Ø±ÛŒ</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: 810101515</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: 25 Ø¢Ø¨Ø§Ù† 1404</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Logistic Regression and Naive Bayes (from scratch)</span> (60)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</li>\n",
    "<li>Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Logistic Regression and Naive Bayes (e.g. \n",
    "sklearn)</span> (40)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ø³ÙˆØ§Ù„ Ø¯Ùˆ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ù‡Ø³ØªÛŒØ¯ ÙˆÙ„ÛŒ Ø¯Ø± Ø³ÙˆØ§Ù„ ÛŒÚ©ØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ù‡Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ù¾Ø§ÛŒÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ Ø§Ø² ØµÙØ± (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ù‡Ù…â€ŒÚ†Ù†ÛŒÙ† Ù†ÛŒØ§Ø² Ø§Ø³ØªØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ù†ÛŒØ² Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ú¯Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ù‡Ø¯Ù Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Øª. Ø§Ø² Ø¢Ù†â€ŒØ¬Ø§ Ú©Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø®Ø§Ù… Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… ØªØ§ Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ§Ù…ÙˆØ²Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙˆÙ† Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Bag of WordsØŒ Ù‡Ø± Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒØ´ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ….\n",
    "<br>\n",
    "Ø¯ÛŒØªØ§Ø³Øª emails_1.csv Ø´Ø§Ù…Ù„ Ø¯Ùˆ Ø³ØªÙˆÙ† Ø§Ø³Øª:\n",
    "<br>\n",
    "text (Ù…ØªÙ† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "status (Ø¨Ø±Ú†Ø³Ø¨ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†Ù†Ø¯Ù‡â€ŒÛŒ spam ÛŒØ§ ham Ø¨ÙˆØ¯Ù† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "Û±. Ø³ØªÙˆÙ† status Ø±Ø§ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (spam â†’ 1 Ùˆ ham â†’ 0).\n",
    "<br>\n",
    "Û². Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯: ØªÙ…Ø§Ù… Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ± Ø§Ù„ÙØ¨Ø§ÛŒÛŒ (Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¹Ù„Ø§Ø¦Ù… Ùˆ ØºÛŒØ±Ù‡) Ø±Ø§ Ø­Ø°Ù Ùˆ Ù‡Ù…Ù‡â€ŒÛŒ Ø­Ø±ÙˆÙ Ø±Ø§ Ú©ÙˆÚ†Ú© (lowercase) Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Û³. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Bag of WordsØŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· Û±Ûµ Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² CountVectorizer Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ sklearn Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "ÛŒÚ© Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¬Ø¯ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Û±Ûµ ÙˆÛŒÚ˜Ú¯ÛŒ + Ø³ØªÙˆÙ† status Ø§Ø³Øª. (ØªÙ…Ø§Ù… Û±Û° Ø±Ø¯ÛŒÙ Ø§ÛŒÙ† Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "emails_path = os.path.join(\"datasets\", \"q1\", \"emails_1.csv\")\n",
    "\n",
    "df = pd.read_csv(emails_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ‰ Congratulations! You have won a FREE ticket ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reminder: The project meeting will be held tom...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exclusive offer just for YOU! Get a free-trial...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey team - the report looks great; please revi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Youâ€™ve been selected for a *FREE* gift card! C...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text status\n",
       "0  ğŸ‰ Congratulations! You have won a FREE ticket ...   spam\n",
       "1  Reminder: The project meeting will be held tom...    ham\n",
       "2  Exclusive offer just for YOU! Get a free-trial...   spam\n",
       "3  Hey team - the report looks great; please revi...    ham\n",
       "4  Youâ€™ve been selected for a *FREE* gift card! C...   spam"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    10 non-null     object\n",
      " 1   status  10 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 292.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ğŸ‰ Congratulations! You have won a FREE ticket ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text status\n",
       "count                                                  10     10\n",
       "unique                                                 10      2\n",
       "top     ğŸ‰ Congratulations! You have won a FREE ticket ...   spam\n",
       "freq                                                    1      6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "spam    6\n",
       "ham     4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoteBook\\AppData\\Local\\Temp\\ipykernel_2776\\2639654632.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({'status': {'ham': 0, 'spam': 1}})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ‰ Congratulations! You have won a FREE ticket ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reminder: The project meeting will be held tom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  status\n",
       "0  ğŸ‰ Congratulations! You have won a FREE ticket ...       1\n",
       "1  Reminder: The project meeting will be held tom...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace({'status': {'ham': 0, 'spam': 1}})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>congratulations you have won a free ticket to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reminder the project meeting will be held tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exclusive offer just for you get a freetrial o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hey team  the report looks great please review...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>youve been selected for a free gift card claim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  status\n",
       "0   congratulations you have won a free ticket to...       1\n",
       "1  reminder the project meeting will be held tomo...       0\n",
       "2  exclusive offer just for you get a freetrial o...       1\n",
       "3  hey team  the report looks great please review...       0\n",
       "4  youve been selected for a free gift card claim...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  frequency\n",
      "112     your         10\n",
      "39      free          6\n",
      "38       for          6\n",
      "93       the          5\n",
      "96        to          4\n",
      "20     claim          3\n",
      "73       our          3\n",
      "79   project          3\n",
      "76    please          3\n",
      "42       get          3\n",
      "111      you          3\n",
      "5        and          3\n",
      "10      been          2\n",
      "97     today          2\n",
      "63   morning          2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "word_frequencies = X.toarray().sum(axis=0)\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "word_freq_df = pd.DataFrame(zip(words, word_frequencies), columns=['word', 'frequency'])\n",
    "\n",
    "top_15_words = word_freq_df.sort_values(by='frequency', ascending=False).head(15)\n",
    "\n",
    "print(top_15_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø² Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø¯ÛŒÚ¯Ø± Ø¨Ø§ ÙØ§ÛŒÙ„ emails_1.csv Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø­Ø§ØµÙ„ Ø§Ø² Ø¢Ù† Ú©Ø§Ø±ÛŒ Ù†Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª. Ø¨Ø±Ø§ÛŒ Ø³Ù‡ÙˆÙ„Øª Ú©Ø§Ø±ØŒ ÙØ§ÛŒÙ„ emails_2.csv Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† ÙØ§ÛŒÙ„ØŒ Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Ù‡Ù…Ø§Ù† ÙØ±Ø§ÛŒÙ†Ø¯ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø± Ø±ÙˆÛŒ ÛµÛ±Û·Û² Ø§ÛŒÙ…ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª. Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ø³Ø§Ø¯Ù‡â€ŒØªØ±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ù‡Ø± Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ§Ù†Ú¯Ø± ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ùˆ Ù‡Ø± Ø³ØªÙˆÙ† Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡â€ŒÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Û³Û°Û°Û° Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª.\n",
    "Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ø¨Ø§ÛŒØ¯ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙØ§ÛŒÙ„ emails_2.csv Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†ÛŒØ¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù† Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯ ØªØ§ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ø¢Ø´Ù†Ø§ Ø´ÙˆÛŒØ¯.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù¾Ù†Ø¬ Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…Ø°Ú©ÙˆØ± Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "emails_path = os.path.join(\"datasets\", \"q1\", \"emails_2.csv\")\n",
    "\n",
    "df = pd.read_csv(emails_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email No.</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>ect</th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>you</th>\n",
       "      <th>hou</th>\n",
       "      <th>...</th>\n",
       "      <th>connevey</th>\n",
       "      <th>jay</th>\n",
       "      <th>valued</th>\n",
       "      <th>lay</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>military</th>\n",
       "      <th>allowing</th>\n",
       "      <th>ff</th>\n",
       "      <th>dry</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email 1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Email 2</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Email 3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Email 4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Email 5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  jay  \\\n",
       "0   Email 1    0   0    1    0    0   0    2    0    0  ...         0    0   \n",
       "1   Email 2    8  13   24    6    6   2  102    1   27  ...         0    0   \n",
       "2   Email 3    0   0    1    0    0   0    8    0    0  ...         0    0   \n",
       "3   Email 4    0   5   22    0    5   1   51    2   10  ...         0    0   \n",
       "4   Email 5    7   6   17    1    5   2   57    0    9  ...         0    0   \n",
       "\n",
       "   valued  lay  infrastructure  military  allowing  ff  dry  Prediction  \n",
       "0       0    0               0         0         0   0    0           0  \n",
       "1       0    0               0         0         0   1    0           0  \n",
       "2       0    0               0         0         0   0    0           0  \n",
       "3       0    0               0         0         0   0    0           0  \n",
       "4       0    0               0         0         0   1    0           0  \n",
       "\n",
       "[5 rows x 3002 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5172 entries, 0 to 5171\n",
      "Columns: 3002 entries, Email No. to Prediction\n",
      "dtypes: int64(3001), object(1)\n",
      "memory usage: 118.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>ect</th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>you</th>\n",
       "      <th>hou</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>connevey</th>\n",
       "      <th>jay</th>\n",
       "      <th>valued</th>\n",
       "      <th>lay</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>military</th>\n",
       "      <th>allowing</th>\n",
       "      <th>ff</th>\n",
       "      <th>dry</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>5172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.640565</td>\n",
       "      <td>6.188128</td>\n",
       "      <td>5.143852</td>\n",
       "      <td>3.075599</td>\n",
       "      <td>3.124710</td>\n",
       "      <td>2.627030</td>\n",
       "      <td>55.517401</td>\n",
       "      <td>2.466551</td>\n",
       "      <td>2.024362</td>\n",
       "      <td>10.600155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.012568</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.098028</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.914733</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>0.290023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.745009</td>\n",
       "      <td>9.534576</td>\n",
       "      <td>14.101142</td>\n",
       "      <td>6.045970</td>\n",
       "      <td>4.680522</td>\n",
       "      <td>6.229845</td>\n",
       "      <td>87.574172</td>\n",
       "      <td>4.314444</td>\n",
       "      <td>6.967878</td>\n",
       "      <td>19.281892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.199682</td>\n",
       "      <td>0.116693</td>\n",
       "      <td>0.569532</td>\n",
       "      <td>0.096252</td>\n",
       "      <td>0.138908</td>\n",
       "      <td>0.072145</td>\n",
       "      <td>2.780203</td>\n",
       "      <td>0.098086</td>\n",
       "      <td>0.453817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>210.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>1898.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               the           to          ect          and          for  \\\n",
       "count  5172.000000  5172.000000  5172.000000  5172.000000  5172.000000   \n",
       "mean      6.640565     6.188128     5.143852     3.075599     3.124710   \n",
       "std      11.745009     9.534576    14.101142     6.045970     4.680522   \n",
       "min       0.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "25%       0.000000     1.000000     1.000000     0.000000     1.000000   \n",
       "50%       3.000000     3.000000     1.000000     1.000000     2.000000   \n",
       "75%       8.000000     7.000000     4.000000     3.000000     4.000000   \n",
       "max     210.000000   132.000000   344.000000    89.000000    47.000000   \n",
       "\n",
       "                of            a          you          hou           in  ...  \\\n",
       "count  5172.000000  5172.000000  5172.000000  5172.000000  5172.000000  ...   \n",
       "mean      2.627030    55.517401     2.466551     2.024362    10.600155  ...   \n",
       "std       6.229845    87.574172     4.314444     6.967878    19.281892  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000    12.000000     0.000000     0.000000     1.000000  ...   \n",
       "50%       1.000000    28.000000     1.000000     0.000000     5.000000  ...   \n",
       "75%       2.000000    62.250000     3.000000     1.000000    12.000000  ...   \n",
       "max      77.000000  1898.000000    70.000000   167.000000   223.000000  ...   \n",
       "\n",
       "          connevey          jay       valued          lay  infrastructure  \\\n",
       "count  5172.000000  5172.000000  5172.000000  5172.000000     5172.000000   \n",
       "mean      0.005027     0.012568     0.010634     0.098028        0.004254   \n",
       "std       0.105788     0.199682     0.116693     0.569532        0.096252   \n",
       "min       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000        0.000000   \n",
       "max       4.000000     7.000000     2.000000    12.000000        3.000000   \n",
       "\n",
       "          military     allowing           ff          dry   Prediction  \n",
       "count  5172.000000  5172.000000  5172.000000  5172.000000  5172.000000  \n",
       "mean      0.006574     0.004060     0.914733     0.006961     0.290023  \n",
       "std       0.138908     0.072145     2.780203     0.098086     0.453817  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     1.000000     0.000000     1.000000  \n",
       "max       4.000000     3.000000   114.000000     4.000000     1.000000  \n",
       "\n",
       "[8 rows x 3001 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns: 3002\n"
     ]
    }
   ],
   "source": [
    "print(\"number of columns:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ù†Ø³Ø¨Øª Û¸Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Û²Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "X_train, X_test, y_train, y_test<br>\n",
    "ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ X_train Ùˆ X_test \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_train_test_split(df, train_percentage=0.8, random_seed=None):\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    shuffled_df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    \n",
    "    train_size = int(len(df) * train_percentage)\n",
    "    \n",
    "    train_df = shuffled_df.iloc[:train_size]\n",
    "    test_df = shuffled_df.iloc[train_size:]\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def split_features_labels(df):\n",
    "    X = df.iloc[:, 1:-1] \n",
    "    y = df.iloc[:, -1] \n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4137, 3000)\n",
      "y_train shape: (4137,)\n",
      "X_test shape: (1035, 3000)\n",
      "y_test shape: (1035,)\n",
      "Training set size: 4137\n",
      "Test set size: 1035\n",
      "Training set num of cols: 3002\n",
      "Test set num of cols: 3002\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = custom_train_test_split(df, train_percentage=0.8, random_seed=0)\n",
    "\n",
    "X_train, y_train = split_features_labels(train_df)\n",
    "X_test, y_test = split_features_labels(test_df)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Training set num of cols: {(train_df.shape[1])}\")\n",
    "print(f\"Test set num of cols: {(test_df.shape[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù¾ÛŒØ´ Ø§Ø² Ø¢Ù†â€ŒÚ©Ù‡ ÙˆØ§Ø±Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´ÙˆÛŒÙ…ØŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¬Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ù‡ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ Ø¨ÙÙ‡Ù…ÛŒÙ… Ù…Ø¯Ù„ ØªØ§ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ù…ÙˆÙÙ‚ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø¯ÙˆØ¯ÙˆÛŒÛŒ (binary classification) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "accuracy(y_true, y_pred) â€” Ù†Ø³Ø¨Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø³Øª Ø¨Ù‡ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§\n",
    "<br>\n",
    "precision(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡â€ŒØ§Ù†Ø¯\n",
    "<br>\n",
    "recall(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±Ø³Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª\n",
    "<br>\n",
    "f1_score(y_true, y_pred) â€” Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø§Ø±Ù…ÙˆÙ†ÛŒÚ© Ø¨ÛŒÙ† precision Ùˆ recallØŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ØªÙˆØ§Ø²Ù† Ù…ÛŒØ§Ù† Ø¢Ù† Ø¯Ùˆ\n",
    "<br>\n",
    "Ù‡Ø± ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.\n",
    "<br>\n",
    " Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø§ Ø¯Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯:    y_true = [0, 1, 1, 0, 1] ---- y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø²  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ numpy Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø«Ø§Ù„:\n",
    "<br>\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "<br>\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "print(accuracy(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "print(precision(y_true, y_pred)) # 1.00\n",
    "<br>\n",
    "print(recall(y_true, y_pred))    # 0.66\n",
    "<br>\n",
    "print(f1_score(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true: list, y_pred: list):\n",
    "\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(\"ERROR: y-pred and y_ture are not equal in size\")\n",
    "        return -1\n",
    "    \n",
    "    tp_tn = 0\n",
    "    fp_fn = 0\n",
    "\n",
    "    result = [a == b for a, b in zip(y_pred, y_true)]\n",
    "    tp_tn = sum(result)\n",
    "\n",
    "    acc = tp_tn / len(y_pred)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def precision(y_true: list, y_pred: list):\n",
    "\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(\"ERROR: y-pred and y_ture are not equal in size\")\n",
    "        return -1\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            fn += 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            fp += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 0:\n",
    "            tn += 1\n",
    "        else:\n",
    "            raise Exception(f\"y_pred and y_test had differnt values than 0 or 1.  y_pred:{y_pred} , y_true:{y_true}\")\n",
    "        \n",
    "    # print(\"True Positives (tp):\", tp)\n",
    "    # print(\"True Negatives (tn):\", tn)\n",
    "    # print(\"False Positives (fp):\", fp)\n",
    "    # print(\"False Negatives (fn):\", fn)\n",
    "    \n",
    "    try:\n",
    "        prec = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        prec = 0\n",
    "        print(\"Warning, in precision Zero division occured\")\n",
    "    return prec\n",
    "\n",
    "def recall(y_true: list, y_pred: list):\n",
    "\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(\"ERROR: y-pred and y_true are not equal in size\")\n",
    "        return -1\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            fn += 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            fp += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 0:\n",
    "            tn += 1\n",
    "        else:\n",
    "            raise Exception(f\"y_pred and y_test had differnt values than 0 or 1.  y_pred:{y_pred} , y_true:{y_true}\")\n",
    "\n",
    "    try:\n",
    "        reca = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        reca = 0\n",
    "        print(\"Warning, in recall Zero division occured\")\n",
    "\n",
    "    return reca\n",
    "\n",
    "def f1_score(y_true: list, y_pred: list):\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(\"ERROR: y-pred and y_ture are not equal in size\")\n",
    "        return -1\n",
    "    \n",
    "    prec = precision(y_true, y_pred)\n",
    "    reca = recall(y_true, y_pred)\n",
    "    try:\n",
    "        f1 = (2 * prec * reca) / (prec + reca) \n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "        print(\"Warning, in F1-score Zero division occured\")\n",
    "\n",
    "    return f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n",
      "precision: 1.0\n",
      "recall: 0.67\n",
      "f1-socre: 0.80\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "\n",
    "print(\"accuracy:\",accuracy(y_true, y_pred))  # 0.80\n",
    "print(\"precision:\", precision(y_true, y_pred)) # 1.00\n",
    "print('recall:', f\"{recall(y_true, y_pred):.2f}\")    # 0.66\n",
    "print(\"f1-socre:\", f\"{f1_score(y_true, y_pred):.2f}\")  # 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Logistic Regression Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ØªØ±ÛŒÚ© Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø§ÛŒÙ¾Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø§Ù†Ù†Ø¯ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø¹Ù‡Ø¯Ù‡ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø³Øª.\n",
    "<br>\n",
    "ğŸ’¡Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.num_iterations = num_iterations\n",
    "        self.weight = None  \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\" Sigmoid function to map predictions to probabilities, with clipping to avoid overflow/underflow \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def add_intercept(self, X):\n",
    "        \"\"\" Add intercept bias term to the feature matrix X \"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\" Compute the cost (binary cross-entropy loss) \"\"\"\n",
    "        m = len(y)\n",
    "        predictions = self.sigmoid(np.dot(X, self.weight))\n",
    "\n",
    "        # Clip predictions to avoid log(0) or log(1) issues\n",
    "        epsilon = 1e-10  # Small value to clip predictions\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "\n",
    "        cost = (-y * np.log(predictions) - (1 - y) * np.log(1 - predictions)).mean()\n",
    "        return cost\n",
    "    \n",
    "    def gradient_descent(self, X, y):\n",
    "        \"\"\" Update the weights using gradient descent \"\"\"\n",
    "        m = len(y)\n",
    "        y_array = y.to_numpy().reshape(-1, 1) if isinstance(y, pd.Series) else y.reshape(-1, 1)\n",
    "        \n",
    "        predictions = self.sigmoid(np.dot(X, self.weight)) \n",
    "        error = predictions - y_array  \n",
    "        \n",
    "        gradient = (1 / m) * np.dot(X.T, error) \n",
    "        self.weight -= self.learning_rate * gradient\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\" Train the model using gradient descent \"\"\"\n",
    "        X = self.add_intercept(X)\n",
    "        \n",
    "        # self.weight = np.zeros((X.shape[1], 1))\n",
    "        self.weight = np.random.randn(X.shape[1], 1) * 0.01\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            self.gradient_descent(X, y)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                loss = self.compute_loss(X, y.to_numpy().reshape(-1, 1) if isinstance(y, pd.Series) else y)\n",
    "                print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\" Predict the labels based on learned weights \"\"\"\n",
    "        X = self.add_intercept(X)  \n",
    "        predictions = self.sigmoid(np.dot(X, self.weight)) \n",
    "        \n",
    "        return (predictions >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (4137, 3000)\n",
      "y_train (4137,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for X_train (non-zero values):\n",
      "Mean: 6.866895210469064\n",
      "Min: 1\n",
      "Max: 1751\n",
      "25th Percentile (Q1): 1.0\n",
      "50th Percentile (Median): 2.0\n",
      "75th Percentile (Q3): 4.0\n",
      "Deciles: [ 1.  1.  1.  1.  2.  2.  3.  5. 12.]\n",
      "\n",
      "Statistics for X_test (non-zero values):\n",
      "Mean: 7.005522794284418\n",
      "Min: 1\n",
      "Max: 2327\n",
      "25th Percentile (Q1): 1.0\n",
      "50th Percentile (Median): 2.0\n",
      "75th Percentile (Q3): 4.0\n",
      "Deciles: [ 1.  1.  1.  1.  2.  2.  3.  5. 13.]\n",
      "\n",
      "Random 5 non-zero values from X_train with their positions:\n",
      "Value: [9 8 1 ... 0 1 0] at row: 2393, column: 129\n",
      "Value: [1 2 2 ... 0 0 0] at row: 3420, column: 1683\n",
      "Value: [7 7 4 ... 0 0 0] at row: 1728, column: 344\n",
      "Value: [0 1 1 ... 0 1 0] at row: 472, column: 141\n",
      "Value: [ 8 13 24 ...  0  1  0] at row: 1326, column: 29\n",
      "\n",
      "Random 5 non-zero values from X_test with their positions:\n",
      "Value: [16  2  1 ...  0  2  0] at row: 297, column: 56\n",
      "Value: [3 5 1 ... 0 0 0] at row: 915, column: 108\n",
      "Value: [1 1 1 ... 0 2 0] at row: 982, column: 21\n",
      "Value: [4 4 4 ... 0 0 0] at row: 300, column: 54\n",
      "Value: [4 9 3 ... 0 0 0] at row: 490, column: 128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def show_statistics_and_random_cells(X_train, X_test, num_random_cells=5):\n",
    "    # Convert to numpy array for processing\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    \n",
    "    # Get non-zero values\n",
    "    non_zero_train = X_train_np[X_train_np != 0]\n",
    "    non_zero_test = X_test_np[X_test_np != 0]\n",
    "    \n",
    "    # Compute statistics for X_train\n",
    "    mean_train = np.mean(non_zero_train)\n",
    "    min_train = np.min(non_zero_train)\n",
    "    max_train = np.max(non_zero_train)\n",
    "    q25_train = np.percentile(non_zero_train, 25)\n",
    "    q50_train = np.percentile(non_zero_train, 50)  # Median\n",
    "    q75_train = np.percentile(non_zero_train, 75)\n",
    "    deciles_train = np.percentile(non_zero_train, np.arange(10, 100, 10))  # 10th, 20th, ..., 90th percentiles\n",
    "    \n",
    "    # Compute statistics for X_test\n",
    "    mean_test = np.mean(non_zero_test)\n",
    "    min_test = np.min(non_zero_test)\n",
    "    max_test = np.max(non_zero_test)\n",
    "    q25_test = np.percentile(non_zero_test, 25)\n",
    "    q50_test = np.percentile(non_zero_test, 50)  # Median\n",
    "    q75_test = np.percentile(non_zero_test, 75)\n",
    "    deciles_test = np.percentile(non_zero_test, np.arange(10, 100, 10))  # 10th, 20th, ..., 90th percentiles\n",
    "\n",
    "    # Display Summary Statistics\n",
    "    print(\"Statistics for X_train (non-zero values):\")\n",
    "    print(f\"Mean: {mean_train}\")\n",
    "    print(f\"Min: {min_train}\")\n",
    "    print(f\"Max: {max_train}\")\n",
    "    print(f\"25th Percentile (Q1): {q25_train}\")\n",
    "    print(f\"50th Percentile (Median): {q50_train}\")\n",
    "    print(f\"75th Percentile (Q3): {q75_train}\")\n",
    "    print(f\"Deciles: {deciles_train}\")\n",
    "    \n",
    "    print(\"\\nStatistics for X_test (non-zero values):\")\n",
    "    print(f\"Mean: {mean_test}\")\n",
    "    print(f\"Min: {min_test}\")\n",
    "    print(f\"Max: {max_test}\")\n",
    "    print(f\"25th Percentile (Q1): {q25_test}\")\n",
    "    print(f\"50th Percentile (Median): {q50_test}\")\n",
    "    print(f\"75th Percentile (Q3): {q75_test}\")\n",
    "    print(f\"Deciles: {deciles_test}\")\n",
    "    \n",
    "    # Show Random Non-Zero Values with their Row and Column Indices\n",
    "    print(f\"\\nRandom {num_random_cells} non-zero values from X_train with their positions:\")\n",
    "    for _ in range(num_random_cells):\n",
    "        rand_idx = np.random.choice(np.nonzero(X_train_np != 0)[0])  # Random row index of non-zero value\n",
    "        rand_value = X_train_np[rand_idx]\n",
    "        print(f\"Value: {rand_value} at row: {rand_idx}, column: {np.random.choice(np.nonzero(X_train_np[rand_idx] != 0)[0])}\")\n",
    "\n",
    "    print(f\"\\nRandom {num_random_cells} non-zero values from X_test with their positions:\")\n",
    "    for _ in range(num_random_cells):\n",
    "        rand_idx = np.random.choice(np.nonzero(X_test_np != 0)[0])  # Random row index of non-zero value\n",
    "        rand_value = X_test_np[rand_idx]\n",
    "        print(f\"Value: {rand_value} at row: {rand_idx}, column: {np.random.choice(np.nonzero(X_test_np[rand_idx] != 0)[0])}\")\n",
    "\n",
    "# Call function to show statistics and random values\n",
    "show_statistics_and_random_cells(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def min_max_normalize(X):\n",
    "#     \"\"\" Min-Max normalization for feature matrix X (scales each feature to [0, 1]) \"\"\"\n",
    "#     X_normalized = X.copy()  \n",
    "    \n",
    "#     if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
    "#         print(\"Warning: Data contains NaN or Inf values. These should be handled before normalization.\")\n",
    "    \n",
    "#     for column in range(X.shape[1]): \n",
    "#         min_value = np.min(X[:, column]) \n",
    "#         max_value = np.max(X[:, column]) \n",
    "        \n",
    "#         if min_value == max_value:\n",
    "#             print(f\"Warning: Column {column} is constant. Skipping normalization for this column.\")\n",
    "#             X_normalized[:, column] = 0 \n",
    "#         else:\n",
    "#             X_normalized[:, column] = (X[:, column] - min_value) / (max_value - min_value)\n",
    "    \n",
    "#     return X_normalized\n",
    "\n",
    "# X_train_normalized = min_max_normalize(X_train.to_numpy())\n",
    "# X_test_normalized = min_max_normalize(X_test.to_numpy()) \n",
    "\n",
    "# y_train_normalized = y_train.to_numpy()\n",
    "# y_test_normalized = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import numpy as np\n",
    "# import numpy as np\n",
    "\n",
    "# def compute_tfidf_matrix(X):\n",
    "#     def compute_tf(matrix):\n",
    "#         matrix = matrix.to_numpy()\n",
    "#         row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        \n",
    "#         row_sums[row_sums == 0] = 1\n",
    "        \n",
    "#         return matrix / row_sums\n",
    "    \n",
    "#     tf = compute_tf(X)\n",
    "\n",
    "#     def compute_idf(matrix):\n",
    "#         N = matrix.shape[0]\n",
    "        \n",
    "#         df = np.count_nonzero(matrix, axis=0)\n",
    "#         idf = np.log((N + 1) / (df + 1e-6))\n",
    "        \n",
    "#         return idf\n",
    "    \n",
    "#     idf = compute_idf(X)\n",
    "#     tfidf = tf * idf \n",
    "    \n",
    "#     return tfidf\n",
    "\n",
    "\n",
    "\n",
    "# X_train_normalized = compute_tfidf_matrix(X_train)\n",
    "# X_test_normalized = compute_tfidf_matrix(X_test)\n",
    "\n",
    "# y_train_normalized = y_train.to_numpy()\n",
    "# y_test_normalized = y_test.to_numpy()\n",
    "\n",
    "# print(\"TF-IDF for training data shape:\", X_train_normalized.shape)\n",
    "# print(\"TF-IDF for test data shape:\", X_test_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom scaled training data shape: (4137, 3000)\n",
      "Custom scaled test data shape: (1035, 3000)\n",
      "Sample values from the scaled training matrix:\n",
      "the    1.180509\n",
      "to     0.745385\n",
      "ect    0.140553\n",
      "and    0.327355\n",
      "for    0.428016\n",
      "of     0.636987\n",
      "a      0.432510\n",
      "you    0.000000\n",
      "hou    0.142905\n",
      "in     0.467967\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def custom_standard_scaler(X):\n",
    "\n",
    "    X_np = X.to_numpy()\n",
    "    \n",
    "    std_devs = np.std(X_np, axis=0)\n",
    "    \n",
    "    std_devs[std_devs == 0] = 1\n",
    "    \n",
    "    X_scaled_np = X_np / std_devs\n",
    "    \n",
    "    X_scaled_df = pd.DataFrame(X_scaled_np, columns=X.columns)\n",
    "    \n",
    "    return X_scaled_df\n",
    "\n",
    "X_train_normalized = custom_standard_scaler(X_train)\n",
    "X_test_normalized = custom_standard_scaler(X_test)\n",
    "\n",
    "y_train_normalized = y_train.to_numpy()\n",
    "y_test_normalized = y_test.to_numpy()\n",
    "\n",
    "print(\"Custom scaled training data shape:\", X_train_normalized.shape)\n",
    "print(\"Custom scaled test data shape:\", X_test_normalized.shape)\n",
    "\n",
    "print(\"Sample values from the scaled training matrix:\")\n",
    "print(X_train_normalized.iloc[0, :10]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           the        to       ect       and       for        of         a  \\\n",
      "0     0.354962  0.000000  0.073710  0.691862  0.637090  0.000000  0.336134   \n",
      "1     0.177481  0.000000  0.073710  0.000000  0.212363  0.000000  0.094538   \n",
      "2     0.887405  0.396793  0.294840  0.345931  0.212363  0.332031  0.315125   \n",
      "3     0.798664  1.091182  0.294840  0.172966  1.061817  0.332031  0.367646   \n",
      "4     0.000000  0.000000  0.073710  0.000000  0.000000  0.000000  0.042017   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1030  1.508588  2.579156  0.073710  0.864828  0.849453  0.830077  1.050418   \n",
      "1031  0.000000  0.297595  0.073710  0.172966  0.212363  0.000000  0.094538   \n",
      "1032  1.064886  1.785570  0.589680  1.902621  2.548360  0.830077  1.533610   \n",
      "1033  0.887405  0.595190  7.076158  1.383724  1.061817  0.332031  1.292014   \n",
      "1034  0.088740  0.793587  0.147420  0.172966  0.212363  0.000000  0.745796   \n",
      "\n",
      "           you       hou        in  ...  enhancements  connevey  jay  valued  \\\n",
      "0     0.203725  0.146152  0.308282  ...           0.0       0.0  0.0     0.0   \n",
      "1     0.000000  0.000000  0.000000  ...           0.0       0.0  0.0     0.0   \n",
      "2     1.426078  0.000000  0.513803  ...           0.0       0.0  0.0     0.0   \n",
      "3     0.203725  0.292304  0.462422  ...           0.0       0.0  0.0     0.0   \n",
      "4     0.203725  0.000000  0.102761  ...           0.0       0.0  0.0     0.0   \n",
      "...        ...       ...       ...  ...           ...       ...  ...     ...   \n",
      "1030  3.667058  0.146152  0.873464  ...           0.0       0.0  0.0     0.0   \n",
      "1031  0.407451  0.000000  0.256901  ...           0.0       0.0  0.0     0.0   \n",
      "1032  1.833529  0.292304  1.490028  ...           0.0       0.0  0.0     0.0   \n",
      "1033  0.000000  7.161455  1.130366  ...           0.0       0.0  0.0     0.0   \n",
      "1034  1.018627  0.000000  0.822084  ...           0.0       0.0  0.0     0.0   \n",
      "\n",
      "      lay  infrastructure  military  allowing        ff        dry  \n",
      "0     0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "1     0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "2     0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "3     0.0             0.0       0.0       0.0  1.807386  14.422373  \n",
      "4     0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "...   ...             ...       ...       ...       ...        ...  \n",
      "1030  0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "1031  0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "1032  0.0             0.0       0.0       0.0  1.204924   0.000000  \n",
      "1033  0.0             0.0       0.0       0.0  0.602462   0.000000  \n",
      "1034  0.0             0.0       0.0       0.0  0.000000   0.000000  \n",
      "\n",
      "[1035 rows x 3000 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in X_train: [0.00000000e+00 8.05204223e-03 1.06301387e-02 ... 6.43124331e+01\n",
      " 6.43272900e+01 6.43272900e+01]\n",
      "Unique values in X_test: [0.00000000e+00 7.30691309e-03 1.02397867e-02 ... 3.21869689e+01\n",
      " 3.21869689e+01 3.21869689e+01]\n",
      "\n",
      "Statistics for non-zero values in X_train (normalized):\n",
      "Mean: 2.384374675841528\n",
      "Min: 0.00805204222572266\n",
      "Max: 64.32729002359339\n",
      "25th Percentile (Q1): 0.4449446678347091\n",
      "50th Percentile (Median): 1.1552066453590224\n",
      "75th Percentile (Q3): 2.8104587859212127\n",
      "Deciles: [0.19182453 0.35933342 0.55951759 0.8157506  1.15520665 1.65159367\n",
      " 2.30516343 3.51923154 5.933692  ]\n",
      "\n",
      "Statistics for non-zero values in X_test (normalized):\n",
      "Mean: 2.402391723465178\n",
      "Min: 0.0073069130872424615\n",
      "Max: 32.186968902305615\n",
      "25th Percentile (Q1): 0.44052182307986004\n",
      "50th Percentile (Median): 1.1583168282148109\n",
      "75th Percentile (Q3): 2.8356856152423595\n",
      "Deciles: [0.18571327 0.34685315 0.53565893 0.80439351 1.15831683 1.6175175\n",
      " 2.30109625 3.49116695 6.04797964]\n",
      "\n",
      "Zero Value Statistics in X_train (normalized):\n",
      "Total Zero Count: 11713249\n",
      "Percentage of Zeros: 94.38%\n",
      "\n",
      "Zero Value Statistics in X_test (normalized):\n",
      "Total Zero Count: 2928640\n",
      "Percentage of Zeros: 94.32%\n",
      "\n",
      "Random 5 non-zero values from X_train with their positions:\n",
      "Value: 1.1623049240453973 at row: 3444, column: 720\n",
      "Value: 5.358873334287569 at row: 3518, column: 93\n",
      "Value: 1.6854737335621064 at row: 758, column: 1705\n",
      "Value: 1.0922993954066065 at row: 552, column: 166\n",
      "Value: 0.20895581726270065 at row: 464, column: 1321\n",
      "\n",
      "Random 5 non-zero values from X_test with their positions:\n",
      "Value: 0.0953218691583122 at row: 658, column: 45\n",
      "Value: 2.2188955119326974 at row: 1007, column: 2523\n",
      "Value: 0.3453855118766131 at row: 709, column: 2669\n",
      "Value: 0.20145748035650143 at row: 223, column: 2647\n",
      "Value: 5.288188746465332 at row: 60, column: 1346\n",
      "Unique values in X_train: [0.00000000e+00 8.05204223e-03 1.06301387e-02 ... 6.43124331e+01\n",
      " 6.43272900e+01 6.43272900e+01]\n",
      "Unique values in X_test: [0.00000000e+00 7.30691309e-03 1.02397867e-02 ... 3.21869689e+01\n",
      " 3.21869689e+01 3.21869689e+01]\n",
      "\n",
      "Statistics for non-zero values in X_train (normalized):\n",
      "Mean: 2.384374675841528\n",
      "Min: 0.00805204222572266\n",
      "Max: 64.32729002359339\n",
      "25th Percentile (Q1): 0.4449446678347091\n",
      "50th Percentile (Median): 1.1552066453590224\n",
      "75th Percentile (Q3): 2.8104587859212127\n",
      "Deciles: [0.19182453 0.35933342 0.55951759 0.8157506  1.15520665 1.65159367\n",
      " 2.30516343 3.51923154 5.933692  ]\n",
      "\n",
      "Statistics for non-zero values in X_test (normalized):\n",
      "Mean: 2.402391723465178\n",
      "Min: 0.0073069130872424615\n",
      "Max: 32.186968902305615\n",
      "25th Percentile (Q1): 0.44052182307986004\n",
      "50th Percentile (Median): 1.1583168282148109\n",
      "75th Percentile (Q3): 2.8356856152423595\n",
      "Deciles: [0.18571327 0.34685315 0.53565893 0.80439351 1.15831683 1.6175175\n",
      " 2.30109625 3.49116695 6.04797964]\n",
      "\n",
      "Zero Value Statistics in X_train (normalized):\n",
      "Total Zero Count: 11713249\n",
      "Percentage of Zeros: 94.38%\n",
      "\n",
      "Zero Value Statistics in X_test (normalized):\n",
      "Total Zero Count: 2928640\n",
      "Percentage of Zeros: 94.32%\n",
      "\n",
      "Random 5 non-zero values from X_train with their positions:\n",
      "Value: 0.051517299247007195 at row: 1990, column: 14\n",
      "Value: 1.385714646629103 at row: 3946, column: 22\n",
      "Value: 0.6190008061542932 at row: 1667, column: 810\n",
      "Value: 0.3180851583237191 at row: 2926, column: 173\n",
      "Value: 1.0776418593801285 at row: 1868, column: 184\n",
      "\n",
      "Random 5 non-zero values from X_test with their positions:\n",
      "Value: 0.460880550927902 at row: 124, column: 72\n",
      "Value: 1.4059975700851048 at row: 582, column: 45\n",
      "Value: 0.35780121793376407 at row: 297, column: 1445\n",
      "Value: 1.5964012392729818 at row: 669, column: 109\n",
      "Value: 0.7703025376171238 at row: 72, column: 462\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_statistics_and_random_cells_normalized(X_train, X_test, num_random_cells=5):\n",
    "    # Convert to numpy arrays for processing\n",
    "    X_train_np = np.array(X_train)  # Ensure this is a numpy array\n",
    "    X_test_np = np.array(X_test)\n",
    "    \n",
    "    # Get non-zero and zero values for X_train\n",
    "    non_zero_train = X_train_np[X_train_np != 0]\n",
    "    zero_train = X_train_np[X_train_np == 0]\n",
    "    \n",
    "    # Get non-zero and zero values for X_test\n",
    "    non_zero_test = X_test_np[X_test_np != 0]\n",
    "    zero_test = X_test_np[X_test_np == 0]\n",
    "    \n",
    "    # Check if the data has only 0s and 1s or if it's normalized as expected\n",
    "    unique_train_vals = np.unique(X_train_np)\n",
    "    unique_test_vals = np.unique(X_test_np)\n",
    "    print(f\"Unique values in X_train: {unique_train_vals}\")\n",
    "    print(f\"Unique values in X_test: {unique_test_vals}\")\n",
    "    \n",
    "    # Compute statistics for non-zero values in X_train\n",
    "    if non_zero_train.size > 0:\n",
    "        mean_train_non_zero = np.mean(non_zero_train)\n",
    "        min_train_non_zero = np.min(non_zero_train)\n",
    "        max_train_non_zero = np.max(non_zero_train)\n",
    "        q25_train_non_zero = np.percentile(non_zero_train, 25)\n",
    "        q50_train_non_zero = np.percentile(non_zero_train, 50)  # Median\n",
    "        q75_train_non_zero = np.percentile(non_zero_train, 75)\n",
    "        deciles_train_non_zero = np.percentile(non_zero_train, np.arange(10, 100, 10))  # 10th, 20th, ..., 90th percentiles\n",
    "    else:\n",
    "        mean_train_non_zero = min_train_non_zero = max_train_non_zero = q25_train_non_zero = q50_train_non_zero = q75_train_non_zero = deciles_train_non_zero = \"N/A\"\n",
    "    \n",
    "    # Compute statistics for non-zero values in X_test\n",
    "    if non_zero_test.size > 0:\n",
    "        mean_test_non_zero = np.mean(non_zero_test)\n",
    "        min_test_non_zero = np.min(non_zero_test)\n",
    "        max_test_non_zero = np.max(non_zero_test)\n",
    "        q25_test_non_zero = np.percentile(non_zero_test, 25)\n",
    "        q50_test_non_zero = np.percentile(non_zero_test, 50)  # Median\n",
    "        q75_test_non_zero = np.percentile(non_zero_test, 75)\n",
    "        deciles_test_non_zero = np.percentile(non_zero_test, np.arange(10, 100, 10))  # 10th, 20th, ..., 90th percentiles\n",
    "    else:\n",
    "        mean_test_non_zero = min_test_non_zero = max_test_non_zero = q25_test_non_zero = q50_test_non_zero = q75_test_non_zero = deciles_test_non_zero = \"N/A\"\n",
    "    \n",
    "    # Compute statistics for zero values in X_train\n",
    "    zero_train_count = zero_train.size\n",
    "    zero_train_percentage = (zero_train_count / X_train_np.size) * 100\n",
    "    \n",
    "    # Compute statistics for zero values in X_test\n",
    "    zero_test_count = zero_test.size\n",
    "    zero_test_percentage = (zero_test_count / X_test_np.size) * 100\n",
    "    \n",
    "    # Display Summary Statistics for non-zero values\n",
    "    print(\"\\nStatistics for non-zero values in X_train (normalized):\")\n",
    "    print(f\"Mean: {mean_train_non_zero}\")\n",
    "    print(f\"Min: {min_train_non_zero}\")\n",
    "    print(f\"Max: {max_train_non_zero}\")\n",
    "    print(f\"25th Percentile (Q1): {q25_train_non_zero}\")\n",
    "    print(f\"50th Percentile (Median): {q50_train_non_zero}\")\n",
    "    print(f\"75th Percentile (Q3): {q75_train_non_zero}\")\n",
    "    print(f\"Deciles: {deciles_train_non_zero}\")\n",
    "    \n",
    "    print(\"\\nStatistics for non-zero values in X_test (normalized):\")\n",
    "    print(f\"Mean: {mean_test_non_zero}\")\n",
    "    print(f\"Min: {min_test_non_zero}\")\n",
    "    print(f\"Max: {max_test_non_zero}\")\n",
    "    print(f\"25th Percentile (Q1): {q25_test_non_zero}\")\n",
    "    print(f\"50th Percentile (Median): {q50_test_non_zero}\")\n",
    "    print(f\"75th Percentile (Q3): {q75_test_non_zero}\")\n",
    "    print(f\"Deciles: {deciles_test_non_zero}\")\n",
    "    \n",
    "    # Display Statistics for Zero Values\n",
    "    print(f\"\\nZero Value Statistics in X_train (normalized):\")\n",
    "    print(f\"Total Zero Count: {zero_train_count}\")\n",
    "    print(f\"Percentage of Zeros: {zero_train_percentage:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nZero Value Statistics in X_test (normalized):\")\n",
    "    print(f\"Total Zero Count: {zero_test_count}\")\n",
    "    print(f\"Percentage of Zeros: {zero_test_percentage:.2f}%\")\n",
    "    \n",
    "    # Show Random Non-Zero Values with their Row and Column Indices\n",
    "    print(f\"\\nRandom {num_random_cells} non-zero values from X_train with their positions:\")\n",
    "    for _ in range(num_random_cells):\n",
    "        rand_row_idx = np.random.choice(np.nonzero(X_train_np != 0)[0])  # Random row index of non-zero value\n",
    "        rand_col_idx = np.random.choice(np.nonzero(X_train_np[rand_row_idx, :] != 0)[0])  # Random column index from the row\n",
    "        rand_value = X_train_np[rand_row_idx, rand_col_idx]\n",
    "        print(f\"Value: {rand_value} at row: {rand_row_idx}, column: {rand_col_idx}\")\n",
    "\n",
    "    print(f\"\\nRandom {num_random_cells} non-zero values from X_test with their positions:\")\n",
    "    for _ in range(num_random_cells):\n",
    "        rand_row_idx = np.random.choice(np.nonzero(X_test_np != 0)[0])  # Random row index of non-zero value\n",
    "        rand_col_idx = np.random.choice(np.nonzero(X_test_np[rand_row_idx, :] != 0)[0])  # Random column index from the row\n",
    "        rand_value = X_test_np[rand_row_idx, rand_col_idx]\n",
    "        print(f\"Value: {rand_value} at row: {rand_row_idx}, column: {rand_col_idx}\")\n",
    "    \n",
    "# Call function to show statistics and random values for normalized data\n",
    "show_statistics_and_random_cells_normalized(X_train_normalized, X_test_normalized)\n",
    "\n",
    "\n",
    "# Call function to show statistics and random values for normalized data\n",
    "show_statistics_and_random_cells_normalized(X_train_normalized, X_test_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_reg = LogisticRegression(learning_rate=0.01, num_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6995000597488459\n",
      "Iteration 100, Loss: 1.059710112148476\n",
      "Iteration 200, Loss: 1.2964155516733298\n",
      "Iteration 300, Loss: 1.466183571871725\n",
      "Iteration 400, Loss: 1.600024272033826\n",
      "Iteration 500, Loss: 1.7111649950124397\n",
      "Iteration 600, Loss: 1.8065567247810512\n",
      "Iteration 700, Loss: 1.890326582814611\n",
      "Iteration 800, Loss: 1.9652754227373685\n",
      "Iteration 900, Loss: 2.033069298854147\n",
      "Iteration 1000, Loss: 2.095064643599596\n",
      "Iteration 1100, Loss: 2.152158521492603\n",
      "Iteration 1200, Loss: 2.2050088886201626\n",
      "Iteration 1300, Loss: 2.2543727462944565\n",
      "Iteration 1400, Loss: 2.3007080691138717\n",
      "Iteration 1500, Loss: 2.3443308273991335\n",
      "Iteration 1600, Loss: 2.3856226577593778\n",
      "Iteration 1700, Loss: 2.4246615878658275\n",
      "Iteration 1800, Loss: 2.4617452580347963\n",
      "Iteration 1900, Loss: 2.4970755778207256\n",
      "Iteration 2000, Loss: 2.530779096237171\n",
      "Iteration 2100, Loss: 2.563011162569264\n",
      "Iteration 2200, Loss: 2.5938959403393533\n",
      "Iteration 2300, Loss: 2.6235817094237657\n",
      "Iteration 2400, Loss: 2.652135984015168\n",
      "Iteration 2500, Loss: 2.679605635844548\n",
      "Iteration 2600, Loss: 2.7060760017807866\n",
      "Iteration 2700, Loss: 2.7316611948600955\n",
      "Iteration 2800, Loss: 2.7564311843040787\n",
      "Iteration 2900, Loss: 2.7803478512963515\n"
     ]
    }
   ],
   "source": [
    "model_logistic_reg.train(X=X_train_normalized, y=y_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_logistic = model_logistic_reg.predict(X=X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: [0.97584541]\n",
      "precision: 0.9831081081081081\n",
      "recall: 0.94\n",
      "f1-socre: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\",accuracy(y_test_normalized, y_predictions_logistic))\n",
    "print(\"precision:\", precision(y_test_normalized, y_predictions_logistic))\n",
    "print('recall:', f\"{recall(y_test_normalized, y_predictions_logistic):.2f}\")\n",
    "print(\"f1-socre:\", f\"{f1_score(y_test_normalized, y_predictions_logistic):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Multinomial Naive Bayes Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø± Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ú†Ø±Ø§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ù…Ø§Ù†Ù†Ø¯ Gaussian Naive Bayes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯ÛŒÙ…ØŸ Ø¢ÛŒØ§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ØŸ\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Appendix Ú©ØªØ§Ø¨ jurafsky Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø§Ø³ÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§Ù„Øª Guassian Naive Bayes Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¯ÛŒØªØ§ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… ÛŒØ§ Ø¯ÛŒØªØ§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø±ÛŒÙ… Ùˆ ÙˆÛŒØ²Ú¯ÛŒ Ù‡Ø§ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ ØªØ§ Ø­Ø¯ÛŒ Ù¾ÛŒØ±ÙˆÛŒ Ø¨Ú©Ù†Ù†Ø¯ Ùˆ Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ù‡Ø§ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ù…Ø§ Ø§Ø² G-NaiveBayes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. Ø¯Ø± Ø­Ø§Ù„Øª Ù…Ø§ Ù‡Ù… Ø¯ÛŒØªØ§ Ú¯Ø³Ø³ØªÙ‡ Ø¯Ø§Ø±ÛŒÙ… Ùˆ Ø¯Ø±Ú©Ù„ Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§ÛŒ Ø´Ù…Ø§Ø±Ø´ÛŒ Ù…ÙˆØ±Ø¯ MultiNomial Naive Bayes Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒÚ©Ù†Ø¯ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¬Ù‡Øª Ù…Ø§ Ø§Ø²ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_probs = None\n",
    "        self.word_probs = None\n",
    "        self.vocab_size = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.vocab_size = n\n",
    "        \n",
    "        class_counts = np.bincount(y)\n",
    "        self.class_probs = class_counts / m\n",
    "        \n",
    "        word_counts = np.zeros((len(self.class_probs), n))\n",
    "        \n",
    "        for i in range(len(self.class_probs)):\n",
    "            word_counts[i] = X[y == i].sum(axis=0)\n",
    "        \n",
    "        # Apply Laplace smoothing \n",
    "        smoothed_word_counts = word_counts + 1\n",
    "        \n",
    "        # Normalize by the sum of word counts in each class \n",
    "        smoothed_word_counts = smoothed_word_counts / smoothed_word_counts.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.word_probs = smoothed_word_counts\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_class_probs = np.log(self.class_probs)\n",
    "        log_word_probs = np.log(self.word_probs)\n",
    "        \n",
    "        log_probs = X.dot(log_word_probs.T) + log_class_probs\n",
    "        \n",
    "        return np.argmax(log_probs, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        log_class_probs = np.log(self.class_probs)\n",
    "        log_word_probs = np.log(self.word_probs)\n",
    "        \n",
    "        log_probs = X.dot(log_word_probs.T) + log_class_probs\n",
    "        \n",
    "        return np.exp(log_probs) / np.sum(np.exp(log_probs), axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive_bayes = MultinomialNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4137, 3000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive_bayes.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_naive = model_naive_bayes.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9410628019323671\n",
      "precision: 0.874251497005988\n",
      "recall: 0.94\n",
      "f1-socre: 0.91\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\",accuracy(y_test.to_numpy(), y_predictions_naive))\n",
    "print(\"precision:\", precision(y_test.to_numpy(), y_predictions_naive))\n",
    "print('recall:', f\"{recall(y_test.to_numpy(), y_predictions_naive):.2f}\")\n",
    "print(\"f1-socre:\", f\"{f1_score(y_test.to_numpy(), y_predictions_naive):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"> Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªØ§ÛŒØ¬ Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¨Ø± Ù†ØªØ§ÛŒØ¬ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.<br>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - accuracy: 0.9410628019323671\n",
      "Naive Bayes - precision: 0.874251497005988\n",
      "Naive Bayes - recall: 0.94\n",
      "Naive Bayes - f1-score: 0.91\n",
      "\n",
      "Logistic Regression - accuracy: 0.9758454106280193\n",
      "Logistic Regression - precision: 0.9831081081081081\n",
      "Logistic Regression - recall: 0.94\n",
      "Logistic Regression - f1-score: 0.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions and metrics for Naive Bayes\n",
    "y_predictions_naive = model_naive_bayes.predict(X=X_test)\n",
    "accuracy_naive = accuracy(y_test.to_numpy(), y_predictions_naive)\n",
    "precision_naive = precision(y_test.to_numpy(), y_predictions_naive)\n",
    "recall_naive = recall(y_test.to_numpy(), y_predictions_naive)\n",
    "f1_naive = f1_score(y_test.to_numpy(), y_predictions_naive)\n",
    "\n",
    "print(\"Naive Bayes - accuracy:\", accuracy_naive)\n",
    "print(\"Naive Bayes - precision:\", precision_naive)\n",
    "print(\"Naive Bayes - recall:\", f\"{recall_naive:.2f}\")\n",
    "print(\"Naive Bayes - f1-score:\", f\"{f1_naive:.2f}\")\n",
    "print()\n",
    "\n",
    "# Predictions and metrics for Logistic Regression\n",
    "y_predictions_logistic = model_logistic_reg.predict(X=X_test_normalized)\n",
    "accuracy_logistic = accuracy(y_test_normalized, y_predictions_logistic)\n",
    "precision_logistic = precision(y_test_normalized, y_predictions_logistic)\n",
    "recall_logistic = recall(y_test_normalized, y_predictions_logistic)\n",
    "f1_logistic = f1_score(y_test_normalized, y_predictions_logistic)\n",
    "\n",
    "print(\"Logistic Regression - accuracy:\", *accuracy_logistic)\n",
    "print(\"Logistic Regression - precision:\", precision_logistic)\n",
    "print(\"Logistic Regression - recall:\", f\"{recall_logistic:.2f}\")\n",
    "print(\"Logistic Regression - f1-score:\", f\"{f1_logistic:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø´Ø´Ù…:</b><br>\n",
    "Ù…Ø¯Ù„ logisitic regression Ø¯Ø± ØªÙ…Ø§Ù…ÛŒ Ù…ØªØ±ÛŒÚ© Ù‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª Ø¨Ù‡ØªØ± Ø§Ø² Multinomial Naive Bayes Ø¹Ù…Ù„ Ú©Ø±Ø¯. Ù‡Ù…Ø¬Ù†ÛŒÙ† Ø¯Ø± Ù…Ø¯Ù„ naive bayes Ù…Ø§ ÛŒÚ© ÙØ±Ø¶ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡ÛŒÙ… Ú©Ù‡ Ù…Ø¯Ù„ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ Ø¢Ù† Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ú©Ù„Ù…Ø§Øª Ù…Ø³ØªÙ‚Ù„ Ù‡Ø³ØªÙ†Ø¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡Ù…Ø¯ÛŒÚ¯Ø± Ø¨Ù‡ Ø´Ø±Ø· Ø¯Ø§Ù†Ø³ØªÙ† Ú©Ù„Ø§Ø³ Ø¢Ù† Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† ÙØ±Ø¶ÛŒÙ‡ Ø¯Ø± ÙˆØ§Ù‚Ø¹ÛŒØª Ø·Ø¨ÛŒØ¹ØªØ§ Ø¯Ø±Ø³Øª Ù†ÛŒØ³Øª. Ø§Ù…Ø§ Ø¯Ø± logisitc regression Ø§ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡Ù… ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ø¨Ø§ ÙˆØ²Ù† Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§ÛŒÙ† Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯. Ø¯Ø± Ù…Ø¯Ù„ naive Ø¨Ø±Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ù…Ø§Øª Ùˆ ÙØ±Ú©Ø§Ù†Ø³ Ù‡Ø§Ø´ÙˆÙ† ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØªÙ‡ Ù…ÛŒØ´Ù‡ Ø§Ù…Ø§ Ø¯Ø± Ø­Ø§Ù„Øª logisitc Ø§Ø±ØªØ¨Ø§Ø· Ú©Ù„Ù…Ø§Øª Ø¨Ø§Ù‡Ù… Ø¯Ø± Ù…Ø¯Ù„ Ø¨Ù‡ Ù†ÙˆØ¹ÛŒ Ù…Ø¯Ù„ Ù…ÛŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø­Ø¯ÛŒ. Ù…ÙˆØ±Ø¯ Ø¯ÛŒÚ¯Ø± Ø¨Ø­Ø« Ø¨Ø§Ù„Ø§Ù†Ø³ Ù†Ø¨ÙˆØ¯Ù† Ú©Ù„Ø§Ø³ Ù‡Ø§ Ø§Ø³Øª Ú©Ù‡ Ù…Ø¯Ù„ Logistic Ø¨Ù‡ØªØ± Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¯ÛŒÚ¯Ø±ÛŒ Ù‡Ù†Ø¯Ù„ Ù…ÛŒÚ©Ù†Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">Ù‡Ø¯Ù Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Logistic Regression Ùˆ Naive Bayes Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ (Ù…Ø¹ØªØ¨Ø±) Ø§Ø³Øª.\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÛŒ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ (Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ Ø®Ø§Ù…)ØŒ ÛŒÚ© Ø³Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ø§ÛŒÙ† URLÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ ØªØ£Ø«ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.<br>Ù†Ú©ØªÙ‡: Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨â€ŒØ®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ (nb_dots)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ (nb_slashes)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø®Ø· â€ŒØªÛŒØ±Ù‡â€ŒÙ‡Ø§ (nb_hyphens)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯\n",
    "<br>\n",
    "<br>\n",
    "- Ø¢ÛŒØ§ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ØŸ Ú†Ø±Ø§ØŸ Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Â«Ø¨Ù„Ù‡Â» Ø§Ø³ØªØŒ Ù†ÙˆØ¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¢Ù† Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø² Ø¨ÛŒÙ† Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Naive Bayes (GaussianNB ÛŒØ§ MultinomialNB) Ú©Ø¯Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ Ø¯Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ø¨Ø§ Ø¢Ù† Ù…Ø¯Ù„ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ… Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Guassian Naive Bayes Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¢Ù† Ø§Ù…ÙˆØ²Ø´ Ù…ÛŒØ¨ÛŒÙ†Ø¯ Ø¯ÛŒÚ¯Ø± Ø§Ø¹Ø¯Ø§Ø¯ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø± Ø§Ø¹Ø¯Ø§Ø¯ Ú©ÙˆÚ†ÛŒÚ© ØªØ± Ú†ÛŒØ±Ù‡ Ù†Ù…ÛŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø§Ù†Ù‡Ø§ Ø¯ÛŒØ¯Ù‡ Ù†Ø´ÙˆÙ†Ø¯ Ø¨Ù„Ú©Ù‡ Ú†ÙˆÙ† Ù‡Ø± Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯Ø´ Ø§Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø§Ø² Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯. Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ ØªØ§Ø«ÛŒØ±ÛŒ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„. Ø¯Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ù†ÙˆØ¹ naive bayes Ù‡Ù… Ø²ÛŒØ±Ø§ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ú©Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒÚ©Ø±Ø¯ÛŒÙ… Ø¯Ø±Ø³Øª Ø§Ø³Øª Ø¯Ø§Ø±ÛŒÙ… ØªØ¹Ø¯Ø§Ø¯ Ù…ÛŒØ´Ù…Ø§Ø±ÛŒÙ… Ø§Ù…Ø§ Ø­Ø¯ÙˆØ¯Ø§ Ù…ÛŒØªÙˆØ§Ù† Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ø®ÙˆØ¨ÛŒ Ú¯ÙØª ØªÙ‚Ø±ÛŒØ¨Ø§ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù¾ÛŒØ±ÙˆÛŒ Ù…ÛŒÚ©Ù†Ù†Ø¯ ÙÙ‚Ø· Ø¯Ø±Ø³Øª Ø§Ø³Øª Ú©Ù‡ Ú¯Ø³Ø³ØªÙ‡ Ù‡Ø³ØªÙ†Ø¯ Ø§Ù…Ø§ Ø´Ù…Ø§Ø±Ù‡ Ù‡Ø§ÛŒ Ø¢Ù† Ù‡Ù…Ù‡ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ù‡Ù… Ø§Ø³Øª ØªØ§ Ø­Ø¯ÛŒ Ùˆ Ù…Ø§Ù†Ù†Ø¯ Ø­Ø§Ù„Øª Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ Ù†ÛŒØ³Øª Ú©Ù‡ ÛŒÚ© Ø³Ø±ÛŒ Ú©Ù„Ù…Ø§Øª Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ Ùˆ Ø¯ÛŒÚ¯Ø±ÛŒ ØµÙØ± Ø¨Ø§Ø± Ø§Ù…Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÙÙ‡ÙˆÙ… Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ø±ÙˆÛŒ Ø§Ø¹Ø¯Ø§Ø¯ Ú¯Ø³Ø³ØªÙ‡ Ù…Ø·Ø±Ø­ Ø§Ø³Øª.\n",
    "Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Logisitic regression Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø­Ø³Ø§Ø³ Ù‡Ø³Øª Ù…Ø¯Ù„ Ø¨Ù‡ Ø§Ø³Ú©ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø¨Ø²Ø±Ú¯ÛŒ Ù‡Ø± Ø¹Ø¯Ø¯Ø¯ ØªØ§Ø«ÛŒØ±ÛŒ Ø®Ø§ØµÛŒ Ø¨Ø± Ø¯Ø± ØµØ±Ø¨ Ú©Ø±Ø¯Ù† Ø¯Ø± ÙˆØ²Ù† Ù‡Ø§ÛŒ Ù…Ø¯Ù„ Ù…ÛŒÚ¯Ø°Ø§Ø±Ø¯ Ù¾Ø³ Ø¨Ø§ÛŒØ¯ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ ØªØ§ ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø± Ø¯ÛŒÚ¯Ø±ÛŒ Ø³Ù„Ø·Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ù†Ø¯ Ùˆ Ù‡Ø±ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø± Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø®ÙˆØ¯ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ø¨Ø§Ø´Ø¯ Ùˆ Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø§Ø³Øª ØªÙˆØ¬Ù‡ Ú©Ù…ØªØ± ÛŒØ§ Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ø¢Ù† Ø´ÙˆØ¯ Ø¯Ø± Ø§Ø®Ù…ÙˆØ²Ø´ ÙˆØ²Ù† Ù‡Ø§ Ø§ÛŒÙ† Ø§ØªÙØ§Ù‚ Ø®ÙˆØ§Ù‡Ø¯ Ø§ÙØªØ§Ø¯. Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø¯Ù„ÛŒÙ„ ØªÙ‚Ø±ÛŒØ¨Ø§ ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù† Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø§Ø² Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "\n",
    "path_q2 = os.path.join(\"datasets\", \"q2\", \"urls.csv\")\n",
    "df = pd.read_csv(path_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.crestonwood.com/router.php</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://shadetreetechnology.com/V4/validation/a...</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://support-appleld.com.secureupdate.duila...</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://rgipt.ac.in</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.iracing.com/tracks/gateway-motorspo...</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url      status\n",
       "0              http://www.crestonwood.com/router.php  legitimate\n",
       "1  http://shadetreetechnology.com/V4/validation/a...    phishing\n",
       "2  https://support-appleld.com.secureupdate.duila...    phishing\n",
       "3                                 http://rgipt.ac.in  legitimate\n",
       "4  http://www.iracing.com/tracks/gateway-motorspo...  legitimate"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11430 entries, 0 to 11429\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     11430 non-null  object\n",
      " 1   status  11430 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 178.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11430</td>\n",
       "      <td>11430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11429</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>http://e710z0ear.du.r.appspot.com/c:/users/use...</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>5715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url      status\n",
       "count                                               11430       11430\n",
       "unique                                              11429           2\n",
       "top     http://e710z0ear.du.r.appspot.com/c:/users/use...  legitimate\n",
       "freq                                                    2        5715"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuralFeatures:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def num_dots(self, X):\n",
    "        return X.str.count(r'\\.')\n",
    "\n",
    "    def num_slashes(self, X):\n",
    "        return X.str.count(r'/')\n",
    "\n",
    "    def num_hyphens(self, X):\n",
    "        return X.str.count(r'-')\n",
    "\n",
    "    def extract_features(self, X):  # X is df['url']\n",
    "        dots = self.num_dots(X)\n",
    "        slashes = self.num_slashes(X)\n",
    "        hyphens = self.num_hyphens(X)\n",
    "        return pd.DataFrame({\n",
    "            'nb_dots': dots,\n",
    "            'nb_slashes': slashes,\n",
    "            'nb_hyphens': hyphens\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "structuralFeatures = StructuralFeatures()\n",
    "df_features_structural = structuralFeatures.extract_features(df['url'])\n",
    "df_label = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
    "\n",
    "df_features_label = df_features_structural.copy()\n",
    "df_features_label['status'] = df_label.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features_structural, df_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes - accuracy: 0.68\n",
      "Gaussian Naive Bayes - precision: 0.83\n",
      "Gaussian Naive Bayes - recall: 0.46\n",
      "Gaussian Naive Bayes - f1-score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_gaussianNB = GaussianNB()\n",
    "\n",
    "model_gaussianNB.fit(X_train, y_train)\n",
    "\n",
    "y_predictions_naive = model_gaussianNB.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_naive = accuracy(y_test.to_numpy(), y_predictions_naive)\n",
    "precision_naive = precision(y_test.to_numpy(), y_predictions_naive)\n",
    "recall_naive = recall(y_test.to_numpy(), y_predictions_naive)\n",
    "f1_naive = f1_score(y_test.to_numpy(), y_predictions_naive)\n",
    "\n",
    "print(\"Gaussian Naive Bayes - accuracy:\", f\"{accuracy_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - precision:\", f\"{precision_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - recall:\", f\"{recall_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - f1-score:\", f\"{f1_naive:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy: 0.67\n",
      "Logistic Regression - precision: 0.72\n",
      "Logistic Regression - recall: 0.58\n",
      "Logistic Regression - f1-score: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_log_reg = LogisticRegression()\n",
    "\n",
    "model_log_reg.fit(X_train_standard, y_train)\n",
    "\n",
    "y_predictions_log_reg = model_log_reg.predict(X_test_standard)\n",
    "\n",
    "\n",
    "accuracy_logistic = accuracy(y_test.to_numpy(), y_predictions_log_reg)\n",
    "precision_logistic = precision(y_test.to_numpy(), y_predictions_log_reg)\n",
    "recall_logistic = recall(y_test.to_numpy(), y_predictions_log_reg)\n",
    "f1_logistic = f1_score(y_test.to_numpy(), y_predictions_log_reg)\n",
    "\n",
    "print(\"Logistic Regression - accuracy:\", f\"{accuracy_logistic:.2f}\")\n",
    "print(\"Logistic Regression - precision:\", f\"{precision_logistic:.2f}\")\n",
    "print(\"Logistic Regression - recall:\", f\"{recall_logistic:.2f}\")\n",
    "print(\"Logistic Regression - f1-score:\", f\"{f1_logistic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nb_dots  nb_slashes  nb_hyphens  status\n",
      "0        3           3           0       0\n",
      "1        1           5           0       1\n",
      "2        4           5           1       1\n",
      "3        2           2           0       0\n",
      "\n",
      "StructuralFeatures\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.674978   0.723656  0.580673  0.644327\n",
      "1           GaussianNB  0.680665   0.831530  0.464193  0.595792\n"
     ]
    }
   ],
   "source": [
    "print(df_features_label.head(4))\n",
    "\n",
    "metrics = {\n",
    "    'Model': ['Logistic Regression', 'GaussianNB'],\n",
    "    'Accuracy': [accuracy_logistic, accuracy_naive],\n",
    "    'Precision': [precision_logistic, precision_naive],\n",
    "    'Recall': [recall_logistic, recall_naive],\n",
    "    'F1 Score': [f1_logistic, f1_naive]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "print()\n",
    "print(\"StructuralFeatures\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- Ø·ÙˆÙ„ Ú©Ù„ Ø¢Ø¯Ø±Ø³ (length_url)\n",
    "<br>\n",
    "- Ù†Ø³Ø¨Øª ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ù‚Ø§Ù… (0â€“9) Ø¨Ù‡ Ú©Ù„ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ (ratio_digits_url)\n",
    "<br>\n",
    "- Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ú©Ù„Ù…Ù‡ (Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ù„ÙØ¨Ø§ÛŒÛŒ) Ø¯Ø± URL Ú©Ù‡ Ø¨Ø§ Ø¹Ù„Ø§Ø¦Ù… Ø¬Ø¯Ø§Ø³Ø§Ø² (Ù†Ù‚Ø·Ù‡ØŒ Ø¹Ù„Ø§Ù…Øªâ€ŒØ³ÙˆØ§Ù„ØŒ Ø§Ø³Ù„Ø´ Ùˆ...) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. (longest_words_raw)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† (Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Naive Bayes Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŸ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯. Ú©Ø¯Ø§Ù… Ù†Ø³Ø®Ù‡ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "Ø®ÛŒØ± - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² naive Bayes Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ Ù…Ù†Ø·Ù‚ÛŒ Ù†ÛŒØ³Øª. Ø²ÛŒØ±Ø§ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ Ù†Ù‡ ØªÙ‚Ø±ÛŒØ¨Ø§ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù¾ÛŒØ±ÙˆÛŒ Ù…ÛŒÚ©Ù†Ù†Ø¯ ØªØ§ Ø§Ø² Gaussian Naive Bayes Ø§Ø³ØªØ§ÙØ¯Ù‡ Ú©Ù†ÛŒÙ… Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ù…Ø§Ø±Ø´ÛŒ Ù†ÛŒØ³ØªÙ†Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒ Ù…Ø§ ØªØ§ Ø§Ø² Multinomial Naive Bayes Ø§Ø³ØªØ§ÙØ¯Ù‡ Ú©Ù†ÛŒÙ…. Ùˆ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¹ Ø¨Ø±Ù†ÙˆÙ„ÛŒ Ù‡Ù… Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒ categorical Ù‡Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø¹Ù„Øª Ø¹Ø¯Ø¯ÛŒ Ø¨ÙˆØ¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù†ÛŒØ² Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ³Øª. ÙÙ„Ø°Ø§ Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ù‡Ù…Ø§Ù† logisitic regression Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ú©Ù†ÛŒÙ….\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class StatisticallFeatures:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def length_url(self, X):\n",
    "        return X.str.len()  \n",
    "    \n",
    "    def ratio_digits_url(self, X):\n",
    "        digits_count = X.str.count(r'\\d') \n",
    "        return digits_count / X.str.len()\n",
    "\n",
    "    def longest_words_raw(self, X):\n",
    "        words = X.str.split(r'[^a-zA-Z]+') \n",
    "        longest_words = words.apply(lambda x: max(x, key=len) if x else '') \n",
    "        return longest_words.str.len()  \n",
    "    \n",
    "    def extract_features(self, X):  # X is df['url']\n",
    "        length = self.length_url(X)\n",
    "        ratio = self.ratio_digits_url(X)\n",
    "        longest = self.longest_words_raw(X)\n",
    "        return pd.DataFrame({\n",
    "            'length_url': length,\n",
    "            'ratio_digits_url': ratio,\n",
    "            'longest_words_raw': longest\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "structuralFeatures = StatisticallFeatures()\n",
    "df_features_statistical = structuralFeatures.extract_features(df['url'])\n",
    "df_label = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
    "\n",
    "df_features_label = df_features_statistical.copy()\n",
    "df_features_label['status'] = df_label.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features_statistical, df_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with StatisticallFeatures\n",
      "Logistic Regression - accuracy: 0.68\n",
      "Logistic Regression - precision: 0.73\n",
      "Logistic Regression - recall: 0.55\n",
      "Logistic Regression - f1-score: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_log_reg = LogisticRegression()\n",
    "\n",
    "model_log_reg.fit(X_train_standard, y_train)\n",
    "\n",
    "y_predictions_log_reg = model_log_reg.predict(X_test_standard)\n",
    "\n",
    "\n",
    "accuracy_logistic = accuracy(y_test.to_numpy(), y_predictions_log_reg)\n",
    "precision_logistic = precision(y_test.to_numpy(), y_predictions_log_reg)\n",
    "recall_logistic = recall(y_test.to_numpy(), y_predictions_log_reg)\n",
    "f1_logistic = f1_score(y_test.to_numpy(), y_predictions_log_reg)\n",
    "\n",
    "print(\"with StatisticallFeatures\")\n",
    "print(\"Logistic Regression - accuracy:\", f\"{accuracy_logistic:.2f}\")\n",
    "print(\"Logistic Regression - precision:\", f\"{precision_logistic:.2f}\")\n",
    "print(\"Logistic Regression - recall:\", f\"{recall_logistic:.2f}\")\n",
    "print(\"Logistic Regression - f1-score:\", f\"{f1_logistic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ù‚Ø¨Ù„ÛŒØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú†Ø±Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± ØªØ´Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø¤Ø«Ø± Ø¨Ø§Ø´Ø¯ØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ„ Ú©Ù‡ Ø¨Ø­Ø« Ø·ÙˆÙ„ Ø¨Ø®Ø´ Ù…ÛŒØ²Ø¨Ø§Ù† url (host) Ø§Ø³Øª Ø²ÛŒØ±Ø§ Ø§Ø¯Ø±Ø³ Ù‡Ø§ÛŒ Ú©Ù‡ Phishing Ù‡Ø³ØªÙ†Ø¯ Ø·ÙˆÙ„ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø¨Ø®Ø´ Ù…ÛŒØ²Ø¨Ø§Ù† Ø¯Ø§Ø±Ù†Ø¯.\n",
    "Ù…ÙˆØ±Ø¯ Ø¨Ø¹Ø¯ÛŒ Ù‡Ù… Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ / Ù‡Ø§ Ø§Ø³Øª Ú©Ù‡ Ù†Ø´Ø§Ù† Ø§Ø² Path Ø¨Ø§ Ø¹Ù…Ù‚ Ø¨ÛŒØ´ØªØ±ÛŒ Ù…ÛŒØ¯Ù‡Ø¯ ÙˆØ§ÛŒÙ† Ù…ØªÛŒÙˆØ§Ù†Ø¯ Ø¨Ø¯Ù„ÛŒÙ„ phishing Ø¨ÙˆØ¯Ù†Ø¯ Ø¨Ø§Ø´Ø¯.\n",
    "Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ track Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ø³Ø§ÛŒØª Ù‡Ø§ÛŒ phishing Ø®ÛŒÙ„ÛŒ Ù…ÙˆØ§Ù‚Ø¹ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒ Ø¨ÛŒØ´ØªØ± Ù†Ø´Ø§Ù† Ø§Ø² ØªÙ‚Ù„Ø¨ Ø¨ÛŒØ´ØªØ± Ù‡Ø³Øª Ùˆ Ø§ÛŒÙ† ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ø§ ÛŒØ§ Ù‡Ù…Ø§Ù† arg Ù‡Ø§ Ø±Ø§ Ù‡Ù… Ù…ÛŒØªÙˆØ§Ù† Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ & Ù‡Ø§ Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class MyCustomFeatures:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def hostname_length(self, X):\n",
    "        def get_hostname_len(url):\n",
    "            parsed = urlparse(url if url.startswith('http') else 'http://' + url)\n",
    "            return len(parsed.hostname) if parsed.hostname else 0\n",
    "        return X.apply(get_hostname_len)\n",
    "\n",
    "    def path_depth(self, X):\n",
    "        def get_path_depth(url):\n",
    "            parsed = urlparse(url if url.startswith('http') else 'http://' + url)\n",
    "            path = parsed.path.strip('/')\n",
    "            return len(path.split('/')) if path else 0\n",
    "        return X.apply(get_path_depth)\n",
    "\n",
    "    def num_query_params(self, X):\n",
    "        def get_query_params(url):\n",
    "            parsed = urlparse(url if url.startswith('http') else 'http://' + url)\n",
    "            query = parsed.query\n",
    "            return len(query.split('&')) if query else 0\n",
    "        return X.apply(get_query_params)\n",
    "\n",
    "    def extract_features(self, X):\n",
    "        hlen = self.hostname_length(X)\n",
    "        pdepth = self.path_depth(X)\n",
    "        qparams = self.num_query_params(X)\n",
    "        return pd.DataFrame({\n",
    "            'hostname_length': hlen,\n",
    "            'path_depth': pdepth,\n",
    "            'num_query_params': qparams\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "customFeatures = MyCustomFeatures()\n",
    "df_features_custom = customFeatures.extract_features(df['url'])\n",
    "df_label = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
    "\n",
    "df_features_label = df_features_custom.copy()\n",
    "df_features_label['status'] = df_label.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features_custom, df_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes - accuracy: 0.67\n",
      "Gaussian Naive Bayes - precision: 0.83\n",
      "Gaussian Naive Bayes - recall: 0.43\n",
      "Gaussian Naive Bayes - f1-score: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_gaussianNB = GaussianNB()\n",
    "\n",
    "model_gaussianNB.fit(X_train_standard, y_train)\n",
    "\n",
    "y_predictions_naive = model_gaussianNB.predict(X_test_standard)\n",
    "\n",
    "\n",
    "accuracy_naive = accuracy(y_test.to_numpy(), y_predictions_naive)\n",
    "precision_naive = precision(y_test.to_numpy(), y_predictions_naive)\n",
    "recall_naive = recall(y_test.to_numpy(), y_predictions_naive)\n",
    "f1_naive = f1_score(y_test.to_numpy(), y_predictions_naive)\n",
    "\n",
    "print(\"Gaussian Naive Bayes - accuracy:\", f\"{accuracy_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - precision:\", f\"{precision_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - recall:\", f\"{recall_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - f1-score:\", f\"{f1_naive:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy: 0.69\n",
      "Logistic Regression - precision: 0.72\n",
      "Logistic Regression - recall: 0.61\n",
      "Logistic Regression - f1-score: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_log_reg = LogisticRegression()\n",
    "\n",
    "model_log_reg.fit(X_train_standard, y_train)\n",
    "\n",
    "y_predictions_log_reg = model_log_reg.predict(X_test_standard)\n",
    "\n",
    "\n",
    "accuracy_logistic = accuracy(y_test.to_numpy(), y_predictions_log_reg)\n",
    "precision_logistic = precision(y_test.to_numpy(), y_predictions_log_reg)\n",
    "recall_logistic = recall(y_test.to_numpy(), y_predictions_log_reg)\n",
    "f1_logistic = f1_score(y_test.to_numpy(), y_predictions_log_reg)\n",
    "\n",
    "print(\"Logistic Regression - accuracy:\", f\"{accuracy_logistic:.2f}\")\n",
    "print(\"Logistic Regression - precision:\", f\"{precision_logistic:.2f}\")\n",
    "print(\"Logistic Regression - recall:\", f\"{recall_logistic:.2f}\")\n",
    "print(\"Logistic Regression - f1-score:\", f\"{f1_logistic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hostname_length  path_depth  num_query_params  status\n",
      "0               19           1                 0       0\n",
      "1               23           3                 0       1\n",
      "2               50           2                 3       1\n",
      "3               11           0                 0       0\n",
      "\n",
      "StructuralFeatures\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.686352   0.721703  0.607517  0.659706\n",
      "1           GaussianNB  0.670604   0.833049  0.427448  0.564991\n"
     ]
    }
   ],
   "source": [
    "print(df_features_label.head(4))\n",
    "\n",
    "metrics = {\n",
    "    'Model': ['Logistic Regression', 'GaussianNB'],\n",
    "    'Accuracy': [accuracy_logistic, accuracy_naive],\n",
    "    'Precision': [precision_logistic, precision_naive],\n",
    "    'Recall': [recall_logistic, recall_naive],\n",
    "    'F1 Score': [f1_logistic, f1_naive]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "print()\n",
    "print(\"CustomFeatures\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ØªÙ…Ø§Ù… Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ (Û³ Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Û³ Ø¢Ù…Ø§Ø±ÛŒ + Û³ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡) Ø±Ø§ Ø¨Ø§ Ù‡Ù… ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯.\n",
    "Ø³Ù¾Ø³ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¢ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ\n",
    "<br>\n",
    "Ø§Ú¯Ø± Ø®ÛŒØ±ØŒ Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ú†ÛŒØ³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹ ØªØ¯Ø§Ø®Ù„ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§ Ùˆ...)\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "Ø¨Ù„Ù‡ - Ø¨Ø¯Ù„ÛŒÙ„ Ø§ÛŒÙ†Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯Ù… Ù‡Ø±Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ù¾Ø±Ø±Ù†Ú¯ ØªØ± Ù…ÛŒÚ©Ø±Ø¯ Ùˆ Ø¨Ù‡ ØºÛŒØ± ÛŒÚ©ÛŒ Ø¯ÙˆØªØ§ÛŒ Ø¢Ù† Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù†Ø¯Ø§Ø´ØªÛŒÙ… Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø§Ú¯Ø± Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± F1 Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø³Ù†Ø¬ÛŒÙ… Ù†Ø³Ø¨Øª Ø¨Ù‡ ØªÚ© 3 ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø± Ø³Ø±ÛŒ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ú©Ù‡ 9 ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§Ù‡Ù… Ø§Ø¯ØºØ§Ù… Ø´Ø¯Ù†Ø¯ Ù†ØªÛŒØ¬Ù‡ Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ Ø¯Ø± Ø­Ø§Ù„Øª logisitc regression .\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_combined = pd.concat([df_features_statistical, df_features_structural, df_features_custom], axis=1)\n",
    "df_label = df['status'].map({'legitimate': 0, 'phishing': 1})\n",
    "\n",
    "df_features_label = df_combined.copy()\n",
    "df_features_label['status'] = df_label.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_combined, df_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes - accuracy: 0.71\n",
      "Gaussian Naive Bayes - precision: 0.87\n",
      "Gaussian Naive Bayes - recall: 0.50\n",
      "Gaussian Naive Bayes - f1-score: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_gaussianNB = GaussianNB()\n",
    "\n",
    "model_gaussianNB.fit(X_train_standard, y_train)\n",
    "\n",
    "y_predictions_naive = model_gaussianNB.predict(X_test_standard)\n",
    "\n",
    "\n",
    "accuracy_naive = accuracy(y_test.to_numpy(), y_predictions_naive)\n",
    "precision_naive = precision(y_test.to_numpy(), y_predictions_naive)\n",
    "recall_naive = recall(y_test.to_numpy(), y_predictions_naive)\n",
    "f1_naive = f1_score(y_test.to_numpy(), y_predictions_naive)\n",
    "\n",
    "print(\"Gaussian Naive Bayes - accuracy:\", f\"{accuracy_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - precision:\", f\"{precision_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - recall:\", f\"{recall_naive:.2f}\")\n",
    "print(\"Gaussian Naive Bayes - f1-score:\", f\"{f1_naive:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy: 0.75\n",
      "Logistic Regression - precision: 0.80\n",
      "Logistic Regression - recall: 0.66\n",
      "Logistic Regression - f1-score: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_log_reg = LogisticRegression()\n",
    "\n",
    "model_log_reg.fit(X_train_standard, y_train)\n",
    "\n",
    "y_predictions_log_reg = model_log_reg.predict(X_test_standard)\n",
    "\n",
    "\n",
    "accuracy_logistic = accuracy(y_test.to_numpy(), y_predictions_log_reg)\n",
    "precision_logistic = precision(y_test.to_numpy(), y_predictions_log_reg)\n",
    "recall_logistic = recall(y_test.to_numpy(), y_predictions_log_reg)\n",
    "f1_logistic = f1_score(y_test.to_numpy(), y_predictions_log_reg)\n",
    "\n",
    "print(\"Logistic Regression - accuracy:\", f\"{accuracy_logistic:.2f}\")\n",
    "print(\"Logistic Regression - precision:\", f\"{precision_logistic:.2f}\")\n",
    "print(\"Logistic Regression - recall:\", f\"{recall_logistic:.2f}\")\n",
    "print(\"Logistic Regression - f1-score:\", f\"{f1_logistic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   length_url  ratio_digits_url  longest_words_raw  nb_dots  nb_slashes  \\\n",
      "0          37          0.000000                 11        3           3   \n",
      "1          77          0.220779                 19        1           5   \n",
      "2         126          0.150794                 13        4           5   \n",
      "3          18          0.000000                  5        2           2   \n",
      "\n",
      "   nb_hyphens  hostname_length  path_depth  num_query_params  status  \n",
      "0           0               19           1                 0       0  \n",
      "1           0               23           3                 0       1  \n",
      "2           1               50           2                 3       1  \n",
      "3           0               11           0                 0       0  \n",
      "\n",
      "StructuralFeatures\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.746719   0.796432  0.663462  0.723891\n",
      "1           GaussianNB  0.713473   0.873282  0.500000  0.635909\n"
     ]
    }
   ],
   "source": [
    "print(df_features_label.head(4))\n",
    "\n",
    "metrics = {\n",
    "    'Model': ['Logistic Regression', 'GaussianNB'],\n",
    "    'Accuracy': [accuracy_logistic, accuracy_naive],\n",
    "    'Precision': [precision_logistic, precision_naive],\n",
    "    'Recall': [recall_logistic, recall_naive],\n",
    "    'F1 Score': [f1_logistic, f1_naive]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "print()\n",
    "print(\"CombinedFeatures\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¨Ø§ÛŒØ¯ \"Ø­Ø¯Ø§Ù‚Ù„\" Ø¨Ù‡ Ø³ÙˆØ§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŸ Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ Ú©Ù…ØªØ±ÛŒÙ†ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯Ù‡ Ø§Ø³ØªØŸ Ø§Ú¯Ø± Ø¨Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- ØºÛŒØ± Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø­ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø®ÙˆØ¯ Ø¢Ø¯Ø±Ø³ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒØŒ Ú†Ù‡ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ØªØ®Ø§Ø° Ù†Ù…ÙˆØ¯ØŸ\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "- Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„: Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ù„ÛŒ Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… Ø­Ø³Ø§Ø¨ Ø¨Ú©Ù†ÛŒÙ… logisitc regression Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯. Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ú†Ù†Ø¯ Ù…ÙˆØ¶ÙˆØ¹ Ù…ÛŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø´Ø¯ Ø¯Ø±Ø§Ø¨ØªØ¯Ø§ Ú©Ù‡ Ø¯Ø± naive Ù…Ø§ ÙØ±Ø¶ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ú©Ù‡ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø´Ø±Ø· Ø¯Ø§Ù†Ø³ØªÙ† Ø§ÛŒÙ†Ú©Ù‡ Ú©Ù„Ø§Ø³ Ø¢Ù† Ú†ÛŒØ³Øª Ù…Ø«Ù„Ø§ phishing Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ± Ù…Ø³ØªÙ‚Ù„ Ø§Ø³Øª Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ù…Ø§ Ù‡Ù… Ø¯Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ø´Ù‡ÙˆØ¯Ø§ Ù‡Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ø±ÛŒØ§Ø¶ÛŒ Ù…ÛŒØªÙˆØ§Ù† Ø¯ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† ÙØ±Ø¶ÛŒ Ú©Ù‡ naive Ø¯Ø§Ø±Ø¯ Ø¯Ø± Ø§ÛŒÙ† 9 ÙˆÛŒÚ˜Ú¯ÛŒ ÛŒØ§ Ø­ØªÛŒ Ú©Ù…ØªØ± Ù…Ø§ Ù†Ù…ØªÙˆØ§Ù†Ø¯ ØµØ¯Ù‚ Ú©Ù†Ø¯ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¶Ø¹ÛŒÙ ØªØ±ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒÚ©Ù†Ø¯.Ø¯Ù„ÛŒÙ„ Ø¨Ø¹Ø¯ÛŒ Ù…ÛŒØªÙˆØ§Ù†Ø¯ Ø§ÛŒÙ† Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± naive Ù…Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ÛŒÚ© ÙØ±Ø¶ Ø¯ÛŒÚ©Ø±ÛŒ Ø¯Ø§Ø±ÛŒÙ… Ø¯Ø± Ø¨Ø­Ø« gaussian Ø¢Ù† Ú©Ù‡ ØªÙˆØ²ÛŒØ­ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ù†Ø±Ù…Ø§Ù„ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ Ø¯Ø±Ø³Øª Ø§Ù…Ø§ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ø¯ÛŒÚ¯Ø± Ø§ØµÙ„Ø§ Ø¯Ø±Ø³Øª Ù†ÛŒØ³Øª. Ø§Ù…Ø§ Logistic regression Ù…Ø¯Ù„ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ† ÙØ±Ø¶ Ù‡Ø§Ø±Ø§ Ù†Ù…ÛŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± Ø¨Ø¯Ø³Øª Ø¢Ù…Ø¯Ù† ÙˆØ²Ù† Ù‡Ø§ Ø¯Ø±Ø³Øª Ú©Ù‡ Ø®Ø·ÛŒ Ø¬Ø¯Ø§ Ù…ÛŒÚ©Ù†Ø¯ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø§Ù…Ø§ Ø­Ø¯Ø§Ù‚Ù„ Ø§Ú¯Ø± ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ùˆ Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø§ÛŒ Ø¨Ù‡ Ø¢Ù† ØªÙˆØ¬Ù‡ Ø´ÙˆØ¯ Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ø¯.\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ø·Ø¨Ù‚ ÙˆØ²Ù† Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø²ÛŒØ± Ú†Ø§Ù¾ Ú©Ø±Ø¯Ù… Ø¨Ø±Ø§ÛŒ log reg Ø¨Ø¯Ø³Øª Ø¢Ù…Ø¯ Ú©Ù‡ hostname_length Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ§Ø«ÛŒØ± Ø±Ø§Ø¯Ø± Ù…Ø¯Ù„ Ù…ÛŒÚ¯Ø°Ø§Ø´Øª Ùˆ nb_dots Ú©Ù…ØªØ±ÛŒÙ† Ù…ÛŒØ²Ø§Ù† ØªØ§Ø«ÛŒØ± Ø±Ø§ Ø¯Ø± Ù…Ø¯Ù„ Ø¯Ø§Ø´Øª Ø¨Ø¯Ù„ÛŒÙ„ ÙˆØ²Ù† Ú©Ù…ØªØ±. Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ù‡ÙˆØ¯ÛŒ Ù‡Ù… Ú©Ù‡ 3 ÙˆÛŒÚ˜Ú¯ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ø®ÙˆØ¯Ù… Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¯Ø§Ø¯Ù†Ø¯ ÛŒØ¹Ù†ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒ ØªØ§Ø«ÛŒØ± Ú¯Ø°Ø§Ø± ØªØ±ÛŒ Ø¨ÙˆÙ†Ø¯ Ùˆ 3 ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ù…Ø§Ø±ÛŒ Ø¯Ø±Ú©Ù„ ØªØ§Ø«ÛŒØ± Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ†Ø¯\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø³ÙˆÙ…: Ø¨Ù„Ù‡ - Ø¨Ø±Ø§ÛŒ logisitc regression Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯ Ø²ÛŒØ±Ø§ Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ú¯ÙØªÙ‡ Ø´Ø¯ Ø­Ø³Ø§Ø³ Ø¨Ù‡ Ø§Ø³Ú©ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø³Øª Ùˆ Ø§Ú¯Ø± Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´ÙˆØ¯ ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø± Ø¯ÛŒÚ¯Ø±ÛŒ Ø³Ù„Ø·Ù‡ Ù¾ÛŒØ¯Ø§ Ù…ÛŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± ÙˆØ²Ù† Ø¯Ù‡ÛŒ Ù…Ø¯Ù„ ØªØ§Ø«ÛŒØ± Ù…ÛŒÚ¯Ø°Ø§Ø±Ø¯. Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ naive bayes Ù†ÛŒØ§Ø²ÛŒ Ù†Ø¨ÙˆØ¯ Ø²ÛŒØ±Ø§ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¢Ù† Ø§Ù…ÙˆØ²Ø´ Ù…ÛŒØ¨ÛŒÙ†Ø¯ Ø¯ÛŒÚ¯Ø± Ø§Ø¹Ø¯Ø§Ø¯ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø± Ø§Ø¹Ø¯Ø§Ø¯ Ú©ÙˆÚ†ÛŒÚ© ØªØ± Ú†ÛŒØ±Ù‡ Ù†Ù…ÛŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø§Ù†Ù‡Ø§ Ø¯ÛŒØ¯Ù‡ Ù†Ø´ÙˆÙ†Ø¯ Ø¨Ù„Ú©Ù‡ Ú†ÙˆÙ† Ù‡Ø± Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯Ø´ Ø§Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø§Ø² Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯. Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ ØªØ§Ø«ÛŒØ±ÛŒ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„. Ø¨Ø±Ø§ÛŒ naive bayes Ù‡Ù… Ø¯Ø± Ø§Ú©Ø«Ø± Ù…ÙˆØ§Ù‚Ø¹ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª Ø¨Ø¯Ù„ÛŒÙ„ ÙˆØ²Ù† Ø¯Ø§Ø± Ù†Ø¨ÙˆØ¯Ù† Ù…Ø¯Ù„ Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø¨ÙˆØ¯Ù† Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡Ù…Ø¯Ú¯ÛŒØ± ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ Ø¨Ø§ ÙØ±Ø¶ Ø¯Ø± ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¨ÙˆØ¯Ù†.\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ú†Ù‡Ø§Ø±Ù…: Ù…ÛŒØªÙˆØ§Ù† Ø§Ø² Ø®ÙˆØ¯ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡ ÙˆØ¨ Ú©Ù‡ Ø¯Ø± Ø¢Ù† Ø¢Ø¯Ø±Ø³ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø±Ø¯. Ù…Ø«Ù„Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú© Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ØµÙØ­Ù‡ ÛŒØ§ ØªØ¹Ø¯Ø§Ø¯ ÙØ±Ù… Ù‡Ø§ Ùˆ ÛŒØ§ Ø­ØªÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡ Ú©Ù‡ Ù…ÛŒØªÙˆØ§Ù† Ø¨Ø§ Ù…Ø¯Ù„ Ù‡Ø§ÛŒ NLP Ø¨Ø± Ø±ÙˆÛŒ Ø¢Ù† Ú©Ø§Ø± Ú©Ø±Ø¯ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             feature  coefficient  abs_coeff\n",
      "6    hostname_length     0.984297   0.984297\n",
      "5         nb_hyphens    -0.811798   0.811798\n",
      "1   ratio_digits_url     0.776703   0.776703\n",
      "8   num_query_params     0.584500   0.584500\n",
      "4         nb_slashes     0.404870   0.404870\n",
      "7         path_depth     0.303883   0.303883\n",
      "0         length_url     0.223223   0.223223\n",
      "2  longest_words_raw    -0.119276   0.119276\n",
      "3            nb_dots    -0.076225   0.076225\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "weights = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': model_log_reg.coef_[0],\n",
    "    'abs_coeff': np.abs(model_log_reg.coef_[0])\n",
    "}).sort_values(by='abs_coeff', ascending=False)\n",
    "\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
